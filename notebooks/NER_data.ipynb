{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Access the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert the vocabulary to a list of tokens\n",
    "vocab_list = list(vocab.keys())\n",
    "\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gpu connection\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "file_path_deu = Path.cwd() / \"data\" / \"deu.list\"\n",
    "file_path_eng = Path.cwd() / \"data\" / \"eng.list\"\n",
    "file_path_ned = Path.cwd() / \"data\" / \"ned.list.PER\"\n",
    "\n",
    "# Function to read and process file into a DataFrame\n",
    "def read_and_process_file(file_path):\n",
    "    with open(file_path, 'r',encoding='latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "    data = [line.strip().split(' ', 1) for line in lines]\n",
    "    return pd.DataFrame(data, columns=['NER', 'Entity'])\n",
    "\n",
    "# Read and process files\n",
    "NER_df_deu = read_and_process_file(file_path_deu)\n",
    "NER_df_eng = read_and_process_file(file_path_eng)\n",
    "NER_df_ned = read_and_process_file(file_path_ned)\n",
    "\n",
    "# Filter English DataFrame to use only 'PER' entities\n",
    "NER_df_eng = NER_df_eng[NER_df_eng['NER'] == 'PER']\n",
    "\n",
    "# Combine all DataFrames\n",
    "NER_df = pd.concat([NER_df_deu, NER_df_eng, NER_df_ned])\n",
    "\n",
    "NER_df['NER'] = NER_df['NER'].replace('LOC', 'Ort:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('PER', 'Person:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('ORG', 'Organisation:')\n",
    "\n",
    "# Convert DataFrame to list of elements\n",
    "def create_training_data(row):\n",
    "    if row['NER'] == 'MISC':\n",
    "        return f\"{row['Entity']}\"\n",
    "    else:\n",
    "        return f\"{row['NER']} {row['Entity']}\"\n",
    "\n",
    "NER_list = [create_training_data(row) for _, row in NER_df.iterrows()]\n",
    "NER_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 5</td>\n",
       "      <td>[A, 5]</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "      <td>[[0.5321832895278931, 0.15855109691619873, 0.2...</td>\n",
       "      <td>[-0.24310845136642456, 0.13687995076179504, -0...</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 66</td>\n",
       "      <td>[A, 66]</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "      <td>[[0.4871409237384796, 0.3022191524505615, 0.23...</td>\n",
       "      <td>[0.32661712169647217, 0.14974229037761688, 0.1...</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 661</td>\n",
       "      <td>[A, 66, ##1]</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "      <td>[[0.33346572518348694, 0.23834776878356934, -0...</td>\n",
       "      <td>[0.12139879912137985, 0.2494209259748459, 0.05...</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalen</td>\n",
       "      <td>[A, ##alen]</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "      <td>[[0.07099006325006485, -0.5678775310516357, 1....</td>\n",
       "      <td>[-0.003420088440179825, -0.260809063911438, 0....</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aare-Mündung</td>\n",
       "      <td>[Aar, ##e, -, Mündung]</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "      <td>[[0.3378963768482208, -0.5541061162948608, -0....</td>\n",
       "      <td>[0.2187633365392685, -0.3105980157852173, 0.07...</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47762</th>\n",
       "      <td>Zyg</td>\n",
       "      <td>[Zy, ##g]</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "      <td>[[-0.36112070083618164, -0.00450560450553894, ...</td>\n",
       "      <td>[0.005554556846618652, 0.19277839362621307, -0...</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47763</th>\n",
       "      <td>Zygmunt</td>\n",
       "      <td>[Zy, ##g, ##m, ##unt]</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "      <td>[[-0.20858515799045563, -0.4874523878097534, -...</td>\n",
       "      <td>[0.246858611702919, -0.13464316725730896, -0.2...</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47764</th>\n",
       "      <td>Zyj</td>\n",
       "      <td>[Zy, ##j]</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "      <td>[[-0.07781310379505157, 0.8064130544662476, -0...</td>\n",
       "      <td>[-0.12115689367055893, 0.37860164046287537, -0...</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47765</th>\n",
       "      <td>Zyk</td>\n",
       "      <td>[Zy, ##k]</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "      <td>[[-0.4067027270793915, 0.1616303026676178, 0.6...</td>\n",
       "      <td>[-0.3863324820995331, -0.24419178068637848, 0....</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47766</th>\n",
       "      <td>Zyr</td>\n",
       "      <td>[Zy, ##r]</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "      <td>[[1.0486658811569214, -0.38755977153778076, 0....</td>\n",
       "      <td>[0.7094437479972839, -0.18272826075553894, 0.1...</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47767 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entity                  Tokens                        Token_IDs  \\\n",
       "0               A 5                  [A, 5]                     [32, 435, 4]   \n",
       "1              A 66                 [A, 66]                   [32, 10092, 4]   \n",
       "2             A 661            [A, 66, ##1]            [32, 10092, 26927, 4]   \n",
       "3             Aalen             [A, ##alen]                     [32, 609, 4]   \n",
       "4      Aare-Mündung  [Aar, ##e, -, Mündung]  [15576, 26897, 26935, 14955, 4]   \n",
       "...             ...                     ...                              ...   \n",
       "47762           Zyg               [Zy, ##g]                [10373, 26908, 4]   \n",
       "47763       Zygmunt   [Zy, ##g, ##m, ##unt]   [10373, 26908, 26911, 1937, 4]   \n",
       "47764           Zyj               [Zy, ##j]                [10373, 26963, 4]   \n",
       "47765           Zyk               [Zy, ##k]                [10373, 26917, 4]   \n",
       "47766           Zyr               [Zy, ##r]                [10373, 26900, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "0      [[0.5321832895278931, 0.15855109691619873, 0.2...   \n",
       "1      [[0.4871409237384796, 0.3022191524505615, 0.23...   \n",
       "2      [[0.33346572518348694, 0.23834776878356934, -0...   \n",
       "3      [[0.07099006325006485, -0.5678775310516357, 1....   \n",
       "4      [[0.3378963768482208, -0.5541061162948608, -0....   \n",
       "...                                                  ...   \n",
       "47762  [[-0.36112070083618164, -0.00450560450553894, ...   \n",
       "47763  [[-0.20858515799045563, -0.4874523878097534, -...   \n",
       "47764  [[-0.07781310379505157, 0.8064130544662476, -0...   \n",
       "47765  [[-0.4067027270793915, 0.1616303026676178, 0.6...   \n",
       "47766  [[1.0486658811569214, -0.38755977153778076, 0....   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "0      [-0.24310845136642456, 0.13687995076179504, -0...   \n",
       "1      [0.32661712169647217, 0.14974229037761688, 0.1...   \n",
       "2      [0.12139879912137985, 0.2494209259748459, 0.05...   \n",
       "3      [-0.003420088440179825, -0.260809063911438, 0....   \n",
       "4      [0.2187633365392685, -0.3105980157852173, 0.07...   \n",
       "...                                                  ...   \n",
       "47762  [0.005554556846618652, 0.19277839362621307, -0...   \n",
       "47763  [0.246858611702919, -0.13464316725730896, -0.2...   \n",
       "47764  [-0.12115689367055893, 0.37860164046287537, -0...   \n",
       "47765  [-0.3863324820995331, -0.24419178068637848, 0....   \n",
       "47766  [0.7094437479972839, -0.18272826075553894, 0.1...   \n",
       "\n",
       "                      Target_Token_IDs  \n",
       "0                         [32, 435, 4]  \n",
       "1                       [32, 10092, 4]  \n",
       "2                [32, 10092, 26927, 4]  \n",
       "3                         [32, 609, 4]  \n",
       "4      [15576, 26897, 26935, 14955, 4]  \n",
       "...                                ...  \n",
       "47762                [10373, 26908, 4]  \n",
       "47763   [10373, 26908, 26911, 1937, 4]  \n",
       "47764                [10373, 26963, 4]  \n",
       "47765                [10373, 26917, 4]  \n",
       "47766                [10373, 26900, 4]  \n",
       "\n",
       "[47767 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "model = BertModel.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Function to get embeddings for a list of tokens\n",
    "def get_embeddings(tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: torch.tensor(v).unsqueeze(0) for k, v in tokens.items()})\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Process each element in NER_list\n",
    "rows = []\n",
    "for element in NER_list:\n",
    "    \n",
    "    full_tokens = tokenizer(element, padding=False, truncation=False, add_special_tokens=True)\n",
    "    \n",
    "    # Get embeddings for the full tokenized input\n",
    "    full_embeddings = get_embeddings(full_tokens)\n",
    "    \n",
    "    # Split the element into NER type and entity\n",
    "    parts = element.split(': ', 1)\n",
    "    entity = parts[1] if len(parts) > 1 else element\n",
    "    \n",
    "    # Tokenize the entity separately (without special tokens)\n",
    "    entity_tokens = tokenizer(entity, padding=False, truncation=False, add_special_tokens=False)\n",
    "    \n",
    "    # Find the start index of entity tokens in the full tokenization\n",
    "    start_index = 1  # Skip [CLS]\n",
    "    if len(parts) > 1:\n",
    "        start_index += len(tokenizer(parts[0] + ':', add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    # Process each token in the entity\n",
    "    tokens = []\n",
    "    token_ids = []\n",
    "    token_embeddings = []\n",
    "    for i, token_id in enumerate(entity_tokens['input_ids']):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        token_embedding = full_embeddings[start_index + i]\n",
    "        \n",
    "        tokens.append(token)\n",
    "        token_ids.append(token_id)\n",
    "        token_embeddings.append(token_embedding.tolist())\n",
    "    \n",
    "    # Add the end of sequence token ID\n",
    "    token_ids.append(4) #[SEP]\n",
    "    token_ids.insert(0,3) #[CLS]\n",
    "    # Calculate average embedding for the entity tokens\n",
    "    avg_embedding = torch.mean(full_embeddings[start_index:start_index+len(entity_tokens['input_ids'])], dim=0).tolist()\n",
    "    \n",
    "    \n",
    "    rows.append({\n",
    "        'Entity': entity,\n",
    "        'Tokens': tokens,\n",
    "        'Token_IDs': token_ids,#token_ids[1:-1] w/ [CLS] and [SEP]\n",
    "        'Token_Embeddings': token_embeddings,\n",
    "        'Average_Embedding': avg_embedding,\n",
    "        'Target_Token_IDs': token_ids\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "# check if any element in the list contains '[UNK]'\n",
    "def contains_unk(token_list):\n",
    "    return any('[UNK]' in token for token in token_list)\n",
    "\n",
    "# Filter out rows where the Token column contains '[UNK]'\n",
    "df = df[~df['Tokens'].apply(contains_unk)]\n",
    "# Filter out rows that are longer than 15 Tokens \n",
    "df = df[df['Target_Token_IDs'].apply(len) <= 15]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 16\n",
      "Target_Token_IDs\n",
      "3      2115\n",
      "4     13098\n",
      "5     12997\n",
      "6      8888\n",
      "7      5205\n",
      "8      2500\n",
      "9      1037\n",
      "10      403\n",
      "11      149\n",
      "12       97\n",
      "13       36\n",
      "14        9\n",
      "15       10\n",
      "16        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# determine the longest sequence\n",
    "print(f\"max length: {df['Target_Token_IDs'].apply(len).max()}\")\n",
    "print(df['Target_Token_IDs'].apply(len).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_df = Path.cwd() / \"data\" / \"df_file.pkl\"\n",
    "\n",
    "# #Save the DataFrame to a pickle file\n",
    "#df.to_pickle(file_path_df)\n",
    "\n",
    "# #Load the DataFrame from the pickle file\n",
    "df= pd.read_pickle(file_path_df)\n",
    "df\n",
    "##### adding Special Tokens \n",
    "#df['Target_Token_IDs'] = df['Target_Token_IDs'].apply(lambda x: [3] + x)\n",
    "#df['Token_IDs'] = df['Token_IDs'].apply(lambda x: x[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the average embedding (input)\n",
    "        avg_embedding = torch.tensor(self.data.iloc[idx]['Average_Embedding'], dtype=torch.float32)\n",
    "        \n",
    "        # Get the token IDs (target)\n",
    "        token_ids = torch.tensor(self.data.iloc[idx]['Target_Token_IDs'], dtype=torch.int64)\n",
    "        \n",
    "        return avg_embedding, token_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad the target sequences\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack the inputs \n",
    "    inputs_stacked = torch.stack(inputs)\n",
    "    \n",
    "    return inputs_stacked, targets_padded\n",
    "\n",
    "# # Create the dataset\n",
    "# full_dataset = CustomDataset(df)\n",
    "\n",
    "# # Calculate split sizes\n",
    "# total_size = len(full_dataset)\n",
    "# train_size = int(0.8 * total_size)\n",
    "# val_size = int(0.1 * total_size)\n",
    "# test_size = total_size - train_size - val_size\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Train loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 12])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.2604119777679443 16.38422203063965\n",
      "Target range: 0 26963\n",
      "\n",
      "Information Validation loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 9])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.132582426071167 17.16713523864746\n",
      "Target range: 0 26979\n",
      "\n",
      "Information Test loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 13])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.5452218055725098 16.662612915039062\n",
      "Target range: 0 26979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_train = Path.cwd() / \"data\" / \"train_dataset.pkl\"\n",
    "file_path_test = Path.cwd() / \"data\" / \"test_dataset.pkl\"\n",
    "file_path_val = Path.cwd() / \"data\" / \"val_dataset.pkl\"\n",
    "\n",
    "######################Save the Data to a pickle file\n",
    "# with open(file_path_train, 'wb') as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# with open(file_path_test, 'wb') as f:\n",
    "#     pickle.dump(val_dataset, f)\n",
    "\n",
    "# with open(file_path_val, 'wb') as f:\n",
    "#     pickle.dump(test_dataset, f)\n",
    "\n",
    "######################Load the Data from the pickle file\n",
    "with open(file_path_train, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_test, 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_val, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Create DataLoaders with collate function\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) ########### hyperparameter optimization batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#Infromation dataloaders\n",
    "for loader_name, loader in [(\"Train\", train_loader), (\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
    "    print(f\"\\nInformation {loader_name} loader:\")\n",
    "    for batch in loader:\n",
    "        print(\"Input shape:\", batch[0].shape)\n",
    "        print(\"Target shape:\", batch[1].shape)\n",
    "        print(\"Input dtype:\", batch[0].dtype)\n",
    "        print(\"Target dtype:\", batch[1].dtype)\n",
    "        print(\"Input range:\", batch[0].min().item(), batch[0].max().item())\n",
    "        print(\"Target range:\", batch[1].min().item(), batch[1].max().item())\n",
    "        break\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForMaskedLM\n",
    "import pandas as pd\n",
    "class CustomLSTMWithBERTMLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bert_model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        \n",
    "        self.bert_word_embeddings = self.bert_model.bert.embeddings.word_embeddings\n",
    "        self.bert_position_embeddings = self.bert_model.bert.embeddings.position_embeddings\n",
    "        self.bert_LayerNorm = self.bert_model.bert.embeddings.LayerNorm\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.bert_mlm_head = self.bert_model.cls\n",
    "        \n",
    "        # Remove these lines to allow fine-tuning\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        assert hidden_size == self.bert_model.config.hidden_size, \"LSTM hidden size must match BERT hidden size\"\n",
    "\n",
    "    def forward(self, x, prev_token_id=None, prev_token_position=None, h=None, c=None):\n",
    "        batch_size = x.size(0)\n",
    "        # Handle the case when there's no previous token\n",
    "        if prev_token_id is None:\n",
    "            dummy_prev_token = torch.zeros(batch_size, self.bert_model.config.hidden_size).to(x.device)\n",
    "            input_tensor = torch.cat([x.unsqueeze(1), dummy_prev_token.unsqueeze(1)], dim=2) \n",
    "        else:\n",
    "            # Get embedding for the previous token\n",
    "            token_embedding = self.bert_word_embeddings(prev_token_id).to(x.device)\n",
    "            position_embedding = self.bert_position_embeddings(prev_token_position).to(x.device)\n",
    "            combined_embeddings = token_embedding + position_embedding\n",
    "            prev_token_embedding = self.bert_LayerNorm(combined_embeddings).to(x.device)\n",
    "            # Concatenate the average embedding with the previous token embedding\n",
    "            input_tensor = torch.cat([x.unsqueeze(1), prev_token_embedding], dim=2)\n",
    "            \n",
    "        # Initialize hidden state and cell state if not provided\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        if c is None:\n",
    "            c = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        out, (h, c) = self.lstm(input_tensor, (h, c))\n",
    "        if self.training: # only use dropout druing training\n",
    "            out = self.dropout(out)\n",
    "        out = self.layer_norm(out)\n",
    "        mlm_output = self.bert_mlm_head(out)  # Shape: (batch_size, 1, vocab_size)\n",
    "        \n",
    "        return mlm_output, h, c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from torch.optim.lr_scheduler import StepLR  #or CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "input_size = 768\n",
    "hidden_size = 768\n",
    "num_layers = 1 #### hyperparameter optimization\n",
    "num_epochs = 2\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01 # L2 regularization factor\n",
    "batch_size = 256\n",
    "# CrossEntropyLoss, expect the inputs in a specific format:\n",
    "# The predictions should be a 2D tensor of shape [N, C] where N is the number of samples and C is the number of classes (vocab size in this case).\n",
    "# The targets should be a 1D tensor of shape [N] containing the class indices.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "bert_model_name = 'bert-base-german-cased'\n",
    "model = CustomLSTMWithBERTMLM(input_size, hidden_size, num_layers, bert_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Decays the learning rate every 10 epochs by a factor of 0.1\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sequence Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMWithBERTMLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, max_seq_length, bert_model_name):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        ####self.dropout = nn.Dropout(0.1)\n",
    "        ####self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        #### Remove these lines to allow fine-tuning\n",
    "        for param in self.bert_mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        assert hidden_size == self.bert_mlm.config.hidden_size, \"LSTM hidden size must match BERT hidden size\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Use the input to initialize the hidden state\n",
    "        h = x.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1) #shape:(num_layers, batch_size, hidden_size)\n",
    "        c = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        # dummy input for the LSTM\n",
    "        dummy_input = torch.zeros(batch_size, 1, self.lstm.hidden_size).to(x.device)\n",
    "        outputs = []\n",
    "        for i in range(self.max_seq_length):\n",
    "            out, (h, c) = self.lstm(dummy_input, (h, c))\n",
    "            ####out = self.dropout(out)\n",
    "            ####out = self.layer_norm(out)\n",
    "            mlm_output = self.bert_mlm.cls(out)\n",
    "            \n",
    "            outputs.append(mlm_output)\n",
    "        \n",
    "        output_tensor = torch.cat(outputs, dim=1)\n",
    "        return output_tensor # output shape: (batch_size, generated_seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log file with current date and time\n",
    "current_time_start_epoch = datetime.datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "log_path = Path.cwd() / \"log\" / f\"log_{current_time_start_epoch}.txt\"\n",
    "# log hyperparameters and model information\n",
    "with open(log_path, 'w') as log_file:\n",
    "    log_file.write(f\"Model Architecture: {model}\\n\\n\")\n",
    "    log_file.write(f\"BERT model: {bert_model_name}\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Input size: {input_size}\\n\")\n",
    "    log_file.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"Max sequence length: {max_seq_length}\\n\")\n",
    "    log_file.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"Loss function: CrossEntropyLoss\\n\")\n",
    "    log_file.write(f\"Number of epochs: {num_epochs}\\n\\n\")\n",
    "    \n",
    "best_model_path = Path.cwd() / \"data\" / \"best_model.pth\"\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        # inference with batch\n",
    "        outputs = model(inputs)\n",
    "        # processing of output and target for training\n",
    "        outputs = outputs[:, :targets.shape[1], :] # we only need to consider the outputs for which we have a target during training\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1)) # Reshape outputs from [batch_size, sequence_length, vocab_size] to be [batch_size * sequence_length, vocab_size]  \n",
    "        targets = targets.reshape(-1) # Reshape targets from [batch_size, sequence_length] to be [batch_size * sequence_length] \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad() # this resets the gradients of all parameters to zero from last batch\n",
    "        loss.backward() # computes the gradient of the loss with respect to each parameter of the model\n",
    "        optimizer.step() # updates the parameters based on the computed gradients\n",
    "    \n",
    "        epoch_train_losses.append(loss.item())\n",
    "   \n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Compute test loss\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs[:, :targets.shape[1], :]\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            epoch_test_losses.append(loss.item())\n",
    "    \n",
    "    avg_test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Save the model if the test loss is the best seen so far\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_test_loss,\n",
    "        }, best_model_path)\n",
    "        \n",
    "    # Log epoch information\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] - {current_time}\\n\")\n",
    "        log_file.write(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\\n\\n\")\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, marker='o', label='Test Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# get the best loss for the filename\n",
    "best_loss_str = f\"{best_test_loss:.4f}\".replace('.', '_')\n",
    "\n",
    "# Construct the file path\n",
    "plot_file_path = Path.cwd() / \"log\" / f\"loss_plot_{current_time_start_epoch}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_file_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#### Implement more comprehensive evaluation metrics (e.g., perplexity, BLEU score for generated text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model, optimizer):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}\\n\\n')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\\n\\n')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }, self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "def log_to_file(message, log_path):\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Train Loss: 18.7690, Test Loss: 22.8057\n",
      "Epoch [2/2], Train Loss: 11.3890, Test Loss: 21.7569\n",
      "Training finished!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create log file with current date and time\n",
    "current_time_start_epoch = datetime.datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "log_path = Path.cwd() / \"log\" / f\"log_{current_time_start_epoch}.txt\"\n",
    "# log hyperparameters and model information\n",
    "with open(log_path, 'w') as log_file:\n",
    "    log_file.write(f\"Model Architecture: {model}\\n\\n\")\n",
    "    log_file.write(f\"BERT model: {bert_model_name}\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Input size: {input_size}\\n\")\n",
    "    log_file.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"weight decay: {weight_decay}\\n\")\n",
    "    log_file.write(f\"Loss function: CrossEntropyLoss\\n\")\n",
    "    log_file.write(f\"Number of epochs: {num_epochs}\\n\\n\")\n",
    "    \n",
    "# Initialize early stopping\n",
    "best_model_path = Path.cwd() / \"data\" / \"best_model.pth\"\n",
    "early_stopping = EarlyStopping(patience=0, verbose=True, path=best_model_path, trace_func=lambda msg: log_to_file(msg, log_path))\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        batch_size, seq_len = targets.shape\n",
    "        h, c = None, None\n",
    "        prev_token, prev_position = None, None\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss = 0\n",
    "        \n",
    "        for t in range(seq_len - 1):\n",
    "            if t != 0:\n",
    "                # Previous token (use ground truth for training)\n",
    "                prev_token = targets[:, t-1].unsqueeze(1).to(device) \n",
    "                prev_position = torch.full((batch_size, 1), t-1, device=device)\n",
    "            # Forward pass\n",
    "            output, h, c = model(inputs, prev_token, prev_position, h, c)\n",
    "            # Compute loss\n",
    "            batch_loss = criterion(output.squeeze(1), targets[:, t])\n",
    "            total_batch_loss += batch_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_train_losses.append(total_batch_loss.item())\n",
    "        \n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # increment scheduler at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Compute test loss\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size, seq_len = targets.shape\n",
    "            h, c = None, None\n",
    "            prev_token, prev_position = None, None\n",
    "            \n",
    "            total_batch_loss = 0\n",
    "            \n",
    "            for t in range(seq_len - 1):\n",
    "                if t != 0:\n",
    "                    # Previous token (use ground truth for training)\n",
    "                    prev_token = targets[:, t-1].unsqueeze(1)\n",
    "                    prev_position = torch.full((batch_size, 1), t-1, device=device)\n",
    "                # Forward pass\n",
    "                output, h, c = model(inputs, prev_token, prev_position, h, c)\n",
    "                # Compute loss\n",
    "                batch_loss = criterion(output.squeeze(1), targets[:, t])\n",
    "                total_batch_loss += batch_loss\n",
    "        \n",
    "            epoch_test_losses.append(total_batch_loss.item())\n",
    "   \n",
    "    avg_test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(avg_test_loss)\n",
    "        \n",
    "    # Log epoch information\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] - {current_time} LR: {scheduler.get_last_lr()[0]}\\n\")\n",
    "        log_file.write(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\\n\\n\")\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(avg_test_loss, model, optimizer)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        with open(log_path, 'a') as log_file:\n",
    "            log_file.write(\"Early stopping\\n\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, marker='o', label='Test Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# get the best loss for the filename\n",
    "best_loss_str = f\"{best_test_loss:.4f}\".replace('.', '_')\n",
    "\n",
    "# Construct the file path\n",
    "plot_file_path = Path.cwd() / \"log\" / f\"loss_plot_{current_time_start_epoch}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_file_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#### Implement more comprehensive evaluation metrics (e.g., perplexity, BLEU score for generated text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()  # for inference\n",
    "# to resume training, set the model train mode\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42000</th>\n",
       "      <td>Tall</td>\n",
       "      <td>[Tall]</td>\n",
       "      <td>[21935, 4]</td>\n",
       "      <td>[[-0.01224720198661089, 0.16867724061012268, -...</td>\n",
       "      <td>[-0.01224720198661089, 0.16867724061012268, -0...</td>\n",
       "      <td>[21935, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42001</th>\n",
       "      <td>Tam</td>\n",
       "      <td>[Tam]</td>\n",
       "      <td>[17524, 4]</td>\n",
       "      <td>[[0.16409598290920258, 0.793487548828125, -0.7...</td>\n",
       "      <td>[0.16409598290920258, 0.793487548828125, -0.71...</td>\n",
       "      <td>[17524, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42002</th>\n",
       "      <td>Tamar</td>\n",
       "      <td>[Tam, ##ar]</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "      <td>[[1.1463826894760132, 0.7378958463668823, 0.41...</td>\n",
       "      <td>[0.4127388000488281, 0.5559422373771667, -0.04...</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42003</th>\n",
       "      <td>Tamara</td>\n",
       "      <td>[Tam, ##ara]</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "      <td>[[0.774370014667511, 0.2208947241306305, 0.663...</td>\n",
       "      <td>[0.5452055931091309, 0.20736496150493622, 0.25...</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42004</th>\n",
       "      <td>Tamara Boros</td>\n",
       "      <td>[Tam, ##ara, Bor, ##os]</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "      <td>[[1.0219417810440063, 0.18805761635303497, 0.3...</td>\n",
       "      <td>[0.8827532529830933, 0.0814586728811264, 0.267...</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42005</th>\n",
       "      <td>Tamarine Tanasugarn</td>\n",
       "      <td>[Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.8510592579841614, -0.1594560146331787, 0.3...</td>\n",
       "      <td>[0.4551093578338623, -0.1477975845336914, 0.10...</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42006</th>\n",
       "      <td>Tamas</td>\n",
       "      <td>[Tam, ##as]</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "      <td>[[1.130900263786316, 0.7732470631599426, 0.357...</td>\n",
       "      <td>[0.6211729049682617, 0.5434377789497375, 0.150...</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42007</th>\n",
       "      <td>Tamas Ajan</td>\n",
       "      <td>[Tam, ##as, A, ##jan]</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "      <td>[[1.140594720840454, 1.101357102394104, 0.5228...</td>\n",
       "      <td>[0.2719385027885437, 0.47032666206359863, 0.33...</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42008</th>\n",
       "      <td>Tamas Szekeres</td>\n",
       "      <td>[Tam, ##as, Sz, ##ek, ##ere, ##s]</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "      <td>[[1.1150412559509277, 1.2008758783340454, 0.47...</td>\n",
       "      <td>[0.7783443927764893, 0.3929256498813629, 0.270...</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42009</th>\n",
       "      <td>Tambuyser</td>\n",
       "      <td>[Tam, ##bu, ##yse, ##r]</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "      <td>[[0.9211291670799255, 0.2917001247406006, -0.3...</td>\n",
       "      <td>[0.6440316438674927, 0.13280363380908966, -0.0...</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42010</th>\n",
       "      <td>Tamer</td>\n",
       "      <td>[Tam, ##er]</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "      <td>[[1.3231805562973022, 0.5179563164710999, 0.93...</td>\n",
       "      <td>[0.7314438819885254, 0.7326130867004395, 0.298...</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42011</th>\n",
       "      <td>Taminiaux</td>\n",
       "      <td>[Tam, ##ini, ##aux]</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "      <td>[[0.7550164461135864, 0.8477469682693481, -0.4...</td>\n",
       "      <td>[0.31464725732803345, 0.3305900990962982, -0.5...</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42012</th>\n",
       "      <td>Tamira</td>\n",
       "      <td>[Tam, ##ira]</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "      <td>[[0.7360913157463074, 0.31316858530044556, 0.5...</td>\n",
       "      <td>[0.37108471989631653, -0.010409042239189148, 0...</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42013</th>\n",
       "      <td>Tamme</td>\n",
       "      <td>[Tam, ##me]</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "      <td>[[0.8440600037574768, 0.8673146963119507, 0.84...</td>\n",
       "      <td>[0.4066227674484253, 0.6386289000511169, 0.443...</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42014</th>\n",
       "      <td>Tamminga</td>\n",
       "      <td>[Tam, ##min, ##ga]</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "      <td>[[1.234696626663208, -0.8913580179214478, 0.67...</td>\n",
       "      <td>[0.4619470536708832, -0.8386526703834534, 0.32...</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42015</th>\n",
       "      <td>Tammo</td>\n",
       "      <td>[Tam, ##mo]</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "      <td>[[0.5058582425117493, 0.5409940481185913, -0.2...</td>\n",
       "      <td>[0.3185293972492218, 0.021780773997306824, -0....</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42016</th>\n",
       "      <td>Tammy</td>\n",
       "      <td>[Tam, ##my]</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "      <td>[[0.910221517086029, 1.2008426189422607, 0.694...</td>\n",
       "      <td>[0.47359079122543335, 0.8301605582237244, 0.23...</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42017</th>\n",
       "      <td>Tan</td>\n",
       "      <td>[Tan]</td>\n",
       "      <td>[15297, 4]</td>\n",
       "      <td>[[-0.293621301651001, 0.779782772064209, -0.14...</td>\n",
       "      <td>[-0.293621301651001, 0.779782772064209, -0.144...</td>\n",
       "      <td>[15297, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42018</th>\n",
       "      <td>Tanaka</td>\n",
       "      <td>[Tan, ##aka]</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "      <td>[[-0.6399266123771667, 0.15942348539829254, 1....</td>\n",
       "      <td>[-0.10410656034946442, -0.09060264378786087, 0...</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42019</th>\n",
       "      <td>Tanasugarn</td>\n",
       "      <td>[Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.2947482764720917, 0.4754425287246704, 0.42...</td>\n",
       "      <td>[0.057072967290878296, -0.4458737373352051, 0....</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42020</th>\n",
       "      <td>Tandjung</td>\n",
       "      <td>[Tan, ##d, ##ju, ##n, ##g]</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "      <td>[[-0.4749807119369507, 0.528508722782135, 0.11...</td>\n",
       "      <td>[0.15352052450180054, -0.29984623193740845, -0...</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42021</th>\n",
       "      <td>Tandy</td>\n",
       "      <td>[Tan, ##dy]</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "      <td>[[-0.8069059252738953, 0.6859312057495117, 0.4...</td>\n",
       "      <td>[-0.3132078945636749, 0.3191651403903961, 0.08...</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42022</th>\n",
       "      <td>Tang</td>\n",
       "      <td>[Tan, ##g]</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "      <td>[[0.033787306398153305, -0.24927137792110443, ...</td>\n",
       "      <td>[0.15189029276371002, -0.43078309297561646, -0...</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42023</th>\n",
       "      <td>Tanghe</td>\n",
       "      <td>[Tan, ##gh, ##e]</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "      <td>[[-0.07595154643058777, 0.12992946803569794, 0...</td>\n",
       "      <td>[-0.23548705875873566, -0.060995280742645264, ...</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42024</th>\n",
       "      <td>Tangs</td>\n",
       "      <td>[Tan, ##gs]</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "      <td>[[0.13123495876789093, -0.16866567730903625, 0...</td>\n",
       "      <td>[0.12566909193992615, -0.07040099054574966, 0....</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42025</th>\n",
       "      <td>Tania</td>\n",
       "      <td>[Tan, ##ia]</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "      <td>[[0.21963322162628174, 1.1262986660003662, 0.7...</td>\n",
       "      <td>[0.05399937182664871, 0.3011305630207062, 0.20...</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42026</th>\n",
       "      <td>Tania Kloeck</td>\n",
       "      <td>[Tan, ##ia, Kl, ##oe, ##ck]</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "      <td>[[0.28413882851600647, 1.5511207580566406, 0.2...</td>\n",
       "      <td>[0.4187382161617279, 0.2899208664894104, -0.14...</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42027</th>\n",
       "      <td>Tania Medo</td>\n",
       "      <td>[Tan, ##ia, Med, ##o]</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "      <td>[[0.5423315167427063, 1.4148615598678589, 0.79...</td>\n",
       "      <td>[0.3000876307487488, 0.23904681205749512, 0.40...</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42028</th>\n",
       "      <td>Tania Polak</td>\n",
       "      <td>[Tan, ##ia, Pol, ##ak]</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "      <td>[[0.5157328248023987, 1.47681725025177, 0.6004...</td>\n",
       "      <td>[0.30176252126693726, 0.2940174341201782, -0.0...</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42029</th>\n",
       "      <td>Tania Poppe</td>\n",
       "      <td>[Tan, ##ia, Pop, ##pe]</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "      <td>[[0.4346654713153839, 1.436200499534607, 0.319...</td>\n",
       "      <td>[0.2676783800125122, 0.15604820847511292, 0.17...</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42030</th>\n",
       "      <td>Tania Prinsier</td>\n",
       "      <td>[Tan, ##ia, Prin, ##sie, ##r]</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "      <td>[[0.5102294683456421, 1.3143318891525269, 0.30...</td>\n",
       "      <td>[0.7683398127555847, 0.12537333369255066, 0.10...</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42031</th>\n",
       "      <td>Taniguchi</td>\n",
       "      <td>[Tan, ##ig, ##uch, ##i]</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "      <td>[[-0.5881728529930115, 1.0786540508270264, 0.7...</td>\n",
       "      <td>[-0.03565274178981781, 0.24008022248744965, -0...</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42032</th>\n",
       "      <td>Tanja</td>\n",
       "      <td>[Tan, ##ja]</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "      <td>[[-0.20097672939300537, 0.6485582590103149, 0....</td>\n",
       "      <td>[-0.048050444573163986, 0.1633566915988922, 0....</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42033</th>\n",
       "      <td>Tanne</td>\n",
       "      <td>[Tan, ##ne]</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "      <td>[[-0.363036572933197, 0.45801427960395813, 0.0...</td>\n",
       "      <td>[0.29078078269958496, 0.022456184029579163, -0...</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42034</th>\n",
       "      <td>Tanneke</td>\n",
       "      <td>[Tan, ##ne, ##ke]</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "      <td>[[-0.536466121673584, 0.41328921914100647, 0.6...</td>\n",
       "      <td>[0.046449288725852966, 0.1203099712729454, -0....</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42035</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>[Tan, ##ner]</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "      <td>[[-0.38445645570755005, 0.7667683959007263, 1....</td>\n",
       "      <td>[0.06236431002616882, 0.5567158460617065, 0.40...</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42036</th>\n",
       "      <td>Tanno</td>\n",
       "      <td>[Tan, ##no]</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "      <td>[[-0.26311466097831726, 0.5683020353317261, 0....</td>\n",
       "      <td>[0.1073189526796341, 0.16556155681610107, 0.02...</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42037</th>\n",
       "      <td>Tanny</td>\n",
       "      <td>[Tan, ##ny]</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "      <td>[[-0.7399507761001587, 0.5193020701408386, 0.4...</td>\n",
       "      <td>[-0.14292527735233307, 0.29106348752975464, 0....</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42038</th>\n",
       "      <td>Tansens</td>\n",
       "      <td>[Tan, ##sens]</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "      <td>[[-1.0175524950027466, 0.42136335372924805, 1....</td>\n",
       "      <td>[-0.522026538848877, 0.5173361301422119, 0.640...</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42039</th>\n",
       "      <td>Tansikoezjina</td>\n",
       "      <td>[Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "      <td>[[-0.15889112651348114, 0.5353723764419556, 1....</td>\n",
       "      <td>[0.16954562067985535, 0.22987063229084015, 0.5...</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42040</th>\n",
       "      <td>Tansykkoezina</td>\n",
       "      <td>[Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "      <td>[[-0.5009126663208008, 0.6513158679008484, 1.4...</td>\n",
       "      <td>[0.06688310205936432, 0.5194838643074036, 0.59...</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42041</th>\n",
       "      <td>Tanya</td>\n",
       "      <td>[Tan, ##ya]</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "      <td>[[-0.27348271012306213, 0.7305521965026855, 0....</td>\n",
       "      <td>[-0.15534719824790955, 0.21959252655506134, 0....</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42042</th>\n",
       "      <td>Tao</td>\n",
       "      <td>[Ta, ##o]</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "      <td>[[0.6866366267204285, -0.32815611362457275, -0...</td>\n",
       "      <td>[0.5244305729866028, -0.42992928624153137, -0....</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42043</th>\n",
       "      <td>Tap</td>\n",
       "      <td>[Ta, ##p]</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "      <td>[[0.0655214861035347, -0.1980666071176529, 0.0...</td>\n",
       "      <td>[-0.09740108251571655, 0.21112042665481567, -0...</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42044</th>\n",
       "      <td>Tapias</td>\n",
       "      <td>[Ta, ##p, ##ias]</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "      <td>[[0.3809676170349121, 0.13550198078155518, -0....</td>\n",
       "      <td>[0.33656492829322815, 0.6334041953086853, -0.5...</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42045</th>\n",
       "      <td>Tappert</td>\n",
       "      <td>[Ta, ##pper, ##t]</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "      <td>[[-0.004451925400644541, 0.3471279740333557, 0...</td>\n",
       "      <td>[0.1981954127550125, 0.2921486794948578, -0.06...</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42046</th>\n",
       "      <td>Tar</td>\n",
       "      <td>[Tar]</td>\n",
       "      <td>[10397, 4]</td>\n",
       "      <td>[[0.12060574442148209, 0.9793983697891235, -0....</td>\n",
       "      <td>[0.12060574442148209, 0.9793983697891235, -0.9...</td>\n",
       "      <td>[10397, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42047</th>\n",
       "      <td>Tara</td>\n",
       "      <td>[Tar, ##a]</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "      <td>[[0.21004708111286163, 1.0180401802062988, -0....</td>\n",
       "      <td>[0.07090392708778381, 0.5829620957374573, -0.0...</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42048</th>\n",
       "      <td>Tarabini</td>\n",
       "      <td>[Tar, ##ab, ##ini]</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "      <td>[[-0.2942725419998169, 0.550433337688446, -0.1...</td>\n",
       "      <td>[-0.24213707447052002, 0.6941204071044922, 0.1...</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42049</th>\n",
       "      <td>Tarallo</td>\n",
       "      <td>[Tar, ##all, ##o]</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "      <td>[[-0.028452834114432335, -0.6263936161994934, ...</td>\n",
       "      <td>[0.09504646807909012, -0.5627711415290833, 0.3...</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Entity                                      Tokens  \\\n",
       "42000                 Tall                                      [Tall]   \n",
       "42001                  Tam                                       [Tam]   \n",
       "42002                Tamar                                 [Tam, ##ar]   \n",
       "42003               Tamara                                [Tam, ##ara]   \n",
       "42004         Tamara Boros                     [Tam, ##ara, Bor, ##os]   \n",
       "42005  Tamarine Tanasugarn  [Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]   \n",
       "42006                Tamas                                 [Tam, ##as]   \n",
       "42007           Tamas Ajan                       [Tam, ##as, A, ##jan]   \n",
       "42008       Tamas Szekeres           [Tam, ##as, Sz, ##ek, ##ere, ##s]   \n",
       "42009            Tambuyser                     [Tam, ##bu, ##yse, ##r]   \n",
       "42010                Tamer                                 [Tam, ##er]   \n",
       "42011            Taminiaux                         [Tam, ##ini, ##aux]   \n",
       "42012               Tamira                                [Tam, ##ira]   \n",
       "42013                Tamme                                 [Tam, ##me]   \n",
       "42014             Tamminga                          [Tam, ##min, ##ga]   \n",
       "42015                Tammo                                 [Tam, ##mo]   \n",
       "42016                Tammy                                 [Tam, ##my]   \n",
       "42017                  Tan                                       [Tan]   \n",
       "42018               Tanaka                                [Tan, ##aka]   \n",
       "42019           Tanasugarn                    [Tan, ##as, ##ug, ##arn]   \n",
       "42020             Tandjung                  [Tan, ##d, ##ju, ##n, ##g]   \n",
       "42021                Tandy                                 [Tan, ##dy]   \n",
       "42022                 Tang                                  [Tan, ##g]   \n",
       "42023               Tanghe                            [Tan, ##gh, ##e]   \n",
       "42024                Tangs                                 [Tan, ##gs]   \n",
       "42025                Tania                                 [Tan, ##ia]   \n",
       "42026         Tania Kloeck                 [Tan, ##ia, Kl, ##oe, ##ck]   \n",
       "42027           Tania Medo                       [Tan, ##ia, Med, ##o]   \n",
       "42028          Tania Polak                      [Tan, ##ia, Pol, ##ak]   \n",
       "42029          Tania Poppe                      [Tan, ##ia, Pop, ##pe]   \n",
       "42030       Tania Prinsier               [Tan, ##ia, Prin, ##sie, ##r]   \n",
       "42031            Taniguchi                     [Tan, ##ig, ##uch, ##i]   \n",
       "42032                Tanja                                 [Tan, ##ja]   \n",
       "42033                Tanne                                 [Tan, ##ne]   \n",
       "42034              Tanneke                           [Tan, ##ne, ##ke]   \n",
       "42035               Tanner                                [Tan, ##ner]   \n",
       "42036                Tanno                                 [Tan, ##no]   \n",
       "42037                Tanny                                 [Tan, ##ny]   \n",
       "42038              Tansens                               [Tan, ##sens]   \n",
       "42039        Tansikoezjina     [Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]   \n",
       "42040        Tansykkoezina     [Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]   \n",
       "42041                Tanya                                 [Tan, ##ya]   \n",
       "42042                  Tao                                   [Ta, ##o]   \n",
       "42043                  Tap                                   [Ta, ##p]   \n",
       "42044               Tapias                            [Ta, ##p, ##ias]   \n",
       "42045              Tappert                           [Ta, ##pper, ##t]   \n",
       "42046                  Tar                                       [Tar]   \n",
       "42047                 Tara                                  [Tar, ##a]   \n",
       "42048             Tarabini                          [Tar, ##ab, ##ini]   \n",
       "42049              Tarallo                           [Tar, ##all, ##o]   \n",
       "\n",
       "                                               Token_IDs  \\\n",
       "42000                                         [21935, 4]   \n",
       "42001                                         [17524, 4]   \n",
       "42002                                     [17524, 33, 4]   \n",
       "42003                                   [17524, 3738, 4]   \n",
       "42004                        [17524, 3738, 3888, 224, 4]   \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]   \n",
       "42006                                     [17524, 45, 4]   \n",
       "42007                          [17524, 45, 32, 10761, 4]   \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]   \n",
       "42009                    [17524, 13874, 18940, 26900, 4]   \n",
       "42010                                      [17524, 6, 4]   \n",
       "42011                             [17524, 5381, 8805, 4]   \n",
       "42012                                   [17524, 7872, 4]   \n",
       "42013                                    [17524, 373, 4]   \n",
       "42014                               [17524, 734, 529, 4]   \n",
       "42015                                   [17524, 4359, 4]   \n",
       "42016                                  [17524, 16601, 4]   \n",
       "42017                                         [15297, 4]   \n",
       "42018                                  [15297, 10550, 4]   \n",
       "42019                          [15297, 45, 389, 7074, 4]   \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]   \n",
       "42021                                   [15297, 6484, 4]   \n",
       "42022                                  [15297, 26908, 4]   \n",
       "42023                           [15297, 13901, 26897, 4]   \n",
       "42024                                    [15297, 753, 4]   \n",
       "42025                                    [15297, 544, 4]   \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]   \n",
       "42027                       [15297, 544, 1371, 26910, 4]   \n",
       "42028                          [15297, 544, 984, 464, 4]   \n",
       "42029                        [15297, 544, 4528, 3500, 4]   \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]   \n",
       "42031                         [15297, 80, 108, 26899, 4]   \n",
       "42032                                   [15297, 3171, 4]   \n",
       "42033                                    [15297, 175, 4]   \n",
       "42034                               [15297, 175, 772, 4]   \n",
       "42035                                    [15297, 344, 4]   \n",
       "42036                                   [15297, 9706, 4]   \n",
       "42037                                   [15297, 9381, 4]   \n",
       "42038                                   [15297, 8531, 4]   \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]   \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]   \n",
       "42041                                   [15297, 5630, 4]   \n",
       "42042                                   [2127, 26910, 4]   \n",
       "42043                                   [2127, 26920, 4]   \n",
       "42044                             [2127, 26920, 3019, 4]   \n",
       "42045                             [2127, 9804, 26901, 4]   \n",
       "42046                                         [10397, 4]   \n",
       "42047                                  [10397, 26903, 4]   \n",
       "42048                              [10397, 228, 5381, 4]   \n",
       "42049                             [10397, 211, 26910, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "42000  [[-0.01224720198661089, 0.16867724061012268, -...   \n",
       "42001  [[0.16409598290920258, 0.793487548828125, -0.7...   \n",
       "42002  [[1.1463826894760132, 0.7378958463668823, 0.41...   \n",
       "42003  [[0.774370014667511, 0.2208947241306305, 0.663...   \n",
       "42004  [[1.0219417810440063, 0.18805761635303497, 0.3...   \n",
       "42005  [[0.8510592579841614, -0.1594560146331787, 0.3...   \n",
       "42006  [[1.130900263786316, 0.7732470631599426, 0.357...   \n",
       "42007  [[1.140594720840454, 1.101357102394104, 0.5228...   \n",
       "42008  [[1.1150412559509277, 1.2008758783340454, 0.47...   \n",
       "42009  [[0.9211291670799255, 0.2917001247406006, -0.3...   \n",
       "42010  [[1.3231805562973022, 0.5179563164710999, 0.93...   \n",
       "42011  [[0.7550164461135864, 0.8477469682693481, -0.4...   \n",
       "42012  [[0.7360913157463074, 0.31316858530044556, 0.5...   \n",
       "42013  [[0.8440600037574768, 0.8673146963119507, 0.84...   \n",
       "42014  [[1.234696626663208, -0.8913580179214478, 0.67...   \n",
       "42015  [[0.5058582425117493, 0.5409940481185913, -0.2...   \n",
       "42016  [[0.910221517086029, 1.2008426189422607, 0.694...   \n",
       "42017  [[-0.293621301651001, 0.779782772064209, -0.14...   \n",
       "42018  [[-0.6399266123771667, 0.15942348539829254, 1....   \n",
       "42019  [[0.2947482764720917, 0.4754425287246704, 0.42...   \n",
       "42020  [[-0.4749807119369507, 0.528508722782135, 0.11...   \n",
       "42021  [[-0.8069059252738953, 0.6859312057495117, 0.4...   \n",
       "42022  [[0.033787306398153305, -0.24927137792110443, ...   \n",
       "42023  [[-0.07595154643058777, 0.12992946803569794, 0...   \n",
       "42024  [[0.13123495876789093, -0.16866567730903625, 0...   \n",
       "42025  [[0.21963322162628174, 1.1262986660003662, 0.7...   \n",
       "42026  [[0.28413882851600647, 1.5511207580566406, 0.2...   \n",
       "42027  [[0.5423315167427063, 1.4148615598678589, 0.79...   \n",
       "42028  [[0.5157328248023987, 1.47681725025177, 0.6004...   \n",
       "42029  [[0.4346654713153839, 1.436200499534607, 0.319...   \n",
       "42030  [[0.5102294683456421, 1.3143318891525269, 0.30...   \n",
       "42031  [[-0.5881728529930115, 1.0786540508270264, 0.7...   \n",
       "42032  [[-0.20097672939300537, 0.6485582590103149, 0....   \n",
       "42033  [[-0.363036572933197, 0.45801427960395813, 0.0...   \n",
       "42034  [[-0.536466121673584, 0.41328921914100647, 0.6...   \n",
       "42035  [[-0.38445645570755005, 0.7667683959007263, 1....   \n",
       "42036  [[-0.26311466097831726, 0.5683020353317261, 0....   \n",
       "42037  [[-0.7399507761001587, 0.5193020701408386, 0.4...   \n",
       "42038  [[-1.0175524950027466, 0.42136335372924805, 1....   \n",
       "42039  [[-0.15889112651348114, 0.5353723764419556, 1....   \n",
       "42040  [[-0.5009126663208008, 0.6513158679008484, 1.4...   \n",
       "42041  [[-0.27348271012306213, 0.7305521965026855, 0....   \n",
       "42042  [[0.6866366267204285, -0.32815611362457275, -0...   \n",
       "42043  [[0.0655214861035347, -0.1980666071176529, 0.0...   \n",
       "42044  [[0.3809676170349121, 0.13550198078155518, -0....   \n",
       "42045  [[-0.004451925400644541, 0.3471279740333557, 0...   \n",
       "42046  [[0.12060574442148209, 0.9793983697891235, -0....   \n",
       "42047  [[0.21004708111286163, 1.0180401802062988, -0....   \n",
       "42048  [[-0.2942725419998169, 0.550433337688446, -0.1...   \n",
       "42049  [[-0.028452834114432335, -0.6263936161994934, ...   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "42000  [-0.01224720198661089, 0.16867724061012268, -0...   \n",
       "42001  [0.16409598290920258, 0.793487548828125, -0.71...   \n",
       "42002  [0.4127388000488281, 0.5559422373771667, -0.04...   \n",
       "42003  [0.5452055931091309, 0.20736496150493622, 0.25...   \n",
       "42004  [0.8827532529830933, 0.0814586728811264, 0.267...   \n",
       "42005  [0.4551093578338623, -0.1477975845336914, 0.10...   \n",
       "42006  [0.6211729049682617, 0.5434377789497375, 0.150...   \n",
       "42007  [0.2719385027885437, 0.47032666206359863, 0.33...   \n",
       "42008  [0.7783443927764893, 0.3929256498813629, 0.270...   \n",
       "42009  [0.6440316438674927, 0.13280363380908966, -0.0...   \n",
       "42010  [0.7314438819885254, 0.7326130867004395, 0.298...   \n",
       "42011  [0.31464725732803345, 0.3305900990962982, -0.5...   \n",
       "42012  [0.37108471989631653, -0.010409042239189148, 0...   \n",
       "42013  [0.4066227674484253, 0.6386289000511169, 0.443...   \n",
       "42014  [0.4619470536708832, -0.8386526703834534, 0.32...   \n",
       "42015  [0.3185293972492218, 0.021780773997306824, -0....   \n",
       "42016  [0.47359079122543335, 0.8301605582237244, 0.23...   \n",
       "42017  [-0.293621301651001, 0.779782772064209, -0.144...   \n",
       "42018  [-0.10410656034946442, -0.09060264378786087, 0...   \n",
       "42019  [0.057072967290878296, -0.4458737373352051, 0....   \n",
       "42020  [0.15352052450180054, -0.29984623193740845, -0...   \n",
       "42021  [-0.3132078945636749, 0.3191651403903961, 0.08...   \n",
       "42022  [0.15189029276371002, -0.43078309297561646, -0...   \n",
       "42023  [-0.23548705875873566, -0.060995280742645264, ...   \n",
       "42024  [0.12566909193992615, -0.07040099054574966, 0....   \n",
       "42025  [0.05399937182664871, 0.3011305630207062, 0.20...   \n",
       "42026  [0.4187382161617279, 0.2899208664894104, -0.14...   \n",
       "42027  [0.3000876307487488, 0.23904681205749512, 0.40...   \n",
       "42028  [0.30176252126693726, 0.2940174341201782, -0.0...   \n",
       "42029  [0.2676783800125122, 0.15604820847511292, 0.17...   \n",
       "42030  [0.7683398127555847, 0.12537333369255066, 0.10...   \n",
       "42031  [-0.03565274178981781, 0.24008022248744965, -0...   \n",
       "42032  [-0.048050444573163986, 0.1633566915988922, 0....   \n",
       "42033  [0.29078078269958496, 0.022456184029579163, -0...   \n",
       "42034  [0.046449288725852966, 0.1203099712729454, -0....   \n",
       "42035  [0.06236431002616882, 0.5567158460617065, 0.40...   \n",
       "42036  [0.1073189526796341, 0.16556155681610107, 0.02...   \n",
       "42037  [-0.14292527735233307, 0.29106348752975464, 0....   \n",
       "42038  [-0.522026538848877, 0.5173361301422119, 0.640...   \n",
       "42039  [0.16954562067985535, 0.22987063229084015, 0.5...   \n",
       "42040  [0.06688310205936432, 0.5194838643074036, 0.59...   \n",
       "42041  [-0.15534719824790955, 0.21959252655506134, 0....   \n",
       "42042  [0.5244305729866028, -0.42992928624153137, -0....   \n",
       "42043  [-0.09740108251571655, 0.21112042665481567, -0...   \n",
       "42044  [0.33656492829322815, 0.6334041953086853, -0.5...   \n",
       "42045  [0.1981954127550125, 0.2921486794948578, -0.06...   \n",
       "42046  [0.12060574442148209, 0.9793983697891235, -0.9...   \n",
       "42047  [0.07090392708778381, 0.5829620957374573, -0.0...   \n",
       "42048  [-0.24213707447052002, 0.6941204071044922, 0.1...   \n",
       "42049  [0.09504646807909012, -0.5627711415290833, 0.3...   \n",
       "\n",
       "                                        Target_Token_IDs  \n",
       "42000                                         [21935, 4]  \n",
       "42001                                         [17524, 4]  \n",
       "42002                                     [17524, 33, 4]  \n",
       "42003                                   [17524, 3738, 4]  \n",
       "42004                        [17524, 3738, 3888, 224, 4]  \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]  \n",
       "42006                                     [17524, 45, 4]  \n",
       "42007                          [17524, 45, 32, 10761, 4]  \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]  \n",
       "42009                    [17524, 13874, 18940, 26900, 4]  \n",
       "42010                                      [17524, 6, 4]  \n",
       "42011                             [17524, 5381, 8805, 4]  \n",
       "42012                                   [17524, 7872, 4]  \n",
       "42013                                    [17524, 373, 4]  \n",
       "42014                               [17524, 734, 529, 4]  \n",
       "42015                                   [17524, 4359, 4]  \n",
       "42016                                  [17524, 16601, 4]  \n",
       "42017                                         [15297, 4]  \n",
       "42018                                  [15297, 10550, 4]  \n",
       "42019                          [15297, 45, 389, 7074, 4]  \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]  \n",
       "42021                                   [15297, 6484, 4]  \n",
       "42022                                  [15297, 26908, 4]  \n",
       "42023                           [15297, 13901, 26897, 4]  \n",
       "42024                                    [15297, 753, 4]  \n",
       "42025                                    [15297, 544, 4]  \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]  \n",
       "42027                       [15297, 544, 1371, 26910, 4]  \n",
       "42028                          [15297, 544, 984, 464, 4]  \n",
       "42029                        [15297, 544, 4528, 3500, 4]  \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]  \n",
       "42031                         [15297, 80, 108, 26899, 4]  \n",
       "42032                                   [15297, 3171, 4]  \n",
       "42033                                    [15297, 175, 4]  \n",
       "42034                               [15297, 175, 772, 4]  \n",
       "42035                                    [15297, 344, 4]  \n",
       "42036                                   [15297, 9706, 4]  \n",
       "42037                                   [15297, 9381, 4]  \n",
       "42038                                   [15297, 8531, 4]  \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]  \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]  \n",
       "42041                                   [15297, 5630, 4]  \n",
       "42042                                   [2127, 26910, 4]  \n",
       "42043                                   [2127, 26920, 4]  \n",
       "42044                             [2127, 26920, 3019, 4]  \n",
       "42045                             [2127, 9804, 26901, 4]  \n",
       "42046                                         [10397, 4]  \n",
       "42047                                  [10397, 26903, 4]  \n",
       "42048                              [10397, 228, 5381, 4]  \n",
       "42049                             [10397, 211, 26910, 4]  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[42000:42050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# # end inference if this token is predicted\n",
    "# eos_token_id = tokenizer.convert_tokens_to_ids('[SEP]')  # EOS token\n",
    "\n",
    "# Example forward pass\n",
    "#input_tensor = torch.randn(1,768)\n",
    "\n",
    "tensor = torch.tensor(df[\"Average_Embedding\"][42042]).unsqueeze(0) #159\n",
    "# Generate Gaussian noise\n",
    "noise_level = 0.6\n",
    "noise = torch.randn_like(tensor) * noise_level\n",
    "# Add noise to tensor\n",
    "input_tensor = tensor + noise\n",
    "\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "for batch_idx in range(output_tensor.size(0)):\n",
    "    for position in range(output_tensor.size(1)):  # Iterate over sequence length\n",
    "        position_probs = output_tensor[batch_idx, position, :]  # Get probabilities for this position\n",
    "        \n",
    "        # Get top 5 probabilities\n",
    "        top_5_values, top_5_indices = torch.topk(position_probs, 1, dim=0)\n",
    "        \n",
    "        print(f\"Position {position}:\")\n",
    "        for i, (value, index) in enumerate(zip(top_5_values, top_5_indices), 1):\n",
    "            token = tokenizer.convert_ids_to_tokens([index.item()])[0]\n",
    "            probability = value.item()\n",
    "            print(f\"  {i}. {token} (probability: {probability:.4f})\")\n",
    "        print()\n",
    "########################################################################\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Example forward pass\n",
    "input_tensor = torch.randn(2,768)\n",
    "\n",
    "\n",
    "input_id = torch.tensor([[0],[0]])    # Shape: [2, 1]\n",
    "position_id = torch.tensor([[0],[0]])  # Shape: [2, 1]\n",
    "\n",
    "output_tensor,h,c = model(input_tensor,input_id,position_id) # output size:[2, 1, 30000] # h size: [2, 1, 768]# c size:[2, 1, 768]\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "for batch_idx in range(inputs.size(0)):\n",
    "        position_probs = output_tensor[batch_idx, 0, :]  # Get probabilities for this position\n",
    "        # Get top 5 probabilities\n",
    "        top_values, top_indices = torch.topk(position_probs, 1, dim=0)\n",
    "        \n",
    "        for i, (value, index) in enumerate(zip(top_values, top_indices), 1):\n",
    "            token = tokenizer.convert_ids_to_tokens([index.item()])[0]\n",
    "            probability = value.item()\n",
    "            print(f\"  {i}. {token} (probability: {probability:.4f})\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(df[\"Average_Embedding\"][8]).unsqueeze(0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
