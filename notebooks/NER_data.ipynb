{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Access the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert the vocabulary to a list of tokens\n",
    "vocab_list = list(vocab.keys())\n",
    "\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gpu connection\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "file_path_deu = Path.cwd() / \"data\" / \"deu.list\"\n",
    "file_path_eng = Path.cwd() / \"data\" / \"eng.list\"\n",
    "file_path_ned = Path.cwd() / \"data\" / \"ned.list.PER\"\n",
    "\n",
    "# Function to read and process file into a DataFrame\n",
    "def read_and_process_file(file_path):\n",
    "    with open(file_path, 'r',encoding='latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "    data = [line.strip().split(' ', 1) for line in lines]\n",
    "    return pd.DataFrame(data, columns=['NER', 'Entity'])\n",
    "\n",
    "# Read and process files\n",
    "NER_df_deu = read_and_process_file(file_path_deu)\n",
    "NER_df_eng = read_and_process_file(file_path_eng)\n",
    "NER_df_ned = read_and_process_file(file_path_ned)\n",
    "\n",
    "# Filter English DataFrame to use only 'PER' entities\n",
    "NER_df_eng = NER_df_eng[NER_df_eng['NER'] == 'PER']\n",
    "\n",
    "# Combine all DataFrames\n",
    "NER_df = pd.concat([NER_df_deu, NER_df_eng, NER_df_ned])\n",
    "\n",
    "NER_df['NER'] = NER_df['NER'].replace('LOC', 'Ort:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('PER', 'Person:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('ORG', 'Organisation:')\n",
    "\n",
    "# Convert DataFrame to list of elements\n",
    "def create_training_data(row):\n",
    "    if row['NER'] == 'MISC':\n",
    "        return f\"{row['Entity']}\"\n",
    "    else:\n",
    "        return f\"{row['NER']} {row['Entity']}\"\n",
    "\n",
    "NER_list = [create_training_data(row) for _, row in NER_df.iterrows()]\n",
    "NER_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 5</td>\n",
       "      <td>[A, 5]</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "      <td>[[0.5321832895278931, 0.15855109691619873, 0.2...</td>\n",
       "      <td>[-0.24310845136642456, 0.13687995076179504, -0...</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 66</td>\n",
       "      <td>[A, 66]</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "      <td>[[0.4871409237384796, 0.3022191524505615, 0.23...</td>\n",
       "      <td>[0.32661712169647217, 0.14974229037761688, 0.1...</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 661</td>\n",
       "      <td>[A, 66, ##1]</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "      <td>[[0.33346572518348694, 0.23834776878356934, -0...</td>\n",
       "      <td>[0.12139879912137985, 0.2494209259748459, 0.05...</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalen</td>\n",
       "      <td>[A, ##alen]</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "      <td>[[0.07099006325006485, -0.5678775310516357, 1....</td>\n",
       "      <td>[-0.003420088440179825, -0.260809063911438, 0....</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aare-Mündung</td>\n",
       "      <td>[Aar, ##e, -, Mündung]</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "      <td>[[0.3378963768482208, -0.5541061162948608, -0....</td>\n",
       "      <td>[0.2187633365392685, -0.3105980157852173, 0.07...</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47762</th>\n",
       "      <td>Zyg</td>\n",
       "      <td>[Zy, ##g]</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "      <td>[[-0.36112070083618164, -0.00450560450553894, ...</td>\n",
       "      <td>[0.005554556846618652, 0.19277839362621307, -0...</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47763</th>\n",
       "      <td>Zygmunt</td>\n",
       "      <td>[Zy, ##g, ##m, ##unt]</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "      <td>[[-0.20858515799045563, -0.4874523878097534, -...</td>\n",
       "      <td>[0.246858611702919, -0.13464316725730896, -0.2...</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47764</th>\n",
       "      <td>Zyj</td>\n",
       "      <td>[Zy, ##j]</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "      <td>[[-0.07781310379505157, 0.8064130544662476, -0...</td>\n",
       "      <td>[-0.12115689367055893, 0.37860164046287537, -0...</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47765</th>\n",
       "      <td>Zyk</td>\n",
       "      <td>[Zy, ##k]</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "      <td>[[-0.4067027270793915, 0.1616303026676178, 0.6...</td>\n",
       "      <td>[-0.3863324820995331, -0.24419178068637848, 0....</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47766</th>\n",
       "      <td>Zyr</td>\n",
       "      <td>[Zy, ##r]</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "      <td>[[1.0486658811569214, -0.38755977153778076, 0....</td>\n",
       "      <td>[0.7094437479972839, -0.18272826075553894, 0.1...</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47767 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entity                  Tokens                        Token_IDs  \\\n",
       "0               A 5                  [A, 5]                     [32, 435, 4]   \n",
       "1              A 66                 [A, 66]                   [32, 10092, 4]   \n",
       "2             A 661            [A, 66, ##1]            [32, 10092, 26927, 4]   \n",
       "3             Aalen             [A, ##alen]                     [32, 609, 4]   \n",
       "4      Aare-Mündung  [Aar, ##e, -, Mündung]  [15576, 26897, 26935, 14955, 4]   \n",
       "...             ...                     ...                              ...   \n",
       "47762           Zyg               [Zy, ##g]                [10373, 26908, 4]   \n",
       "47763       Zygmunt   [Zy, ##g, ##m, ##unt]   [10373, 26908, 26911, 1937, 4]   \n",
       "47764           Zyj               [Zy, ##j]                [10373, 26963, 4]   \n",
       "47765           Zyk               [Zy, ##k]                [10373, 26917, 4]   \n",
       "47766           Zyr               [Zy, ##r]                [10373, 26900, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "0      [[0.5321832895278931, 0.15855109691619873, 0.2...   \n",
       "1      [[0.4871409237384796, 0.3022191524505615, 0.23...   \n",
       "2      [[0.33346572518348694, 0.23834776878356934, -0...   \n",
       "3      [[0.07099006325006485, -0.5678775310516357, 1....   \n",
       "4      [[0.3378963768482208, -0.5541061162948608, -0....   \n",
       "...                                                  ...   \n",
       "47762  [[-0.36112070083618164, -0.00450560450553894, ...   \n",
       "47763  [[-0.20858515799045563, -0.4874523878097534, -...   \n",
       "47764  [[-0.07781310379505157, 0.8064130544662476, -0...   \n",
       "47765  [[-0.4067027270793915, 0.1616303026676178, 0.6...   \n",
       "47766  [[1.0486658811569214, -0.38755977153778076, 0....   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "0      [-0.24310845136642456, 0.13687995076179504, -0...   \n",
       "1      [0.32661712169647217, 0.14974229037761688, 0.1...   \n",
       "2      [0.12139879912137985, 0.2494209259748459, 0.05...   \n",
       "3      [-0.003420088440179825, -0.260809063911438, 0....   \n",
       "4      [0.2187633365392685, -0.3105980157852173, 0.07...   \n",
       "...                                                  ...   \n",
       "47762  [0.005554556846618652, 0.19277839362621307, -0...   \n",
       "47763  [0.246858611702919, -0.13464316725730896, -0.2...   \n",
       "47764  [-0.12115689367055893, 0.37860164046287537, -0...   \n",
       "47765  [-0.3863324820995331, -0.24419178068637848, 0....   \n",
       "47766  [0.7094437479972839, -0.18272826075553894, 0.1...   \n",
       "\n",
       "                      Target_Token_IDs  \n",
       "0                         [32, 435, 4]  \n",
       "1                       [32, 10092, 4]  \n",
       "2                [32, 10092, 26927, 4]  \n",
       "3                         [32, 609, 4]  \n",
       "4      [15576, 26897, 26935, 14955, 4]  \n",
       "...                                ...  \n",
       "47762                [10373, 26908, 4]  \n",
       "47763   [10373, 26908, 26911, 1937, 4]  \n",
       "47764                [10373, 26963, 4]  \n",
       "47765                [10373, 26917, 4]  \n",
       "47766                [10373, 26900, 4]  \n",
       "\n",
       "[47767 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "model = BertModel.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Function to get embeddings for a list of tokens\n",
    "def get_embeddings(tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: torch.tensor(v).unsqueeze(0) for k, v in tokens.items()})\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Process each element in NER_list\n",
    "rows = []\n",
    "for element in NER_list:\n",
    "    \n",
    "    full_tokens = tokenizer(element, padding=False, truncation=False, add_special_tokens=True)\n",
    "    \n",
    "    # Get embeddings for the full tokenized input\n",
    "    full_embeddings = get_embeddings(full_tokens)\n",
    "    \n",
    "    # Split the element into NER type and entity\n",
    "    parts = element.split(': ', 1)\n",
    "    entity = parts[1] if len(parts) > 1 else element\n",
    "    \n",
    "    # Tokenize the entity separately (without special tokens)\n",
    "    entity_tokens = tokenizer(entity, padding=False, truncation=False, add_special_tokens=False)\n",
    "    \n",
    "    # Find the start index of entity tokens in the full tokenization\n",
    "    start_index = 1  # Skip [CLS]\n",
    "    if len(parts) > 1:\n",
    "        start_index += len(tokenizer(parts[0] + ':', add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    # Process each token in the entity\n",
    "    tokens = []\n",
    "    token_ids = []\n",
    "    token_embeddings = []\n",
    "    for i, token_id in enumerate(entity_tokens['input_ids']):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        token_embedding = full_embeddings[start_index + i]\n",
    "        \n",
    "        tokens.append(token)\n",
    "        token_ids.append(token_id)\n",
    "        token_embeddings.append(token_embedding.tolist())\n",
    "    \n",
    "    # Add the end of sequence token ID\n",
    "    token_ids.append(4) #[SEP]\n",
    "    token_ids.insert(0,3) #[CLS]\n",
    "    # Calculate average embedding for the entity tokens\n",
    "    avg_embedding = torch.mean(full_embeddings[start_index:start_index+len(entity_tokens['input_ids'])], dim=0).tolist()\n",
    "    \n",
    "    \n",
    "    rows.append({\n",
    "        'Entity': entity,\n",
    "        'Tokens': tokens,\n",
    "        'Token_IDs': token_ids,#token_ids[1:-1] w/ [CLS] and [SEP]\n",
    "        'Token_Embeddings': token_embeddings,\n",
    "        'Average_Embedding': avg_embedding,\n",
    "        'Target_Token_IDs': token_ids\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "# check if any element in the list contains '[UNK]'\n",
    "def contains_unk(token_list):\n",
    "    return any('[UNK]' in token for token in token_list)\n",
    "\n",
    "# Filter out rows where the Token column contains '[UNK]'\n",
    "df = df[~df['Tokens'].apply(contains_unk)]\n",
    "# Filter out rows that are longer than 15 Tokens \n",
    "df = df[df['Target_Token_IDs'].apply(len) <= 15]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 16\n",
      "Target_Token_IDs\n",
      "3      2115\n",
      "4     13098\n",
      "5     12997\n",
      "6      8888\n",
      "7      5205\n",
      "8      2500\n",
      "9      1037\n",
      "10      403\n",
      "11      149\n",
      "12       97\n",
      "13       36\n",
      "14        9\n",
      "15       10\n",
      "16        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# determine the longest sequence\n",
    "print(f\"max length: {df['Target_Token_IDs'].apply(len).max()}\")\n",
    "print(df['Target_Token_IDs'].apply(len).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_df = Path.cwd() / \"data\" / \"df_file.pkl\"\n",
    "\n",
    "# #Save the DataFrame to a pickle file\n",
    "#df.to_pickle(file_path_df)\n",
    "\n",
    "# #Load the DataFrame from the pickle file\n",
    "df= pd.read_pickle(file_path_df)\n",
    "df\n",
    "##### adding Special Tokens \n",
    "#df['Target_Token_IDs'] = df['Target_Token_IDs'].apply(lambda x: [3] + x)\n",
    "#df['Token_IDs'] = df['Token_IDs'].apply(lambda x: x[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the average embedding (input)\n",
    "        avg_embedding = torch.tensor(self.data.iloc[idx]['Average_Embedding'], dtype=torch.float32)\n",
    "        \n",
    "        # Get the token IDs (target)\n",
    "        token_ids = torch.tensor(self.data.iloc[idx]['Target_Token_IDs'], dtype=torch.int64)\n",
    "        \n",
    "        return avg_embedding, token_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad the target sequences\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack the inputs \n",
    "    inputs_stacked = torch.stack(inputs)\n",
    "    \n",
    "    return inputs_stacked, targets_padded\n",
    "\n",
    "# # Create the dataset\n",
    "# full_dataset = CustomDataset(df)\n",
    "\n",
    "# # Calculate split sizes\n",
    "# total_size = len(full_dataset)\n",
    "# train_size = int(0.8 * total_size)\n",
    "# val_size = int(0.1 * total_size)\n",
    "# test_size = total_size - train_size - val_size\n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Train loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 12])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.2255961894989014 16.7394962310791\n",
      "Target range: 0 26979\n",
      "\n",
      "Information Validation loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 9])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.132582426071167 17.16713523864746\n",
      "Target range: 0 26979\n",
      "\n",
      "Information Test loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 13])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.5452218055725098 16.662612915039062\n",
      "Target range: 0 26979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_train = Path.cwd() / \"data\" / \"train_dataset.pkl\"\n",
    "file_path_test = Path.cwd() / \"data\" / \"test_dataset.pkl\"\n",
    "file_path_val = Path.cwd() / \"data\" / \"val_dataset.pkl\"\n",
    "\n",
    "######################Save the Data to a pickle file\n",
    "# with open(file_path_train, 'wb') as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# with open(file_path_test, 'wb') as f:\n",
    "#     pickle.dump(val_dataset, f)\n",
    "\n",
    "# with open(file_path_val, 'wb') as f:\n",
    "#     pickle.dump(test_dataset, f)\n",
    "\n",
    "######################Load the Data from the pickle file\n",
    "with open(file_path_train, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_test, 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_val, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Create DataLoaders with collate function\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) ########### hyperparameter optimization batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#Infromation dataloaders\n",
    "for loader_name, loader in [(\"Train\", train_loader), (\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
    "    print(f\"\\nInformation {loader_name} loader:\")\n",
    "    for batch in loader:\n",
    "        print(\"Input shape:\", batch[0].shape)\n",
    "        print(\"Target shape:\", batch[1].shape)\n",
    "        print(\"Input dtype:\", batch[0].dtype)\n",
    "        print(\"Target dtype:\", batch[1].dtype)\n",
    "        print(\"Input range:\", batch[0].min().item(), batch[0].max().item())\n",
    "        print(\"Target range:\", batch[1].min().item(), batch[1].max().item())\n",
    "        break\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForMaskedLM\n",
    "import pandas as pd\n",
    "class CustomLSTMWithBERTMLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bert_model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        \n",
    "        self.bert_word_embeddings = self.bert_model.bert.embeddings.word_embeddings\n",
    "        self.bert_position_embeddings = self.bert_model.bert.embeddings.position_embeddings\n",
    "        self.bert_LayerNorm = self.bert_model.bert.embeddings.LayerNorm\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size + hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.bert_mlm_head = self.bert_model.cls\n",
    "        \n",
    "        # Remove these lines to allow fine-tuning\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        assert hidden_size == self.bert_model.config.hidden_size, \"LSTM hidden size must match BERT hidden size\"\n",
    "\n",
    "    def forward(self, x, prev_token_id=None, prev_token_position=None, h=None, c=None):\n",
    "        batch_size = x.size(0)\n",
    "        # Handle the case when there's no previous token\n",
    "        if prev_token_id is None:\n",
    "            dummy_prev_token = torch.zeros(batch_size, self.bert_model.config.hidden_size).to(x.device)\n",
    "            input_tensor = torch.cat([x.unsqueeze(1), dummy_prev_token.unsqueeze(1)], dim=2) \n",
    "        else:\n",
    "            # Get embedding for the previous token\n",
    "            token_embedding = self.bert_word_embeddings(prev_token_id).to(x.device)\n",
    "            position_embedding = self.bert_position_embeddings(prev_token_position).to(x.device)\n",
    "            combined_embeddings = token_embedding + position_embedding\n",
    "            prev_token_embedding = self.bert_LayerNorm(combined_embeddings).to(x.device)\n",
    "            # Concatenate the average embedding with the previous token embedding\n",
    "            input_tensor = torch.cat([x.unsqueeze(1), prev_token_embedding], dim=2)\n",
    "            \n",
    "        # Initialize hidden state and cell state if not provided\n",
    "        if h is None:\n",
    "            h = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        if c is None:\n",
    "            c = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        out, (h, c) = self.lstm(input_tensor, (h, c))\n",
    "        if self.training: # only use dropout druing training\n",
    "            out = self.dropout(out)\n",
    "        out = self.layer_norm(out)\n",
    "        mlm_output = self.bert_mlm_head(out)  # Shape: (batch_size, 1, vocab_size)\n",
    "        \n",
    "        return mlm_output, h, c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "# from torch.optim.lr_scheduler import StepLR  #or CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from sacrebleu import sentence_bleu # SacreBLEU is generally faster it can be used on the gpu\n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 768\n",
    "num_layers = 2 #### hyperparameter optimization\n",
    "num_epochs = 150\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01 # L2 regularization factor\n",
    "batch_size = 256\n",
    "# CrossEntropyLoss, expect the inputs in a specific format:\n",
    "# The predictions should be a 2D tensor of shape [N, C] where N is the number of samples and C is the number of classes (vocab size in this case).\n",
    "# The targets should be a 1D tensor of shape [N] containing the class indices.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "bert_model_name = 'bert-base-german-cased'\n",
    "model = CustomLSTMWithBERTMLM(input_size, hidden_size, num_layers, bert_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# scheduler = StepLR(optimizer, step_size=15, gamma=0.1)  # Decays the learning rate every 10 epochs by a factor of 0.1\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10) #  used defaults: threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False \n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sequence Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMWithBERTMLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, max_seq_length, bert_model_name):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        ####self.dropout = nn.Dropout(0.1)\n",
    "        ####self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        #### Remove these lines to allow fine-tuning\n",
    "        for param in self.bert_mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        assert hidden_size == self.bert_mlm.config.hidden_size, \"LSTM hidden size must match BERT hidden size\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Use the input to initialize the hidden state\n",
    "        h = x.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1) #shape:(num_layers, batch_size, hidden_size)\n",
    "        c = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        # dummy input for the LSTM\n",
    "        dummy_input = torch.zeros(batch_size, 1, self.lstm.hidden_size).to(x.device)\n",
    "        outputs = []\n",
    "        for i in range(self.max_seq_length):\n",
    "            out, (h, c) = self.lstm(dummy_input, (h, c))\n",
    "            ####out = self.dropout(out)\n",
    "            ####out = self.layer_norm(out)\n",
    "            mlm_output = self.bert_mlm.cls(out)\n",
    "            \n",
    "            outputs.append(mlm_output)\n",
    "        \n",
    "        output_tensor = torch.cat(outputs, dim=1)\n",
    "        return output_tensor # output shape: (batch_size, generated_seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log file with current date and time\n",
    "current_time_start_epoch = datetime.datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "log_path = Path.cwd() / \"log\" / f\"log_{current_time_start_epoch}.txt\"\n",
    "# log hyperparameters and model information\n",
    "with open(log_path, 'w') as log_file:\n",
    "    log_file.write(f\"Model Architecture: {model}\\n\\n\")\n",
    "    log_file.write(f\"BERT model: {bert_model_name}\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Input size: {input_size}\\n\")\n",
    "    log_file.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"Max sequence length: {max_seq_length}\\n\")\n",
    "    log_file.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"Loss function: CrossEntropyLoss\\n\")\n",
    "    log_file.write(f\"Number of epochs: {num_epochs}\\n\\n\")\n",
    "    \n",
    "best_model_path = Path.cwd() / \"data\" / \"best_model.pth\"\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        # inference with batch\n",
    "        outputs = model(inputs)\n",
    "        # processing of output and target for training\n",
    "        outputs = outputs[:, :targets.shape[1], :] # we only need to consider the outputs for which we have a target during training\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1)) # Reshape outputs from [batch_size, sequence_length, vocab_size] to be [batch_size * sequence_length, vocab_size]  \n",
    "        targets = targets.reshape(-1) # Reshape targets from [batch_size, sequence_length] to be [batch_size * sequence_length] \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad() # this resets the gradients of all parameters to zero from last batch\n",
    "        loss.backward() # computes the gradient of the loss with respect to each parameter of the model\n",
    "        optimizer.step() # updates the parameters based on the computed gradients\n",
    "    \n",
    "        epoch_train_losses.append(loss.item())\n",
    "   \n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Compute test loss\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs[:, :targets.shape[1], :]\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            epoch_test_losses.append(loss.item())\n",
    "    \n",
    "    avg_test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Save the model if the test loss is the best seen so far\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_test_loss,\n",
    "        }, best_model_path)\n",
    "        \n",
    "    # Log epoch information\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] - {current_time}\\n\")\n",
    "        log_file.write(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\\n\\n\")\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, marker='o', label='Test Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# get the best loss for the filename\n",
    "best_loss_str = f\"{best_test_loss:.4f}\".replace('.', '_')\n",
    "\n",
    "# Construct the file path\n",
    "plot_file_path = Path.cwd() / \"log\" / f\"loss_plot_{current_time_start_epoch}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_file_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#### Implement more comprehensive evaluation metrics (e.g., perplexity, BLEU score for generated text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if metric doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time metric improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each metric improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        # self.metric_min = np.inf  # when smaller is better\n",
    "        self.metric_min = - np.inf  # when bigger is better\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, metric, model, optimizer):\n",
    "\n",
    "        # score = -metric   # smaller is better\n",
    "        score = metric    # bigger is better\n",
    "               \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric, model, optimizer)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}\\n\\n')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metric, model, optimizer)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, metric, model, optimizer):\n",
    "        '''Saves model when metric improved.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Improved metric ({self.metric_min:.6f} --> {metric:.6f}).  Saving model ...\\n\\n')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metric': metric,\n",
    "        }, self.path)\n",
    "        self.metric_min = metric\n",
    "        \n",
    "def log_to_file(message, log_path):\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Train Loss: 23.1703, Test Loss: 4.1128 BLEU score word: 43.8824 BLEU score token: 68.3220\n",
      "\n",
      "\n",
      "Epoch [2/150], Train Loss: 14.7291, Test Loss: 4.0141 BLEU score word: 48.9049 BLEU score token: 70.8951\n",
      "\n",
      "\n",
      "Epoch [3/150], Train Loss: 11.4286, Test Loss: 3.9439 BLEU score word: 56.9390 BLEU score token: 76.2322\n",
      "\n",
      "\n",
      "Epoch [4/150], Train Loss: 8.6116, Test Loss: 3.6528 BLEU score word: 63.7831 BLEU score token: 80.5732\n",
      "\n",
      "\n",
      "Epoch [5/150], Train Loss: 5.6473, Test Loss: 3.6901 BLEU score word: 68.4574 BLEU score token: 83.0842\n",
      "\n",
      "\n",
      "Epoch [6/150], Train Loss: 4.1131, Test Loss: 3.8966 BLEU score word: 68.1884 BLEU score token: 83.2877\n",
      "\n",
      "\n",
      "Epoch [7/150], Train Loss: 3.2482, Test Loss: 3.6617 BLEU score word: 73.1968 BLEU score token: 85.6608\n",
      "\n",
      "\n",
      "Epoch [8/150], Train Loss: 2.5555, Test Loss: 3.7179 BLEU score word: 73.6710 BLEU score token: 85.8923\n",
      "\n",
      "\n",
      "Epoch [9/150], Train Loss: 1.9733, Test Loss: 4.0596 BLEU score word: 75.2831 BLEU score token: 86.9998\n",
      "\n",
      "\n",
      "Epoch [10/150], Train Loss: 1.5232, Test Loss: 3.7929 BLEU score word: 75.7723 BLEU score token: 87.3117\n",
      "\n",
      "\n",
      "Epoch [11/150], Train Loss: 1.3089, Test Loss: 3.7905 BLEU score word: 76.8165 BLEU score token: 87.7315\n",
      "\n",
      "\n",
      "Epoch [12/150], Train Loss: 1.3005, Test Loss: 3.5274 BLEU score word: 76.6696 BLEU score token: 87.6862\n",
      "\n",
      "\n",
      "Epoch [13/150], Train Loss: 1.7392, Test Loss: 3.8161 BLEU score word: 75.6847 BLEU score token: 86.7584\n",
      "\n",
      "\n",
      "Epoch [14/150], Train Loss: 1.7191, Test Loss: 4.2600 BLEU score word: 76.4806 BLEU score token: 87.4075\n",
      "\n",
      "\n",
      "Epoch [15/150], Train Loss: 1.6561, Test Loss: 3.9555 BLEU score word: 72.1201 BLEU score token: 84.6780\n",
      "\n",
      "\n",
      "Epoch [16/150], Train Loss: 1.7629, Test Loss: 3.5870 BLEU score word: 76.0222 BLEU score token: 87.1273\n",
      "\n",
      "\n",
      "Epoch [17/150], Train Loss: 1.4202, Test Loss: 3.7398 BLEU score word: 74.5976 BLEU score token: 86.4543\n",
      "\n",
      "\n",
      "Epoch [18/150], Train Loss: 1.4463, Test Loss: 4.1369 BLEU score word: 75.6069 BLEU score token: 86.8869\n",
      "\n",
      "\n",
      "Epoch [19/150], Train Loss: 1.4422, Test Loss: 4.0812 BLEU score word: 74.1456 BLEU score token: 85.8773\n",
      "\n",
      "\n",
      "Epoch [20/150], Train Loss: 1.1730, Test Loss: 4.3010 BLEU score word: 74.9186 BLEU score token: 86.3296\n",
      "\n",
      "\n",
      "Epoch [21/150], Train Loss: 0.9729, Test Loss: 3.8201 BLEU score word: 75.5194 BLEU score token: 86.9410\n",
      "\n",
      "\n",
      "Epoch [22/150], Train Loss: 1.1880, Test Loss: 3.6288 BLEU score word: 74.9848 BLEU score token: 86.1945\n",
      "\n",
      "\n",
      "Epoch [23/150], Train Loss: 0.6553, Test Loss: 3.6685 BLEU score word: 78.0111 BLEU score token: 88.2530\n",
      "\n",
      "\n",
      "Epoch [24/150], Train Loss: 0.2901, Test Loss: 3.6759 BLEU score word: 78.5983 BLEU score token: 88.6101\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_bleu(hypotheses, references):\n",
    "    return [sentence_bleu(hyp, [ref]).score for hyp, ref in zip(hypotheses, references)] # sentence_bleu() returns a BLEUScore object and the .score attribute gives you the final BLEU score as a float between 0 and 100.\n",
    "\n",
    "def batch_decode_tokens(sequences, tokenizer):\n",
    "    # Decode each sequence of token IDs into token strings\n",
    "    return [\" \".join(tokenizer.convert_ids_to_tokens(sequence)) for sequence in sequences]\n",
    "\n",
    "# Create log file with current date and time\n",
    "current_time_start_epoch = datetime.datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "log_path = Path.cwd() / \"log\" / f\"log_{current_time_start_epoch}.txt\"\n",
    "# log hyperparameters and model information\n",
    "with open(log_path, 'w') as log_file:\n",
    "    log_file.write(f\"Model Architecture: {model}\\n\\n\")\n",
    "    log_file.write(f\"BERT model: {bert_model_name}\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Input size: {input_size}\\n\")\n",
    "    log_file.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"weight decay: {weight_decay}\\n\")\n",
    "    log_file.write(f\"Loss function: CrossEntropyLoss\\n\")\n",
    "    log_file.write(f\"Number of epochs: {num_epochs}\\n\\n\")\n",
    "    \n",
    "# Initialize early stopping\n",
    "best_model_path = Path.cwd() / \"data\" / f\"best_model_{current_time_start_epoch}.pth\"\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True, path=best_model_path, trace_func=lambda msg: log_to_file(msg, log_path))\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_bleu_scores = []\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        batch_size, seq_len = targets.shape\n",
    "        h, c = None, None\n",
    "        prev_token, prev_position = None, None\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss = 0\n",
    "        \n",
    "        for t in range(seq_len - 1):\n",
    "            if t != 0:\n",
    "                # Previous token (use ground truth for training)\n",
    "                prev_token = targets[:, t-1].unsqueeze(1).to(device) \n",
    "                prev_position = torch.full((batch_size, 1), t-1, device=device)\n",
    "            # Forward pass\n",
    "            output, h, c = model(inputs, prev_token, prev_position, h, c)\n",
    "            # Compute loss\n",
    "            batch_loss = criterion(output.squeeze(1), targets[:, t])\n",
    "            total_batch_loss += batch_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_train_losses.append(total_batch_loss.item())\n",
    "        \n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Compute test loss\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    epoch_test_bleu_score_words = []\n",
    "    epoch_test_bleu_score_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch_size, seq_len = targets.shape\n",
    "            h, c = None, None\n",
    "            prev_token, prev_position = None, None\n",
    "            \n",
    "            generated_sequence = torch.zeros((batch_size, seq_len), device=device, dtype=torch.long)\n",
    "            total_loss = 0\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                # Forward pass\n",
    "                output, h, c = model(inputs, prev_token, prev_position, h, c)\n",
    "                \n",
    "                # Compute loss for current time step\n",
    "                step_loss = criterion(output.squeeze(1), targets[:, t])\n",
    "                total_loss += step_loss\n",
    "                \n",
    "                # Get the most likely next token\n",
    "                next_token = output.argmax(dim=-1)\n",
    "                generated_sequence[:, t] = next_token.squeeze(1)\n",
    "                \n",
    "                # Update for next iteration\n",
    "                prev_token = next_token\n",
    "                prev_position = torch.full((batch_size, 1), t, device=device)\n",
    "\n",
    "            # Calculate average loss for the sequence\n",
    "            batch_loss = total_loss / seq_len\n",
    "            epoch_test_losses.append(batch_loss.item())\n",
    "            \n",
    "            # Calculate BLEU score / BLEU uses n-grams to measure how many times n-gram matches occur between the generated text\n",
    "            # decode sequence into whole words\n",
    "            pred_words = tokenizer.batch_decode(generated_sequence, skip_special_tokens= False)\n",
    "            target_words = tokenizer.batch_decode(targets, skip_special_tokens= False)\n",
    "            # decode sequence into token\n",
    "            pred_tokens = batch_decode_tokens(generated_sequence, tokenizer)\n",
    "            target_tokens = batch_decode_tokens(targets, tokenizer)\n",
    "            # filter sequence until eos token //\n",
    "            pred_words = [pred_word[len(\"[CLS]\"):pred_word.index(\"[SEP]\")] if \"[SEP]\" in pred_word else pred_word[len(\"[CLS] \"):] for pred_word in pred_words]\n",
    "            target_words = [target_word[len(\"[CLS]\"):target_word.index(\"[SEP]\")] if \"[SEP]\" in target_word else target_word[len(\"[CLS] \"):] for target_word in target_words]\n",
    "            pred_tokens = [pred_token[:pred_token.index(\"[SEP]\")+ len(\"[SEP]\")] if \"[SEP]\" in pred_token else pred_token for pred_token in pred_tokens]\n",
    "            target_tokens = [target_token[:target_token.index(\"[SEP]\")+ len(\"[SEP]\")] if \"[SEP]\" in target_token else target_token for target_token in target_tokens]\n",
    "            # Calculate BLEU scores in batch\n",
    "            epoch_test_bleu_score_word = batch_bleu(pred_words, target_words)\n",
    "            epoch_test_bleu_score_token = batch_bleu(pred_tokens, target_tokens)\n",
    "            # save bleu scores\n",
    "            epoch_test_bleu_score_words.extend(epoch_test_bleu_score_word)\n",
    "            epoch_test_bleu_score_tokens.extend(epoch_test_bleu_score_token)\n",
    "\n",
    "    avg_test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    avg_test_bleu_score_word = sum(epoch_test_bleu_score_words) / len(epoch_test_bleu_score_words)\n",
    "    avg_test_bleu_score_token = sum(epoch_test_bleu_score_tokens) / len(epoch_test_bleu_score_tokens)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # increment scheduler at the end of each epoch\n",
    "    scheduler.step(avg_test_bleu_score_token)\n",
    "    \n",
    "    # Log epoch information\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] - {current_time} LR: {scheduler.get_last_lr()[0]}\\n\")\n",
    "        log_file.write(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f} BLEU score word: {avg_test_bleu_score_word:.4f} BLEU score token: {avg_test_bleu_score_token:.4f}\\n\\n\")\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f} BLEU score word: {avg_test_bleu_score_word:.4f} BLEU score token: {avg_test_bleu_score_token:.4f}\\n\\n\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(avg_test_bleu_score_token, model, optimizer)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        total_epoch = epoch\n",
    "        with open(log_path, 'a') as log_file:\n",
    "            log_file.write(\"Early stopping\\n\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, total_epoch + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, total_epoch + 1), test_losses, marker='o', label='Test Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# get the best loss for the filename\n",
    "best_loss_str = f\"{best_test_loss:.4f}\".replace('.', '_')\n",
    "\n",
    "# Construct the file path\n",
    "plot_file_path = Path.cwd() / \"log\" / f\"loss_plot_{current_time_start_epoch}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_file_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()  # for inference\n",
    "# to resume training, set the model train mode\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42000</th>\n",
       "      <td>Tall</td>\n",
       "      <td>[Tall]</td>\n",
       "      <td>[21935, 4]</td>\n",
       "      <td>[[-0.01224720198661089, 0.16867724061012268, -...</td>\n",
       "      <td>[-0.01224720198661089, 0.16867724061012268, -0...</td>\n",
       "      <td>[21935, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42001</th>\n",
       "      <td>Tam</td>\n",
       "      <td>[Tam]</td>\n",
       "      <td>[17524, 4]</td>\n",
       "      <td>[[0.16409598290920258, 0.793487548828125, -0.7...</td>\n",
       "      <td>[0.16409598290920258, 0.793487548828125, -0.71...</td>\n",
       "      <td>[17524, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42002</th>\n",
       "      <td>Tamar</td>\n",
       "      <td>[Tam, ##ar]</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "      <td>[[1.1463826894760132, 0.7378958463668823, 0.41...</td>\n",
       "      <td>[0.4127388000488281, 0.5559422373771667, -0.04...</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42003</th>\n",
       "      <td>Tamara</td>\n",
       "      <td>[Tam, ##ara]</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "      <td>[[0.774370014667511, 0.2208947241306305, 0.663...</td>\n",
       "      <td>[0.5452055931091309, 0.20736496150493622, 0.25...</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42004</th>\n",
       "      <td>Tamara Boros</td>\n",
       "      <td>[Tam, ##ara, Bor, ##os]</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "      <td>[[1.0219417810440063, 0.18805761635303497, 0.3...</td>\n",
       "      <td>[0.8827532529830933, 0.0814586728811264, 0.267...</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42005</th>\n",
       "      <td>Tamarine Tanasugarn</td>\n",
       "      <td>[Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.8510592579841614, -0.1594560146331787, 0.3...</td>\n",
       "      <td>[0.4551093578338623, -0.1477975845336914, 0.10...</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42006</th>\n",
       "      <td>Tamas</td>\n",
       "      <td>[Tam, ##as]</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "      <td>[[1.130900263786316, 0.7732470631599426, 0.357...</td>\n",
       "      <td>[0.6211729049682617, 0.5434377789497375, 0.150...</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42007</th>\n",
       "      <td>Tamas Ajan</td>\n",
       "      <td>[Tam, ##as, A, ##jan]</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "      <td>[[1.140594720840454, 1.101357102394104, 0.5228...</td>\n",
       "      <td>[0.2719385027885437, 0.47032666206359863, 0.33...</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42008</th>\n",
       "      <td>Tamas Szekeres</td>\n",
       "      <td>[Tam, ##as, Sz, ##ek, ##ere, ##s]</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "      <td>[[1.1150412559509277, 1.2008758783340454, 0.47...</td>\n",
       "      <td>[0.7783443927764893, 0.3929256498813629, 0.270...</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42009</th>\n",
       "      <td>Tambuyser</td>\n",
       "      <td>[Tam, ##bu, ##yse, ##r]</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "      <td>[[0.9211291670799255, 0.2917001247406006, -0.3...</td>\n",
       "      <td>[0.6440316438674927, 0.13280363380908966, -0.0...</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42010</th>\n",
       "      <td>Tamer</td>\n",
       "      <td>[Tam, ##er]</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "      <td>[[1.3231805562973022, 0.5179563164710999, 0.93...</td>\n",
       "      <td>[0.7314438819885254, 0.7326130867004395, 0.298...</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42011</th>\n",
       "      <td>Taminiaux</td>\n",
       "      <td>[Tam, ##ini, ##aux]</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "      <td>[[0.7550164461135864, 0.8477469682693481, -0.4...</td>\n",
       "      <td>[0.31464725732803345, 0.3305900990962982, -0.5...</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42012</th>\n",
       "      <td>Tamira</td>\n",
       "      <td>[Tam, ##ira]</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "      <td>[[0.7360913157463074, 0.31316858530044556, 0.5...</td>\n",
       "      <td>[0.37108471989631653, -0.010409042239189148, 0...</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42013</th>\n",
       "      <td>Tamme</td>\n",
       "      <td>[Tam, ##me]</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "      <td>[[0.8440600037574768, 0.8673146963119507, 0.84...</td>\n",
       "      <td>[0.4066227674484253, 0.6386289000511169, 0.443...</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42014</th>\n",
       "      <td>Tamminga</td>\n",
       "      <td>[Tam, ##min, ##ga]</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "      <td>[[1.234696626663208, -0.8913580179214478, 0.67...</td>\n",
       "      <td>[0.4619470536708832, -0.8386526703834534, 0.32...</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42015</th>\n",
       "      <td>Tammo</td>\n",
       "      <td>[Tam, ##mo]</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "      <td>[[0.5058582425117493, 0.5409940481185913, -0.2...</td>\n",
       "      <td>[0.3185293972492218, 0.021780773997306824, -0....</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42016</th>\n",
       "      <td>Tammy</td>\n",
       "      <td>[Tam, ##my]</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "      <td>[[0.910221517086029, 1.2008426189422607, 0.694...</td>\n",
       "      <td>[0.47359079122543335, 0.8301605582237244, 0.23...</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42017</th>\n",
       "      <td>Tan</td>\n",
       "      <td>[Tan]</td>\n",
       "      <td>[15297, 4]</td>\n",
       "      <td>[[-0.293621301651001, 0.779782772064209, -0.14...</td>\n",
       "      <td>[-0.293621301651001, 0.779782772064209, -0.144...</td>\n",
       "      <td>[15297, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42018</th>\n",
       "      <td>Tanaka</td>\n",
       "      <td>[Tan, ##aka]</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "      <td>[[-0.6399266123771667, 0.15942348539829254, 1....</td>\n",
       "      <td>[-0.10410656034946442, -0.09060264378786087, 0...</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42019</th>\n",
       "      <td>Tanasugarn</td>\n",
       "      <td>[Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.2947482764720917, 0.4754425287246704, 0.42...</td>\n",
       "      <td>[0.057072967290878296, -0.4458737373352051, 0....</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42020</th>\n",
       "      <td>Tandjung</td>\n",
       "      <td>[Tan, ##d, ##ju, ##n, ##g]</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "      <td>[[-0.4749807119369507, 0.528508722782135, 0.11...</td>\n",
       "      <td>[0.15352052450180054, -0.29984623193740845, -0...</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42021</th>\n",
       "      <td>Tandy</td>\n",
       "      <td>[Tan, ##dy]</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "      <td>[[-0.8069059252738953, 0.6859312057495117, 0.4...</td>\n",
       "      <td>[-0.3132078945636749, 0.3191651403903961, 0.08...</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42022</th>\n",
       "      <td>Tang</td>\n",
       "      <td>[Tan, ##g]</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "      <td>[[0.033787306398153305, -0.24927137792110443, ...</td>\n",
       "      <td>[0.15189029276371002, -0.43078309297561646, -0...</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42023</th>\n",
       "      <td>Tanghe</td>\n",
       "      <td>[Tan, ##gh, ##e]</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "      <td>[[-0.07595154643058777, 0.12992946803569794, 0...</td>\n",
       "      <td>[-0.23548705875873566, -0.060995280742645264, ...</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42024</th>\n",
       "      <td>Tangs</td>\n",
       "      <td>[Tan, ##gs]</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "      <td>[[0.13123495876789093, -0.16866567730903625, 0...</td>\n",
       "      <td>[0.12566909193992615, -0.07040099054574966, 0....</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42025</th>\n",
       "      <td>Tania</td>\n",
       "      <td>[Tan, ##ia]</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "      <td>[[0.21963322162628174, 1.1262986660003662, 0.7...</td>\n",
       "      <td>[0.05399937182664871, 0.3011305630207062, 0.20...</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42026</th>\n",
       "      <td>Tania Kloeck</td>\n",
       "      <td>[Tan, ##ia, Kl, ##oe, ##ck]</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "      <td>[[0.28413882851600647, 1.5511207580566406, 0.2...</td>\n",
       "      <td>[0.4187382161617279, 0.2899208664894104, -0.14...</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42027</th>\n",
       "      <td>Tania Medo</td>\n",
       "      <td>[Tan, ##ia, Med, ##o]</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "      <td>[[0.5423315167427063, 1.4148615598678589, 0.79...</td>\n",
       "      <td>[0.3000876307487488, 0.23904681205749512, 0.40...</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42028</th>\n",
       "      <td>Tania Polak</td>\n",
       "      <td>[Tan, ##ia, Pol, ##ak]</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "      <td>[[0.5157328248023987, 1.47681725025177, 0.6004...</td>\n",
       "      <td>[0.30176252126693726, 0.2940174341201782, -0.0...</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42029</th>\n",
       "      <td>Tania Poppe</td>\n",
       "      <td>[Tan, ##ia, Pop, ##pe]</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "      <td>[[0.4346654713153839, 1.436200499534607, 0.319...</td>\n",
       "      <td>[0.2676783800125122, 0.15604820847511292, 0.17...</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42030</th>\n",
       "      <td>Tania Prinsier</td>\n",
       "      <td>[Tan, ##ia, Prin, ##sie, ##r]</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "      <td>[[0.5102294683456421, 1.3143318891525269, 0.30...</td>\n",
       "      <td>[0.7683398127555847, 0.12537333369255066, 0.10...</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42031</th>\n",
       "      <td>Taniguchi</td>\n",
       "      <td>[Tan, ##ig, ##uch, ##i]</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "      <td>[[-0.5881728529930115, 1.0786540508270264, 0.7...</td>\n",
       "      <td>[-0.03565274178981781, 0.24008022248744965, -0...</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42032</th>\n",
       "      <td>Tanja</td>\n",
       "      <td>[Tan, ##ja]</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "      <td>[[-0.20097672939300537, 0.6485582590103149, 0....</td>\n",
       "      <td>[-0.048050444573163986, 0.1633566915988922, 0....</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42033</th>\n",
       "      <td>Tanne</td>\n",
       "      <td>[Tan, ##ne]</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "      <td>[[-0.363036572933197, 0.45801427960395813, 0.0...</td>\n",
       "      <td>[0.29078078269958496, 0.022456184029579163, -0...</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42034</th>\n",
       "      <td>Tanneke</td>\n",
       "      <td>[Tan, ##ne, ##ke]</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "      <td>[[-0.536466121673584, 0.41328921914100647, 0.6...</td>\n",
       "      <td>[0.046449288725852966, 0.1203099712729454, -0....</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42035</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>[Tan, ##ner]</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "      <td>[[-0.38445645570755005, 0.7667683959007263, 1....</td>\n",
       "      <td>[0.06236431002616882, 0.5567158460617065, 0.40...</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42036</th>\n",
       "      <td>Tanno</td>\n",
       "      <td>[Tan, ##no]</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "      <td>[[-0.26311466097831726, 0.5683020353317261, 0....</td>\n",
       "      <td>[0.1073189526796341, 0.16556155681610107, 0.02...</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42037</th>\n",
       "      <td>Tanny</td>\n",
       "      <td>[Tan, ##ny]</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "      <td>[[-0.7399507761001587, 0.5193020701408386, 0.4...</td>\n",
       "      <td>[-0.14292527735233307, 0.29106348752975464, 0....</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42038</th>\n",
       "      <td>Tansens</td>\n",
       "      <td>[Tan, ##sens]</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "      <td>[[-1.0175524950027466, 0.42136335372924805, 1....</td>\n",
       "      <td>[-0.522026538848877, 0.5173361301422119, 0.640...</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42039</th>\n",
       "      <td>Tansikoezjina</td>\n",
       "      <td>[Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "      <td>[[-0.15889112651348114, 0.5353723764419556, 1....</td>\n",
       "      <td>[0.16954562067985535, 0.22987063229084015, 0.5...</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42040</th>\n",
       "      <td>Tansykkoezina</td>\n",
       "      <td>[Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "      <td>[[-0.5009126663208008, 0.6513158679008484, 1.4...</td>\n",
       "      <td>[0.06688310205936432, 0.5194838643074036, 0.59...</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42041</th>\n",
       "      <td>Tanya</td>\n",
       "      <td>[Tan, ##ya]</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "      <td>[[-0.27348271012306213, 0.7305521965026855, 0....</td>\n",
       "      <td>[-0.15534719824790955, 0.21959252655506134, 0....</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42042</th>\n",
       "      <td>Tao</td>\n",
       "      <td>[Ta, ##o]</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "      <td>[[0.6866366267204285, -0.32815611362457275, -0...</td>\n",
       "      <td>[0.5244305729866028, -0.42992928624153137, -0....</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42043</th>\n",
       "      <td>Tap</td>\n",
       "      <td>[Ta, ##p]</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "      <td>[[0.0655214861035347, -0.1980666071176529, 0.0...</td>\n",
       "      <td>[-0.09740108251571655, 0.21112042665481567, -0...</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42044</th>\n",
       "      <td>Tapias</td>\n",
       "      <td>[Ta, ##p, ##ias]</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "      <td>[[0.3809676170349121, 0.13550198078155518, -0....</td>\n",
       "      <td>[0.33656492829322815, 0.6334041953086853, -0.5...</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42045</th>\n",
       "      <td>Tappert</td>\n",
       "      <td>[Ta, ##pper, ##t]</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "      <td>[[-0.004451925400644541, 0.3471279740333557, 0...</td>\n",
       "      <td>[0.1981954127550125, 0.2921486794948578, -0.06...</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42046</th>\n",
       "      <td>Tar</td>\n",
       "      <td>[Tar]</td>\n",
       "      <td>[10397, 4]</td>\n",
       "      <td>[[0.12060574442148209, 0.9793983697891235, -0....</td>\n",
       "      <td>[0.12060574442148209, 0.9793983697891235, -0.9...</td>\n",
       "      <td>[10397, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42047</th>\n",
       "      <td>Tara</td>\n",
       "      <td>[Tar, ##a]</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "      <td>[[0.21004708111286163, 1.0180401802062988, -0....</td>\n",
       "      <td>[0.07090392708778381, 0.5829620957374573, -0.0...</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42048</th>\n",
       "      <td>Tarabini</td>\n",
       "      <td>[Tar, ##ab, ##ini]</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "      <td>[[-0.2942725419998169, 0.550433337688446, -0.1...</td>\n",
       "      <td>[-0.24213707447052002, 0.6941204071044922, 0.1...</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42049</th>\n",
       "      <td>Tarallo</td>\n",
       "      <td>[Tar, ##all, ##o]</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "      <td>[[-0.028452834114432335, -0.6263936161994934, ...</td>\n",
       "      <td>[0.09504646807909012, -0.5627711415290833, 0.3...</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Entity                                      Tokens  \\\n",
       "42000                 Tall                                      [Tall]   \n",
       "42001                  Tam                                       [Tam]   \n",
       "42002                Tamar                                 [Tam, ##ar]   \n",
       "42003               Tamara                                [Tam, ##ara]   \n",
       "42004         Tamara Boros                     [Tam, ##ara, Bor, ##os]   \n",
       "42005  Tamarine Tanasugarn  [Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]   \n",
       "42006                Tamas                                 [Tam, ##as]   \n",
       "42007           Tamas Ajan                       [Tam, ##as, A, ##jan]   \n",
       "42008       Tamas Szekeres           [Tam, ##as, Sz, ##ek, ##ere, ##s]   \n",
       "42009            Tambuyser                     [Tam, ##bu, ##yse, ##r]   \n",
       "42010                Tamer                                 [Tam, ##er]   \n",
       "42011            Taminiaux                         [Tam, ##ini, ##aux]   \n",
       "42012               Tamira                                [Tam, ##ira]   \n",
       "42013                Tamme                                 [Tam, ##me]   \n",
       "42014             Tamminga                          [Tam, ##min, ##ga]   \n",
       "42015                Tammo                                 [Tam, ##mo]   \n",
       "42016                Tammy                                 [Tam, ##my]   \n",
       "42017                  Tan                                       [Tan]   \n",
       "42018               Tanaka                                [Tan, ##aka]   \n",
       "42019           Tanasugarn                    [Tan, ##as, ##ug, ##arn]   \n",
       "42020             Tandjung                  [Tan, ##d, ##ju, ##n, ##g]   \n",
       "42021                Tandy                                 [Tan, ##dy]   \n",
       "42022                 Tang                                  [Tan, ##g]   \n",
       "42023               Tanghe                            [Tan, ##gh, ##e]   \n",
       "42024                Tangs                                 [Tan, ##gs]   \n",
       "42025                Tania                                 [Tan, ##ia]   \n",
       "42026         Tania Kloeck                 [Tan, ##ia, Kl, ##oe, ##ck]   \n",
       "42027           Tania Medo                       [Tan, ##ia, Med, ##o]   \n",
       "42028          Tania Polak                      [Tan, ##ia, Pol, ##ak]   \n",
       "42029          Tania Poppe                      [Tan, ##ia, Pop, ##pe]   \n",
       "42030       Tania Prinsier               [Tan, ##ia, Prin, ##sie, ##r]   \n",
       "42031            Taniguchi                     [Tan, ##ig, ##uch, ##i]   \n",
       "42032                Tanja                                 [Tan, ##ja]   \n",
       "42033                Tanne                                 [Tan, ##ne]   \n",
       "42034              Tanneke                           [Tan, ##ne, ##ke]   \n",
       "42035               Tanner                                [Tan, ##ner]   \n",
       "42036                Tanno                                 [Tan, ##no]   \n",
       "42037                Tanny                                 [Tan, ##ny]   \n",
       "42038              Tansens                               [Tan, ##sens]   \n",
       "42039        Tansikoezjina     [Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]   \n",
       "42040        Tansykkoezina     [Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]   \n",
       "42041                Tanya                                 [Tan, ##ya]   \n",
       "42042                  Tao                                   [Ta, ##o]   \n",
       "42043                  Tap                                   [Ta, ##p]   \n",
       "42044               Tapias                            [Ta, ##p, ##ias]   \n",
       "42045              Tappert                           [Ta, ##pper, ##t]   \n",
       "42046                  Tar                                       [Tar]   \n",
       "42047                 Tara                                  [Tar, ##a]   \n",
       "42048             Tarabini                          [Tar, ##ab, ##ini]   \n",
       "42049              Tarallo                           [Tar, ##all, ##o]   \n",
       "\n",
       "                                               Token_IDs  \\\n",
       "42000                                         [21935, 4]   \n",
       "42001                                         [17524, 4]   \n",
       "42002                                     [17524, 33, 4]   \n",
       "42003                                   [17524, 3738, 4]   \n",
       "42004                        [17524, 3738, 3888, 224, 4]   \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]   \n",
       "42006                                     [17524, 45, 4]   \n",
       "42007                          [17524, 45, 32, 10761, 4]   \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]   \n",
       "42009                    [17524, 13874, 18940, 26900, 4]   \n",
       "42010                                      [17524, 6, 4]   \n",
       "42011                             [17524, 5381, 8805, 4]   \n",
       "42012                                   [17524, 7872, 4]   \n",
       "42013                                    [17524, 373, 4]   \n",
       "42014                               [17524, 734, 529, 4]   \n",
       "42015                                   [17524, 4359, 4]   \n",
       "42016                                  [17524, 16601, 4]   \n",
       "42017                                         [15297, 4]   \n",
       "42018                                  [15297, 10550, 4]   \n",
       "42019                          [15297, 45, 389, 7074, 4]   \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]   \n",
       "42021                                   [15297, 6484, 4]   \n",
       "42022                                  [15297, 26908, 4]   \n",
       "42023                           [15297, 13901, 26897, 4]   \n",
       "42024                                    [15297, 753, 4]   \n",
       "42025                                    [15297, 544, 4]   \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]   \n",
       "42027                       [15297, 544, 1371, 26910, 4]   \n",
       "42028                          [15297, 544, 984, 464, 4]   \n",
       "42029                        [15297, 544, 4528, 3500, 4]   \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]   \n",
       "42031                         [15297, 80, 108, 26899, 4]   \n",
       "42032                                   [15297, 3171, 4]   \n",
       "42033                                    [15297, 175, 4]   \n",
       "42034                               [15297, 175, 772, 4]   \n",
       "42035                                    [15297, 344, 4]   \n",
       "42036                                   [15297, 9706, 4]   \n",
       "42037                                   [15297, 9381, 4]   \n",
       "42038                                   [15297, 8531, 4]   \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]   \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]   \n",
       "42041                                   [15297, 5630, 4]   \n",
       "42042                                   [2127, 26910, 4]   \n",
       "42043                                   [2127, 26920, 4]   \n",
       "42044                             [2127, 26920, 3019, 4]   \n",
       "42045                             [2127, 9804, 26901, 4]   \n",
       "42046                                         [10397, 4]   \n",
       "42047                                  [10397, 26903, 4]   \n",
       "42048                              [10397, 228, 5381, 4]   \n",
       "42049                             [10397, 211, 26910, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "42000  [[-0.01224720198661089, 0.16867724061012268, -...   \n",
       "42001  [[0.16409598290920258, 0.793487548828125, -0.7...   \n",
       "42002  [[1.1463826894760132, 0.7378958463668823, 0.41...   \n",
       "42003  [[0.774370014667511, 0.2208947241306305, 0.663...   \n",
       "42004  [[1.0219417810440063, 0.18805761635303497, 0.3...   \n",
       "42005  [[0.8510592579841614, -0.1594560146331787, 0.3...   \n",
       "42006  [[1.130900263786316, 0.7732470631599426, 0.357...   \n",
       "42007  [[1.140594720840454, 1.101357102394104, 0.5228...   \n",
       "42008  [[1.1150412559509277, 1.2008758783340454, 0.47...   \n",
       "42009  [[0.9211291670799255, 0.2917001247406006, -0.3...   \n",
       "42010  [[1.3231805562973022, 0.5179563164710999, 0.93...   \n",
       "42011  [[0.7550164461135864, 0.8477469682693481, -0.4...   \n",
       "42012  [[0.7360913157463074, 0.31316858530044556, 0.5...   \n",
       "42013  [[0.8440600037574768, 0.8673146963119507, 0.84...   \n",
       "42014  [[1.234696626663208, -0.8913580179214478, 0.67...   \n",
       "42015  [[0.5058582425117493, 0.5409940481185913, -0.2...   \n",
       "42016  [[0.910221517086029, 1.2008426189422607, 0.694...   \n",
       "42017  [[-0.293621301651001, 0.779782772064209, -0.14...   \n",
       "42018  [[-0.6399266123771667, 0.15942348539829254, 1....   \n",
       "42019  [[0.2947482764720917, 0.4754425287246704, 0.42...   \n",
       "42020  [[-0.4749807119369507, 0.528508722782135, 0.11...   \n",
       "42021  [[-0.8069059252738953, 0.6859312057495117, 0.4...   \n",
       "42022  [[0.033787306398153305, -0.24927137792110443, ...   \n",
       "42023  [[-0.07595154643058777, 0.12992946803569794, 0...   \n",
       "42024  [[0.13123495876789093, -0.16866567730903625, 0...   \n",
       "42025  [[0.21963322162628174, 1.1262986660003662, 0.7...   \n",
       "42026  [[0.28413882851600647, 1.5511207580566406, 0.2...   \n",
       "42027  [[0.5423315167427063, 1.4148615598678589, 0.79...   \n",
       "42028  [[0.5157328248023987, 1.47681725025177, 0.6004...   \n",
       "42029  [[0.4346654713153839, 1.436200499534607, 0.319...   \n",
       "42030  [[0.5102294683456421, 1.3143318891525269, 0.30...   \n",
       "42031  [[-0.5881728529930115, 1.0786540508270264, 0.7...   \n",
       "42032  [[-0.20097672939300537, 0.6485582590103149, 0....   \n",
       "42033  [[-0.363036572933197, 0.45801427960395813, 0.0...   \n",
       "42034  [[-0.536466121673584, 0.41328921914100647, 0.6...   \n",
       "42035  [[-0.38445645570755005, 0.7667683959007263, 1....   \n",
       "42036  [[-0.26311466097831726, 0.5683020353317261, 0....   \n",
       "42037  [[-0.7399507761001587, 0.5193020701408386, 0.4...   \n",
       "42038  [[-1.0175524950027466, 0.42136335372924805, 1....   \n",
       "42039  [[-0.15889112651348114, 0.5353723764419556, 1....   \n",
       "42040  [[-0.5009126663208008, 0.6513158679008484, 1.4...   \n",
       "42041  [[-0.27348271012306213, 0.7305521965026855, 0....   \n",
       "42042  [[0.6866366267204285, -0.32815611362457275, -0...   \n",
       "42043  [[0.0655214861035347, -0.1980666071176529, 0.0...   \n",
       "42044  [[0.3809676170349121, 0.13550198078155518, -0....   \n",
       "42045  [[-0.004451925400644541, 0.3471279740333557, 0...   \n",
       "42046  [[0.12060574442148209, 0.9793983697891235, -0....   \n",
       "42047  [[0.21004708111286163, 1.0180401802062988, -0....   \n",
       "42048  [[-0.2942725419998169, 0.550433337688446, -0.1...   \n",
       "42049  [[-0.028452834114432335, -0.6263936161994934, ...   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "42000  [-0.01224720198661089, 0.16867724061012268, -0...   \n",
       "42001  [0.16409598290920258, 0.793487548828125, -0.71...   \n",
       "42002  [0.4127388000488281, 0.5559422373771667, -0.04...   \n",
       "42003  [0.5452055931091309, 0.20736496150493622, 0.25...   \n",
       "42004  [0.8827532529830933, 0.0814586728811264, 0.267...   \n",
       "42005  [0.4551093578338623, -0.1477975845336914, 0.10...   \n",
       "42006  [0.6211729049682617, 0.5434377789497375, 0.150...   \n",
       "42007  [0.2719385027885437, 0.47032666206359863, 0.33...   \n",
       "42008  [0.7783443927764893, 0.3929256498813629, 0.270...   \n",
       "42009  [0.6440316438674927, 0.13280363380908966, -0.0...   \n",
       "42010  [0.7314438819885254, 0.7326130867004395, 0.298...   \n",
       "42011  [0.31464725732803345, 0.3305900990962982, -0.5...   \n",
       "42012  [0.37108471989631653, -0.010409042239189148, 0...   \n",
       "42013  [0.4066227674484253, 0.6386289000511169, 0.443...   \n",
       "42014  [0.4619470536708832, -0.8386526703834534, 0.32...   \n",
       "42015  [0.3185293972492218, 0.021780773997306824, -0....   \n",
       "42016  [0.47359079122543335, 0.8301605582237244, 0.23...   \n",
       "42017  [-0.293621301651001, 0.779782772064209, -0.144...   \n",
       "42018  [-0.10410656034946442, -0.09060264378786087, 0...   \n",
       "42019  [0.057072967290878296, -0.4458737373352051, 0....   \n",
       "42020  [0.15352052450180054, -0.29984623193740845, -0...   \n",
       "42021  [-0.3132078945636749, 0.3191651403903961, 0.08...   \n",
       "42022  [0.15189029276371002, -0.43078309297561646, -0...   \n",
       "42023  [-0.23548705875873566, -0.060995280742645264, ...   \n",
       "42024  [0.12566909193992615, -0.07040099054574966, 0....   \n",
       "42025  [0.05399937182664871, 0.3011305630207062, 0.20...   \n",
       "42026  [0.4187382161617279, 0.2899208664894104, -0.14...   \n",
       "42027  [0.3000876307487488, 0.23904681205749512, 0.40...   \n",
       "42028  [0.30176252126693726, 0.2940174341201782, -0.0...   \n",
       "42029  [0.2676783800125122, 0.15604820847511292, 0.17...   \n",
       "42030  [0.7683398127555847, 0.12537333369255066, 0.10...   \n",
       "42031  [-0.03565274178981781, 0.24008022248744965, -0...   \n",
       "42032  [-0.048050444573163986, 0.1633566915988922, 0....   \n",
       "42033  [0.29078078269958496, 0.022456184029579163, -0...   \n",
       "42034  [0.046449288725852966, 0.1203099712729454, -0....   \n",
       "42035  [0.06236431002616882, 0.5567158460617065, 0.40...   \n",
       "42036  [0.1073189526796341, 0.16556155681610107, 0.02...   \n",
       "42037  [-0.14292527735233307, 0.29106348752975464, 0....   \n",
       "42038  [-0.522026538848877, 0.5173361301422119, 0.640...   \n",
       "42039  [0.16954562067985535, 0.22987063229084015, 0.5...   \n",
       "42040  [0.06688310205936432, 0.5194838643074036, 0.59...   \n",
       "42041  [-0.15534719824790955, 0.21959252655506134, 0....   \n",
       "42042  [0.5244305729866028, -0.42992928624153137, -0....   \n",
       "42043  [-0.09740108251571655, 0.21112042665481567, -0...   \n",
       "42044  [0.33656492829322815, 0.6334041953086853, -0.5...   \n",
       "42045  [0.1981954127550125, 0.2921486794948578, -0.06...   \n",
       "42046  [0.12060574442148209, 0.9793983697891235, -0.9...   \n",
       "42047  [0.07090392708778381, 0.5829620957374573, -0.0...   \n",
       "42048  [-0.24213707447052002, 0.6941204071044922, 0.1...   \n",
       "42049  [0.09504646807909012, -0.5627711415290833, 0.3...   \n",
       "\n",
       "                                        Target_Token_IDs  \n",
       "42000                                         [21935, 4]  \n",
       "42001                                         [17524, 4]  \n",
       "42002                                     [17524, 33, 4]  \n",
       "42003                                   [17524, 3738, 4]  \n",
       "42004                        [17524, 3738, 3888, 224, 4]  \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]  \n",
       "42006                                     [17524, 45, 4]  \n",
       "42007                          [17524, 45, 32, 10761, 4]  \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]  \n",
       "42009                    [17524, 13874, 18940, 26900, 4]  \n",
       "42010                                      [17524, 6, 4]  \n",
       "42011                             [17524, 5381, 8805, 4]  \n",
       "42012                                   [17524, 7872, 4]  \n",
       "42013                                    [17524, 373, 4]  \n",
       "42014                               [17524, 734, 529, 4]  \n",
       "42015                                   [17524, 4359, 4]  \n",
       "42016                                  [17524, 16601, 4]  \n",
       "42017                                         [15297, 4]  \n",
       "42018                                  [15297, 10550, 4]  \n",
       "42019                          [15297, 45, 389, 7074, 4]  \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]  \n",
       "42021                                   [15297, 6484, 4]  \n",
       "42022                                  [15297, 26908, 4]  \n",
       "42023                           [15297, 13901, 26897, 4]  \n",
       "42024                                    [15297, 753, 4]  \n",
       "42025                                    [15297, 544, 4]  \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]  \n",
       "42027                       [15297, 544, 1371, 26910, 4]  \n",
       "42028                          [15297, 544, 984, 464, 4]  \n",
       "42029                        [15297, 544, 4528, 3500, 4]  \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]  \n",
       "42031                         [15297, 80, 108, 26899, 4]  \n",
       "42032                                   [15297, 3171, 4]  \n",
       "42033                                    [15297, 175, 4]  \n",
       "42034                               [15297, 175, 772, 4]  \n",
       "42035                                    [15297, 344, 4]  \n",
       "42036                                   [15297, 9706, 4]  \n",
       "42037                                   [15297, 9381, 4]  \n",
       "42038                                   [15297, 8531, 4]  \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]  \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]  \n",
       "42041                                   [15297, 5630, 4]  \n",
       "42042                                   [2127, 26910, 4]  \n",
       "42043                                   [2127, 26920, 4]  \n",
       "42044                             [2127, 26920, 3019, 4]  \n",
       "42045                             [2127, 9804, 26901, 4]  \n",
       "42046                                         [10397, 4]  \n",
       "42047                                  [10397, 26903, 4]  \n",
       "42048                              [10397, 228, 5381, 4]  \n",
       "42049                             [10397, 211, 26910, 4]  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[42000:42050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# # end inference if this token is predicted\n",
    "# eos_token_id = tokenizer.convert_tokens_to_ids('[SEP]')  # EOS token\n",
    "\n",
    "# Example forward pass\n",
    "#input_tensor = torch.randn(1,768)\n",
    "\n",
    "tensor = torch.tensor(df[\"Average_Embedding\"][42042]).unsqueeze(0) #159\n",
    "# Generate Gaussian noise\n",
    "noise_level = 0.6\n",
    "noise = torch.randn_like(tensor) * noise_level\n",
    "# Add noise to tensor\n",
    "input_tensor = tensor + noise\n",
    "\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "for batch_idx in range(output_tensor.size(0)):\n",
    "    for position in range(output_tensor.size(1)):  # Iterate over sequence length\n",
    "        position_probs = output_tensor[batch_idx, position, :]  # Get probabilities for this position\n",
    "        \n",
    "        # Get top 5 probabilities\n",
    "        top_5_values, top_5_indices = torch.topk(position_probs, 1, dim=0)\n",
    "        \n",
    "        print(f\"Position {position}:\")\n",
    "        for i, (value, index) in enumerate(zip(top_5_values, top_5_indices), 1):\n",
    "            token = tokenizer.convert_ids_to_tokens([index.item()])[0]\n",
    "            probability = value.item()\n",
    "            print(f\"  {i}. {token} (probability: {probability:.4f})\")\n",
    "        print()\n",
    "########################################################################\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Example forward pass\n",
    "input_tensor = torch.randn(2,768)\n",
    "\n",
    "\n",
    "input_id = torch.tensor([[0],[0]])    # Shape: [2, 1]\n",
    "position_id = torch.tensor([[0],[0]])  # Shape: [2, 1]\n",
    "\n",
    "output_tensor,h,c = model(input_tensor,input_id,position_id) # output size:[2, 1, 30000] # h size: [2, 1, 768]# c size:[2, 1, 768]\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "for batch_idx in range(inputs.size(0)):\n",
    "        position_probs = output_tensor[batch_idx, 0, :]  # Get probabilities for this position\n",
    "        # Get top 5 probabilities\n",
    "        top_values, top_indices = torch.topk(position_probs, 1, dim=0)\n",
    "        \n",
    "        for i, (value, index) in enumerate(zip(top_values, top_indices), 1):\n",
    "            token = tokenizer.convert_ids_to_tokens([index.item()])[0]\n",
    "            probability = value.item()\n",
    "            print(f\"  {i}. {token} (probability: {probability:.4f})\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(df[\"Average_Embedding\"][8]).unsqueeze(0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
