{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[unused3001]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '##er',\n",
       " '##en',\n",
       " '##ch',\n",
       " 'd',\n",
       " '##ei',\n",
       " '##un',\n",
       " '##ie',\n",
       " '##st',\n",
       " '##in',\n",
       " '##an',\n",
       " '##es',\n",
       " '##ein',\n",
       " 'a',\n",
       " 's',\n",
       " '##ich',\n",
       " 'der',\n",
       " '##ge',\n",
       " '##on',\n",
       " 'S',\n",
       " 'w',\n",
       " '##te',\n",
       " '##ung',\n",
       " '##sch',\n",
       " '##und',\n",
       " 'die',\n",
       " 'v',\n",
       " 'A',\n",
       " '##ar',\n",
       " '##or',\n",
       " 'B',\n",
       " 'D',\n",
       " '##ur',\n",
       " '##it',\n",
       " 'ein',\n",
       " 'b',\n",
       " '##ll',\n",
       " 'und',\n",
       " '##at',\n",
       " 'z',\n",
       " '##as',\n",
       " 'i',\n",
       " '##ten',\n",
       " '##hr',\n",
       " '##em',\n",
       " 'in',\n",
       " '##us',\n",
       " '##gen',\n",
       " 'n',\n",
       " '##al',\n",
       " 'E',\n",
       " 'M',\n",
       " '##de',\n",
       " 'K',\n",
       " 'm',\n",
       " '##ter',\n",
       " 'G',\n",
       " '1',\n",
       " '##uf',\n",
       " 'F',\n",
       " '##den',\n",
       " '##der',\n",
       " 'er',\n",
       " '##lich',\n",
       " 'f',\n",
       " 'h',\n",
       " '##ach',\n",
       " 'P',\n",
       " '##ber',\n",
       " 'V',\n",
       " '##et',\n",
       " 'R',\n",
       " '##el',\n",
       " '##eit',\n",
       " 'W',\n",
       " '##ig',\n",
       " 'zu',\n",
       " '##is',\n",
       " '##ra',\n",
       " 'H',\n",
       " '##isch',\n",
       " 'den',\n",
       " 'ge',\n",
       " 'von',\n",
       " 'T',\n",
       " '##icht',\n",
       " 'des',\n",
       " '##am',\n",
       " 'das',\n",
       " 'L',\n",
       " '##ür',\n",
       " 'k',\n",
       " '##ier',\n",
       " '##re',\n",
       " '2',\n",
       " '##ol',\n",
       " '##om',\n",
       " 'N',\n",
       " 'I',\n",
       " 'an',\n",
       " '##ut',\n",
       " 'im',\n",
       " '##um',\n",
       " '##uch',\n",
       " '##and',\n",
       " '##ck',\n",
       " 'g',\n",
       " 'J',\n",
       " '##ben',\n",
       " 'mit',\n",
       " 'auf',\n",
       " '##ste',\n",
       " '##ent',\n",
       " '##ion',\n",
       " '##ro',\n",
       " 'ver',\n",
       " '##ls',\n",
       " 'Z',\n",
       " '[unused_punctuation4]',\n",
       " '##ir',\n",
       " 'Die',\n",
       " '##il',\n",
       " 'ist',\n",
       " 'dem',\n",
       " '##la',\n",
       " 'St',\n",
       " 'be',\n",
       " '##rei',\n",
       " 'Ver',\n",
       " '##le',\n",
       " '##ahr',\n",
       " '##ft',\n",
       " 'U',\n",
       " '##sen',\n",
       " '##ger',\n",
       " '##ann',\n",
       " '19',\n",
       " 'für',\n",
       " '##ah',\n",
       " 'sich',\n",
       " '##se',\n",
       " '##kt',\n",
       " 'aus',\n",
       " '20',\n",
       " 'nicht',\n",
       " 'Sch',\n",
       " '[unused_punctuation3]',\n",
       " 'An',\n",
       " 'als',\n",
       " '##ungs',\n",
       " 'eine',\n",
       " '##ischen',\n",
       " 'C',\n",
       " '##än',\n",
       " 'wur',\n",
       " '##men',\n",
       " '##ver',\n",
       " '##au',\n",
       " 'Be',\n",
       " '##ff',\n",
       " '##be',\n",
       " '##wei',\n",
       " 'sein',\n",
       " '##sp',\n",
       " 'O',\n",
       " 'o',\n",
       " '##och',\n",
       " '##ik',\n",
       " 'In',\n",
       " 'un',\n",
       " '##ne',\n",
       " '##esch',\n",
       " 'Er',\n",
       " 'bei',\n",
       " '##eil',\n",
       " 'l',\n",
       " 'so',\n",
       " '##sten',\n",
       " '##über',\n",
       " '##ungen',\n",
       " 'war',\n",
       " 'Re',\n",
       " '##ien',\n",
       " 'nach',\n",
       " '##etz',\n",
       " '##ün',\n",
       " '##cht',\n",
       " 'wurde',\n",
       " 'hat',\n",
       " 'auch',\n",
       " '##iel',\n",
       " '##ri',\n",
       " '##ehr',\n",
       " 'Ein',\n",
       " '##lä',\n",
       " 'vor',\n",
       " '##scha',\n",
       " '##gt',\n",
       " 'Jahr',\n",
       " 'über',\n",
       " 'sch',\n",
       " '##hn',\n",
       " '##stell',\n",
       " '##he',\n",
       " 't',\n",
       " '##ind',\n",
       " '##all',\n",
       " '##ern',\n",
       " 'sie',\n",
       " '##urch',\n",
       " '##hen',\n",
       " '##eich',\n",
       " '##eu',\n",
       " '##aus',\n",
       " '##igen',\n",
       " '##ät',\n",
       " 'dass',\n",
       " '##rie',\n",
       " '##ag',\n",
       " '##os',\n",
       " 'einer',\n",
       " 'Ab',\n",
       " '##ad',\n",
       " '##ab',\n",
       " 'es',\n",
       " '##ort',\n",
       " 'j',\n",
       " 'wir',\n",
       " 'Der',\n",
       " 'Un',\n",
       " 'am',\n",
       " '##ul',\n",
       " '##ot',\n",
       " '##lie',\n",
       " '##ation',\n",
       " 'wer',\n",
       " 'Ge',\n",
       " 'st',\n",
       " '[unused_punctuation2]',\n",
       " 'p',\n",
       " '##ster',\n",
       " 'wie',\n",
       " '##schaft',\n",
       " '##lichen',\n",
       " '##enn',\n",
       " '##ang',\n",
       " '##ser',\n",
       " 'zur',\n",
       " '##im',\n",
       " 'ihr',\n",
       " 'bis',\n",
       " '##hal',\n",
       " '##rt',\n",
       " '##end',\n",
       " 'um',\n",
       " 'zum',\n",
       " 'durch',\n",
       " '##ische',\n",
       " '##wer',\n",
       " '##lle',\n",
       " '##rä',\n",
       " 'werden',\n",
       " 'unter',\n",
       " '##hl',\n",
       " '##ück',\n",
       " '##ing',\n",
       " '##zu',\n",
       " '##ru',\n",
       " 'ent',\n",
       " '201',\n",
       " 'vom',\n",
       " 'Sp',\n",
       " '##tra',\n",
       " '200',\n",
       " '##mer',\n",
       " '##chen',\n",
       " 'ab',\n",
       " '##gel',\n",
       " '##eut',\n",
       " '##est',\n",
       " '##ühr',\n",
       " 'En',\n",
       " 'sind',\n",
       " '##ige',\n",
       " '##op',\n",
       " '##ran',\n",
       " 'Aus',\n",
       " 'wird',\n",
       " '##iv',\n",
       " '3',\n",
       " 'Das',\n",
       " '##ör',\n",
       " 'einem',\n",
       " 'Ber',\n",
       " '##rund',\n",
       " '##eiter',\n",
       " '##00',\n",
       " '##art',\n",
       " 'einen',\n",
       " 'Mit',\n",
       " '##tet',\n",
       " '##nen',\n",
       " '##atz',\n",
       " '##uss',\n",
       " 'oder',\n",
       " '##ges',\n",
       " 'Vor',\n",
       " 'ha',\n",
       " '##ön',\n",
       " 'Pro',\n",
       " 'Auf',\n",
       " '##eits',\n",
       " 'Ar',\n",
       " '##ill',\n",
       " '##ed',\n",
       " '##her',\n",
       " '##iert',\n",
       " '##liche',\n",
       " '##geb',\n",
       " '##ist',\n",
       " '##auf',\n",
       " 'Nach',\n",
       " '##pp',\n",
       " '##ahl',\n",
       " '##oll',\n",
       " '##zen',\n",
       " '##eutsch',\n",
       " '##tel',\n",
       " '##ht',\n",
       " '##pf',\n",
       " '##ert',\n",
       " '##ass',\n",
       " '##rit',\n",
       " 'sp',\n",
       " '18',\n",
       " '##ell',\n",
       " '##ord',\n",
       " '##eben',\n",
       " '##che',\n",
       " '##ner',\n",
       " '##är',\n",
       " 'Im',\n",
       " '##lin',\n",
       " 'da',\n",
       " 'all',\n",
       " 'sei',\n",
       " '##bar',\n",
       " '##ähr',\n",
       " '##lag',\n",
       " '##gl',\n",
       " '##ers',\n",
       " 'nur',\n",
       " 'noch',\n",
       " '##rün',\n",
       " '##keit',\n",
       " '##det',\n",
       " '##alt',\n",
       " '##lt',\n",
       " '##uß',\n",
       " '##wie',\n",
       " '##anz',\n",
       " '##iz',\n",
       " '##og',\n",
       " '##trag',\n",
       " '##rü',\n",
       " 'ber',\n",
       " 'Sie',\n",
       " '##elt',\n",
       " '##me',\n",
       " '##land',\n",
       " '##läger',\n",
       " '##ommen',\n",
       " '##bt',\n",
       " '##eine',\n",
       " '##nis',\n",
       " 'mehr',\n",
       " 'ander',\n",
       " 'zwei',\n",
       " 'gegen',\n",
       " '##we',\n",
       " '##orm',\n",
       " 'aber',\n",
       " '##acht',\n",
       " 'Unter',\n",
       " '##ug',\n",
       " 'dar',\n",
       " 'Ma',\n",
       " 'Kläger',\n",
       " '##dig',\n",
       " '##fahr',\n",
       " '##pt',\n",
       " '##ffen',\n",
       " 'gew',\n",
       " 're',\n",
       " '##ess',\n",
       " '4',\n",
       " '##).',\n",
       " '##fol',\n",
       " '##ub',\n",
       " '##eiten',\n",
       " '##ohn',\n",
       " 'zw',\n",
       " '##adt',\n",
       " '##ick',\n",
       " '##onder',\n",
       " '##ud',\n",
       " '##ust',\n",
       " '##ken',\n",
       " '##mt',\n",
       " 'Land',\n",
       " '##ant',\n",
       " '##ze',\n",
       " 'Zeit',\n",
       " '##führ',\n",
       " 'dies',\n",
       " '##ho',\n",
       " '[unused_punctuation8]',\n",
       " '##etzt',\n",
       " 'Abs',\n",
       " 'weiter',\n",
       " '##ap',\n",
       " '##nah',\n",
       " '##wa',\n",
       " '##id',\n",
       " '##dung',\n",
       " '##ss',\n",
       " 'kön',\n",
       " '##ech',\n",
       " '##tern',\n",
       " '##ic',\n",
       " '5',\n",
       " '##arb',\n",
       " '##ha',\n",
       " 'Kon',\n",
       " '##neh',\n",
       " '##teil',\n",
       " '##fall',\n",
       " 'Ent',\n",
       " 'eines',\n",
       " '##ren',\n",
       " '##lei',\n",
       " '##mal',\n",
       " '##zeit',\n",
       " '##ob',\n",
       " '##ßen',\n",
       " '##ler',\n",
       " '##fen',\n",
       " '##utz',\n",
       " '##stand',\n",
       " 'e',\n",
       " 'Ges',\n",
       " '##ierte',\n",
       " '##aten',\n",
       " '##ieren',\n",
       " 'soll',\n",
       " 'lie',\n",
       " 'hin',\n",
       " '##bur',\n",
       " '199',\n",
       " '##ak',\n",
       " 'Arb',\n",
       " 'hatte',\n",
       " 'Bei',\n",
       " 'wurden',\n",
       " '##orden',\n",
       " '##äch',\n",
       " 'seiner',\n",
       " 'Recht',\n",
       " '##reich',\n",
       " 'haben',\n",
       " '##mit',\n",
       " 'neu',\n",
       " '##lage',\n",
       " 'man',\n",
       " 'kann',\n",
       " '##.2',\n",
       " '##itz',\n",
       " 'Es',\n",
       " 'Bet',\n",
       " '##heit',\n",
       " '##lan',\n",
       " '##assen',\n",
       " '##att',\n",
       " '##ats',\n",
       " '##bst',\n",
       " '##glich',\n",
       " 'Um',\n",
       " '##dem',\n",
       " '##ok',\n",
       " 'kon',\n",
       " 'Mar',\n",
       " 'r',\n",
       " 'Deutsch',\n",
       " 'seine',\n",
       " 'Bund',\n",
       " '##),',\n",
       " '##hm',\n",
       " 'Al',\n",
       " '##üt',\n",
       " 'gen',\n",
       " '##luss',\n",
       " 'Se',\n",
       " 'So',\n",
       " '##ße',\n",
       " '##gem',\n",
       " '##ktion',\n",
       " '##vor',\n",
       " '##klag',\n",
       " '##spr',\n",
       " '##ast',\n",
       " 'et',\n",
       " 'Her',\n",
       " '##zi',\n",
       " '##fe',\n",
       " '##iger',\n",
       " '##los',\n",
       " '##br',\n",
       " '##eigen',\n",
       " 'bes',\n",
       " 'Eur',\n",
       " 'wieder',\n",
       " '##\",',\n",
       " 'Ü',\n",
       " 'Ch',\n",
       " '##ga',\n",
       " '##schei',\n",
       " '##ande',\n",
       " '##ischer',\n",
       " '##gesch',\n",
       " 'dieser',\n",
       " '6',\n",
       " '##ierung',\n",
       " '##vers',\n",
       " '##gan',\n",
       " 'ins',\n",
       " 'ges',\n",
       " 'bet',\n",
       " '##ens',\n",
       " 'je',\n",
       " '##ia',\n",
       " 'sowie',\n",
       " '##stän',\n",
       " '##01',\n",
       " '##ften',\n",
       " 'Besch',\n",
       " '##ammen',\n",
       " 'Fra',\n",
       " '##schen',\n",
       " '##od',\n",
       " '##nung',\n",
       " 'habe',\n",
       " '##sicht',\n",
       " 'wenn',\n",
       " '##ahn',\n",
       " 'gem',\n",
       " 'Stadt',\n",
       " '##ale',\n",
       " '##igkeit',\n",
       " '##lo',\n",
       " '##ität',\n",
       " 'Grund',\n",
       " '##rich',\n",
       " '##rat',\n",
       " '##schie',\n",
       " 'le',\n",
       " 'Am',\n",
       " '##halten',\n",
       " '##olit',\n",
       " 'Jahre',\n",
       " '##halt',\n",
       " '##sk',\n",
       " 'De',\n",
       " '##amt',\n",
       " 'Bundes',\n",
       " '##annt',\n",
       " 'Beklag',\n",
       " '##öff',\n",
       " 'ger',\n",
       " '##reis',\n",
       " '##stellt',\n",
       " '##inder',\n",
       " 'dam',\n",
       " '##ard',\n",
       " '##ov',\n",
       " '##hör',\n",
       " '##üd',\n",
       " '##elbst',\n",
       " '##unden',\n",
       " '##lung',\n",
       " 'Über',\n",
       " '##rieb',\n",
       " '##for',\n",
       " 'zwischen',\n",
       " 'Zu',\n",
       " '##burg',\n",
       " '##erk',\n",
       " 'Ste',\n",
       " 'seit',\n",
       " '##ließ',\n",
       " '##net',\n",
       " 'Jahren',\n",
       " '##usammen',\n",
       " '##eist',\n",
       " '##per',\n",
       " '##alen',\n",
       " 'Gem',\n",
       " '##ow',\n",
       " '##gung',\n",
       " '##hä',\n",
       " 'Fl',\n",
       " '##walt',\n",
       " '##eld',\n",
       " 'Fran',\n",
       " '##sel',\n",
       " '##aupt',\n",
       " 'diese',\n",
       " '##grün',\n",
       " '##omm',\n",
       " 'jed',\n",
       " 'erst',\n",
       " '##tete',\n",
       " '##zeich',\n",
       " 'ange',\n",
       " 'Reg',\n",
       " '##ember',\n",
       " '##fer',\n",
       " '##zent',\n",
       " '##änder',\n",
       " '##qu',\n",
       " '7',\n",
       " 'deutsch',\n",
       " 'waren',\n",
       " '##ark',\n",
       " '##geben',\n",
       " '##inn',\n",
       " '##eg',\n",
       " '##rech',\n",
       " '17',\n",
       " '##stehen',\n",
       " 'Gesch',\n",
       " '##rö',\n",
       " '##gericht',\n",
       " '##eister',\n",
       " '##weise',\n",
       " 'gro',\n",
       " 'her',\n",
       " '##icher',\n",
       " '##recht',\n",
       " '##bau',\n",
       " 'Sta',\n",
       " 'Wir',\n",
       " 'Ro',\n",
       " '##tem',\n",
       " '##tschaft',\n",
       " '15',\n",
       " '##ium',\n",
       " '##berg',\n",
       " '##.1',\n",
       " '##ingen',\n",
       " '##iet',\n",
       " '198',\n",
       " 'ausge',\n",
       " '##uer',\n",
       " 'keine',\n",
       " '##leich',\n",
       " 'dann',\n",
       " 'worden',\n",
       " 'Ger',\n",
       " 'Ko',\n",
       " '##rupp',\n",
       " '##einen',\n",
       " '##oren',\n",
       " '##setz',\n",
       " 'drei',\n",
       " '##ieder',\n",
       " '##ay',\n",
       " 'bef',\n",
       " 'ihre',\n",
       " '##eter',\n",
       " '##eim',\n",
       " '##satz',\n",
       " '##äter',\n",
       " '##glie',\n",
       " '##fü',\n",
       " '##iss',\n",
       " 'Br',\n",
       " '##haus',\n",
       " '##halb',\n",
       " '##bl',\n",
       " '##hö',\n",
       " '8',\n",
       " '##pl',\n",
       " '##hne',\n",
       " '##imm',\n",
       " '##ater',\n",
       " '##grund',\n",
       " '##äß',\n",
       " 'hier',\n",
       " '##ks',\n",
       " '##nehmen',\n",
       " 'zurück',\n",
       " '##gend',\n",
       " '##ett',\n",
       " 'bek',\n",
       " '##ibt',\n",
       " '##ont',\n",
       " '##ähl',\n",
       " '##steht',\n",
       " '##wir',\n",
       " 'kl',\n",
       " 'Berlin',\n",
       " 'ihn',\n",
       " 'Teil',\n",
       " 'können',\n",
       " 'Gemein',\n",
       " '##zie',\n",
       " '197',\n",
       " '##hem',\n",
       " '##ekt',\n",
       " '##ußer',\n",
       " '##St',\n",
       " '##spiel',\n",
       " '##amen',\n",
       " 'Welt',\n",
       " 'Geb',\n",
       " '##üs',\n",
       " '##eb',\n",
       " '##nte',\n",
       " '##igt',\n",
       " '##min',\n",
       " '##ild',\n",
       " '##du',\n",
       " '##pr',\n",
       " 'eigen',\n",
       " '##gte',\n",
       " '##ionen',\n",
       " '16',\n",
       " 'jedoch',\n",
       " 'wo',\n",
       " '##igung',\n",
       " '##000',\n",
       " 'Da',\n",
       " 'gesch',\n",
       " 'Rechts',\n",
       " '##mann',\n",
       " 'Art',\n",
       " '##ma',\n",
       " '##egen',\n",
       " '##gs',\n",
       " '##öglich',\n",
       " '##öl',\n",
       " '##issen',\n",
       " '##nahme',\n",
       " '##iele',\n",
       " '##ielt',\n",
       " '##äft',\n",
       " 'etwa',\n",
       " 'Stra',\n",
       " '9',\n",
       " 'schon',\n",
       " '##ende',\n",
       " '##licher',\n",
       " '##amm',\n",
       " '##pro',\n",
       " '##anden',\n",
       " '##ding',\n",
       " '##ienst',\n",
       " '##ke',\n",
       " 'Sa',\n",
       " '##ian',\n",
       " '##fahren',\n",
       " '##war',\n",
       " 'bereits',\n",
       " '##if',\n",
       " '##tre',\n",
       " '##reit',\n",
       " 'ersten',\n",
       " 'selbst',\n",
       " 'führ',\n",
       " 'Or',\n",
       " 'beim',\n",
       " '##io',\n",
       " 'ihm',\n",
       " 'Bau',\n",
       " 'beg',\n",
       " 'fol',\n",
       " '##kte',\n",
       " '##zeu',\n",
       " 'best',\n",
       " '##achen',\n",
       " '##llen',\n",
       " '##räs',\n",
       " '196',\n",
       " 'diesem',\n",
       " 'La',\n",
       " 'seinen',\n",
       " '##kehr',\n",
       " '##essen',\n",
       " '##ährend',\n",
       " '194',\n",
       " 'Spiel',\n",
       " '##bes',\n",
       " '##ierten',\n",
       " 'Klägerin',\n",
       " '##hol',\n",
       " '12',\n",
       " 'ihrer',\n",
       " '##wick',\n",
       " 'seinem',\n",
       " '##falls',\n",
       " 'Kl',\n",
       " 'Men',\n",
       " '##licht',\n",
       " '##ister',\n",
       " 'Q',\n",
       " '10',\n",
       " '##form',\n",
       " '##wä',\n",
       " '##öffent',\n",
       " 'besch',\n",
       " '##1.',\n",
       " 'Leben',\n",
       " 'gel',\n",
       " '##isten',\n",
       " '##zt',\n",
       " 'Pl',\n",
       " 'Auch',\n",
       " 'Deutschland',\n",
       " '##esen',\n",
       " 'ob',\n",
       " '14',\n",
       " '##our',\n",
       " '##als',\n",
       " '##ive',\n",
       " '##spruch',\n",
       " '##ehen',\n",
       " '##gegen',\n",
       " '##zel',\n",
       " '##deut',\n",
       " '##mittel',\n",
       " '##fte',\n",
       " 'Antrag',\n",
       " 'anderen',\n",
       " '##eute',\n",
       " '##abei',\n",
       " 'damit',\n",
       " '##atur',\n",
       " '##zw',\n",
       " '##rift',\n",
       " '##unkt',\n",
       " 'Aut',\n",
       " '##ucht',\n",
       " '13',\n",
       " '##gang',\n",
       " '##stellung',\n",
       " '##lagen',\n",
       " '##stra',\n",
       " 'Part',\n",
       " '##ker',\n",
       " 'Für',\n",
       " '##ikan',\n",
       " '##ult',\n",
       " '##ilm',\n",
       " '##ogen',\n",
       " 'geb',\n",
       " 'viel',\n",
       " '##gleich',\n",
       " '##hält',\n",
       " '(1',\n",
       " 'Ort',\n",
       " '##ald',\n",
       " '##wend',\n",
       " 'Hö',\n",
       " 'später',\n",
       " 'Pf',\n",
       " 'wen',\n",
       " '##forder',\n",
       " '##iden',\n",
       " 'kom',\n",
       " '##ym',\n",
       " '##ander',\n",
       " '1.',\n",
       " '##amil',\n",
       " 'einge',\n",
       " '##hin',\n",
       " '##fä',\n",
       " '##ichte',\n",
       " 'Als',\n",
       " 'gibt',\n",
       " 'Mus',\n",
       " 'erk',\n",
       " 'dort',\n",
       " 'Bes',\n",
       " 'erfol',\n",
       " '##asser',\n",
       " 'de',\n",
       " '##richt',\n",
       " '195',\n",
       " '##cken',\n",
       " '##zahl',\n",
       " 'Ho',\n",
       " '##ivers',\n",
       " '##erson',\n",
       " '##ition',\n",
       " 'Kar',\n",
       " 'Haupt',\n",
       " 'gleich',\n",
       " 'Le',\n",
       " 'Vers',\n",
       " 'eben',\n",
       " '##ph',\n",
       " 'Mill',\n",
       " 'Eine',\n",
       " 'Euro',\n",
       " '##agen',\n",
       " '##aum',\n",
       " 'Urt',\n",
       " 'immer',\n",
       " '##auer',\n",
       " '##kommen',\n",
       " '##.0',\n",
       " 'Ende',\n",
       " '##ine',\n",
       " '##orf',\n",
       " 'Ne',\n",
       " 'Mon',\n",
       " '##ache',\n",
       " '##ober',\n",
       " '##lauf',\n",
       " '193',\n",
       " 'ohne',\n",
       " 'Hin',\n",
       " '##ank',\n",
       " 'verb',\n",
       " 'Kir',\n",
       " '##unter',\n",
       " '##undert',\n",
       " '##ire',\n",
       " 'Bl',\n",
       " 'Arbeit',\n",
       " '##\".',\n",
       " 'Frau',\n",
       " '##ritt',\n",
       " '##ös',\n",
       " '##gangen',\n",
       " '##ce',\n",
       " 'Gro',\n",
       " '##olog',\n",
       " 'fest',\n",
       " '##iff',\n",
       " '##ondern',\n",
       " '##aut',\n",
       " '##enden',\n",
       " 'fin',\n",
       " 'Qu',\n",
       " 'US',\n",
       " 'was',\n",
       " '##ate',\n",
       " 'Min',\n",
       " '##änger',\n",
       " '##.201',\n",
       " '##anu',\n",
       " 'sondern',\n",
       " 'liegt',\n",
       " '##hof',\n",
       " '##hand',\n",
       " '##ätz',\n",
       " '##schaften',\n",
       " 'Mitglie',\n",
       " '##olitik',\n",
       " 'Arbeits',\n",
       " 'ar',\n",
       " 'müs',\n",
       " 'Ober',\n",
       " '##ule',\n",
       " 'Mann',\n",
       " 'Mün',\n",
       " 'weil',\n",
       " '##ade',\n",
       " 'Pol',\n",
       " '##gri',\n",
       " '##fl',\n",
       " 'alle',\n",
       " '##stellen',\n",
       " '##sta',\n",
       " 'Gen',\n",
       " 'verl',\n",
       " 'Frank',\n",
       " '##scheidung',\n",
       " '##cke',\n",
       " '##rin',\n",
       " '##uni',\n",
       " '##yst',\n",
       " '##genden',\n",
       " 'recht',\n",
       " ...]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "# Access the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert the vocabulary to a list of tokens\n",
    "vocab_list = list(vocab.keys())\n",
    "\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test gpu connection\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ort: A 5',\n",
       " 'Ort: A 66',\n",
       " 'Ort: A 661',\n",
       " 'Ort: Aalen',\n",
       " 'Ort: Aare-Mündung',\n",
       " 'Ort: Abchasien',\n",
       " 'Ort: Abrams Complex',\n",
       " 'Ort: Abtei Münsterschwarzach',\n",
       " 'Ort: Achenbachstraße',\n",
       " 'Ort: Adalbertstraße',\n",
       " 'Ort: Adam-und-Eva-Brunnen',\n",
       " 'Ort: Adria',\n",
       " 'Ort: Adrianopel',\n",
       " 'Ort: Affenbrunnen',\n",
       " 'Ort: Affentorhaus',\n",
       " 'Ort: Affentorhaus West',\n",
       " 'Ort: Afrika',\n",
       " 'Ort: Airlenbach',\n",
       " 'Ort: Albert-Schweizer-Kinderdorf',\n",
       " 'Ort: Alexanderstraße',\n",
       " 'Ort: Alleenring',\n",
       " 'Ort: Allgäu',\n",
       " 'Ort: Alma Ata',\n",
       " 'Ort: Alt-Bergen',\n",
       " 'Ort: Alte Haus von 1327',\n",
       " 'Ort: Alte Josefshaus',\n",
       " 'Ort: Alten Brücke',\n",
       " 'Ort: Altenbuch',\n",
       " 'Ort: ALTENBURG',\n",
       " 'Ort: Alten Frankfurter Straße',\n",
       " 'Ort: Altenhilfe-Zentrum',\n",
       " 'Ort: Altenwohnheim',\n",
       " 'Ort: Alte Oper',\n",
       " 'Ort: Alter Kirchhainer Weg',\n",
       " 'Ort: Altertumsmuseum',\n",
       " 'Ort: Alte Schulstraße',\n",
       " 'Ort: Alt-Heidelberg',\n",
       " 'Ort: Altmünstermühle',\n",
       " 'Ort: Alt-Nied',\n",
       " 'Ort: Alt-Niederursel',\n",
       " 'Ort: Alt-Sachsenhausen',\n",
       " 'Ort: AltSachsenhausens',\n",
       " 'Ort: AltSchwanheim',\n",
       " 'Ort: Alt-Schwanheim',\n",
       " 'Ort: AltSossenheim',\n",
       " 'Ort: Altstadt',\n",
       " 'Ort: Altstädter Markt',\n",
       " 'Ort: Am Bügel',\n",
       " 'Ort: Amerika',\n",
       " 'Ort: Am Ginnheimer Wäldchen',\n",
       " 'Ort: Am Hohlakker',\n",
       " 'Ort: Amsterdam',\n",
       " 'Ort: Am Villaberg',\n",
       " 'Ort: Am Wiesenhof',\n",
       " 'Ort: An den Röthen',\n",
       " 'Ort: An der Roseneller',\n",
       " 'Ort: Anhausen',\n",
       " 'Ort: Anna-Schmidt-Schule',\n",
       " 'Ort: Anne-Frank-Schule',\n",
       " 'Ort: Antoniterstraße',\n",
       " 'Ort: Antwerpen',\n",
       " 'Ort: AOK-Haus',\n",
       " 'Ort: Apotheke am Markt',\n",
       " 'Ort: Appelsgasse',\n",
       " 'Ort: Arbeiter-',\n",
       " 'Ort: Architekturmuseums',\n",
       " 'Ort: Archivkino Caligari',\n",
       " 'Ort: Argentinien',\n",
       " 'Ort: Argentiniens',\n",
       " 'Ort: Armenien',\n",
       " 'Ort: Armeniens',\n",
       " 'Ort: Army-Gelände',\n",
       " 'Ort: Arndtstraße',\n",
       " 'Ort: Arnsburger Straße',\n",
       " 'Ort: Arolsen-Landau',\n",
       " 'Ort: Artischockenbrunnen',\n",
       " 'Ort: Arvelaez',\n",
       " 'Ort: Aschaffenburg',\n",
       " 'Ort: Aschaffenburger Straße',\n",
       " 'Ort: Aserbaidschan',\n",
       " 'Ort: Aserbaidschans',\n",
       " 'Ort: asiatische Subkontinent',\n",
       " 'Ort: Asien',\n",
       " 'Ort: Assenheim',\n",
       " 'Ort: Assenheimer Straße',\n",
       " 'Ort: Aßlarer Straße',\n",
       " 'Ort: Athen',\n",
       " 'Ort: ATHEN',\n",
       " 'Ort: Athener Kasernen',\n",
       " 'Ort: Atlantik',\n",
       " 'Ort: Auerbach',\n",
       " 'Ort: AUERBACH',\n",
       " 'Ort: August-Schanz-Straße',\n",
       " 'Ort: Auschwitz',\n",
       " 'Ort: Australien',\n",
       " 'Ort: Ayers-Kaserne',\n",
       " 'Ort: B 40',\n",
       " 'Ort: Babenhausen',\n",
       " 'Ort: Babenhäuser Landstraße',\n",
       " 'Ort: Baden',\n",
       " 'Ort: Baden-Württemberg',\n",
       " 'Ort: Badeort Miedzyzdroje',\n",
       " 'Ort: Bad Homburg',\n",
       " 'Ort: BAD HOMBURG',\n",
       " 'Ort: Bad Homburgs',\n",
       " 'Ort: Bad Mergentheim',\n",
       " 'Ort: Bad Oeynhausen',\n",
       " 'Ort: Bad Orb',\n",
       " 'Ort: Bad Soden',\n",
       " 'Ort: Bad Vilbel',\n",
       " 'Ort: BAD VILBEL',\n",
       " 'Ort: Bad Vilbelern',\n",
       " 'Ort: Bahamas',\n",
       " 'Ort: Bahia Blanca',\n",
       " 'Ort: Bahnbetriebskantine',\n",
       " 'Ort: Bahnhof',\n",
       " 'Ort: Bahnhofs-',\n",
       " 'Ort: Bahnhofstraße',\n",
       " 'Ort: Bahnhofsviertel',\n",
       " 'Ort: Baku',\n",
       " 'Ort: Balduinstraße',\n",
       " 'Ort: Balkan',\n",
       " 'Ort: Bamni',\n",
       " 'Ort: Bangkok',\n",
       " 'Ort: Bangkoks',\n",
       " 'Ort: Bankenplatz Berlin',\n",
       " 'Ort: Barbarossastraße',\n",
       " 'Ort: Barckhausstraße',\n",
       " 'Ort: Barckhaustraße',\n",
       " 'Ort: BärengassenScheune',\n",
       " 'Ort: Basel',\n",
       " 'Ort: Basilika',\n",
       " 'Ort: Baskenland',\n",
       " 'Ort: Basler Straße',\n",
       " 'Ort: Batschkapp',\n",
       " 'Ort: Batschkapp Station',\n",
       " 'Ort: Bauernstaates',\n",
       " 'Ort: Baum im Odenwald',\n",
       " 'Ort: Bayern',\n",
       " 'Ort: Bechtenwaldstraße',\n",
       " 'Ort: Beim Kelch',\n",
       " 'Ort: Belgien',\n",
       " 'Ort: Belgrad',\n",
       " 'Ort: Belgrano',\n",
       " 'Ort: Ben-Gurion-Ring',\n",
       " 'Ort: Bereich Nordend-Mitte',\n",
       " 'Ort: Bergen',\n",
       " 'Ort: BergenEnkheim',\n",
       " 'Ort: Bergen-Enkheim',\n",
       " 'Ort: BERGEN-ENKHEIM',\n",
       " 'Ort: Berger Hang',\n",
       " 'Ort: Berger Straße',\n",
       " 'Ort: Berg-Karabach',\n",
       " 'Ort: Bergkirchen',\n",
       " 'Ort: Bergstraße',\n",
       " 'Ort: Berkersheim',\n",
       " 'Ort: Berkersheimer Weg',\n",
       " 'Ort: Berkersheim und Nied',\n",
       " 'Ort: Berlin',\n",
       " 'Ort: Berliner Stadtschlosses',\n",
       " 'Ort: Berliner Straße',\n",
       " 'Ort: Berlins',\n",
       " 'Ort: Berner Straße',\n",
       " 'Ort: Berthold-Otto-Schule',\n",
       " 'Ort: Bertramstraße',\n",
       " 'Ort: Bethmann-Hollweg-',\n",
       " 'Ort: Bethmann-Hollweg-Straße',\n",
       " 'Ort: Beuthener Straße',\n",
       " 'Ort: Bezirk F',\n",
       " 'Ort: Bezirksbad Nieder-Eschbach',\n",
       " 'Ort: Bezirksbad Süd',\n",
       " 'Ort: Bezirkssportanlage Hahnstraße',\n",
       " 'Ort: Bezirkssportanlage Nieder-Eschbach',\n",
       " 'Ort: BG-Unfallklinik',\n",
       " 'Ort: Bialystok',\n",
       " 'Ort: Bieberach',\n",
       " 'Ort: Biebergemünd',\n",
       " 'Ort: Biebrich',\n",
       " 'Ort: Bielefeld',\n",
       " 'Ort: Bierstadt',\n",
       " 'Ort: Billtalhöhe',\n",
       " 'Ort: Bingelsweg',\n",
       " 'Ort: Birstein',\n",
       " 'Ort: Bischof-Ketteler-Haus',\n",
       " 'Ort: Bischofsheim',\n",
       " 'Ort: Bistum Limburg',\n",
       " 'Ort: Bizzenbachtal',\n",
       " 'Ort: Blankenheimer Straße',\n",
       " 'Ort: Blätterwald',\n",
       " 'Ort: Bleichstraße',\n",
       " 'Ort: Bleiweißstraße',\n",
       " 'Ort: Bockenheim',\n",
       " 'Ort: BOCKENHEIM',\n",
       " 'Ort: Bockenheimer Landstraße',\n",
       " 'Ort: Bockenheimer Warte',\n",
       " 'Ort: Bogotá',\n",
       " 'Ort: Bokenheim',\n",
       " 'Ort: Bokkenheim',\n",
       " 'Ort: Bolongaropalast',\n",
       " 'Ort: Bolschoi',\n",
       " 'Ort: Bonames',\n",
       " 'Ort: BONAMES',\n",
       " 'Ort: Bonameser Sportplatz',\n",
       " 'Ort: Bonn',\n",
       " 'Ort: BONN',\n",
       " 'Ort: Bonner Kunstverein',\n",
       " 'Ort: Bonns',\n",
       " 'Ort: Bönstadt',\n",
       " 'Ort: Bordeaux',\n",
       " 'Ort: Bornheim',\n",
       " 'Ort: BORNHEIM',\n",
       " 'Ort: Bornwiesenweg',\n",
       " 'Ort: Borsdorf',\n",
       " 'Ort: Borsigallee',\n",
       " 'Ort: Boseweg',\n",
       " 'Ort: Bosnien',\n",
       " 'Ort: Bosnienes',\n",
       " 'Ort: Bosnien-Herzegowina',\n",
       " 'Ort: Bosniens',\n",
       " 'Ort: Botanischen Garten',\n",
       " 'Ort: Böttgerstr.',\n",
       " 'Ort: Bovelands',\n",
       " 'Ort: Brandenburg',\n",
       " 'Ort: Brandenburger Straße',\n",
       " 'Ort: Brandenburgs',\n",
       " 'Ort: Brandholz',\n",
       " 'Ort: Brasilien',\n",
       " 'Ort: BRD-Federführungen',\n",
       " 'Ort: Breitenbachbrücke',\n",
       " 'Ort: Bremen',\n",
       " 'Ort: Bremer Straße',\n",
       " 'Ort: Brentano-',\n",
       " 'Ort: Brentanobad',\n",
       " 'Ort: Brentanopark',\n",
       " 'Ort: Breslauer Straße',\n",
       " 'Ort: Bretagne',\n",
       " 'Ort: Broadway',\n",
       " 'Ort: Brotfabrik',\n",
       " 'Ort: BRUCHKÖBEL',\n",
       " 'Ort: Brunhildenstr.',\n",
       " 'Ort: Buchenrodestraße',\n",
       " 'Ort: Buchrain-',\n",
       " 'Ort: Buchrainplatz',\n",
       " 'Ort: Buchrainplatzes',\n",
       " 'Ort: Buchrainstraße',\n",
       " 'Ort: Budapest',\n",
       " 'Ort: BÜDINGEN',\n",
       " 'Ort: BÜDINGEN-NORD',\n",
       " 'Ort: BÜDINGEN-SÜD',\n",
       " 'Ort: Büdinger Straße',\n",
       " 'Ort: Büdungen',\n",
       " 'Ort: Buenos Aires',\n",
       " 'Ort: Bügel',\n",
       " 'Ort: Bük-Gebirge',\n",
       " 'Ort: Bulgarien',\n",
       " 'Ort: Bundesbahngelände',\n",
       " 'Ort: Bundesrepublik',\n",
       " 'Ort: Bundesrepublik Deutschland',\n",
       " 'Ort: Bundestag-Plenarsaals',\n",
       " 'Ort: Burg Dreieichenhain',\n",
       " 'Ort: Bürgerhaus Bornheim',\n",
       " 'Ort: Bürgerhaus Dornbusch',\n",
       " 'Ort: Bürgerhaus Griesheim',\n",
       " 'Ort: Bürgerhaus Heilsberg',\n",
       " 'Ort: Bürgerhaus Nieder-Erlenbach',\n",
       " 'Ort: Bürgerhaus Nordweststadt',\n",
       " 'Ort: Bürgerhaus Philantropin',\n",
       " 'Ort: Bürgertreff Bockenheim',\n",
       " 'Ort: Bürgertreff Depot',\n",
       " 'Ort: Bürgertreff Gutleut',\n",
       " 'Ort: Bürgertreff Philanthropin',\n",
       " 'Ort: Burgfeld',\n",
       " 'Ort: Burgfestspiele',\n",
       " 'Ort: Burglehen',\n",
       " 'Ort: Burgstraße',\n",
       " 'Ort: Burnaston',\n",
       " 'Ort: Burnitzstraße',\n",
       " 'Ort: Buschwiesen',\n",
       " 'Ort: Büttelborn',\n",
       " 'Ort: BÜTTELBORN',\n",
       " 'Ort: Büttelborner DRK-Heimes',\n",
       " 'Ort: BUTZBACH',\n",
       " 'Ort: Café Exzess',\n",
       " 'Ort: Café Katakombe',\n",
       " 'Ort: Café Plazz',\n",
       " 'Ort: Cali',\n",
       " 'Ort: Caligari',\n",
       " 'Ort: Camberger Brücke',\n",
       " 'Ort: Camberger Straße',\n",
       " 'Ort: Campingplatz Niederrad',\n",
       " 'Ort: Candlelight Dinner Theater',\n",
       " 'Ort: Candlelight Theater',\n",
       " 'Ort: Capuccino',\n",
       " 'Ort: Caribbean',\n",
       " 'Ort: Carl-Goerdeler-Straße',\n",
       " 'Ort: Carl-Schurz-Siedlung',\n",
       " 'Ort: Carl-von-Drais-Straße',\n",
       " 'Ort: Chicago',\n",
       " 'Ort: China',\n",
       " 'Ort: Chocó',\n",
       " 'Ort: ChristKönig',\n",
       " 'Ort: Christ-König',\n",
       " 'Ort: Christ-König-Gemeindezentrum',\n",
       " 'Ort: Christobal',\n",
       " 'Ort: Christuskirche',\n",
       " 'Ort: City-West',\n",
       " 'Ort: Clubhaus Ginnheim',\n",
       " 'Ort: Clubraum 1',\n",
       " 'Ort: Colmar',\n",
       " 'Ort: Conte',\n",
       " \"Ort: Cooky 's\",\n",
       " 'Ort: Corelli',\n",
       " 'Ort: Covent Garden',\n",
       " 'Ort: Creglingen',\n",
       " 'Ort: Cristobal',\n",
       " 'Ort: Damaschkeanger',\n",
       " 'Ort: Dammbach',\n",
       " 'Ort: Dänemark',\n",
       " 'Ort: Darmstadt',\n",
       " 'Ort: DARMSTADT',\n",
       " 'Ort: Darmstadt-Dieburg',\n",
       " 'Ort: Darmstadt-Dieburgs',\n",
       " 'Ort: Darmstädter Krone',\n",
       " 'Ort: Darmstädter Landstraße',\n",
       " 'Ort: Darmstädter Straße',\n",
       " 'Ort: DARMSTADT-WEST',\n",
       " 'Ort: Dassower',\n",
       " 'Ort: DDR',\n",
       " 'Ort: Decameron',\n",
       " 'Ort: Deichtorhallen',\n",
       " 'Ort: DEN HAAG',\n",
       " 'Ort: Departement Moselle',\n",
       " 'Ort: der hof',\n",
       " 'Ort: Deuil-la-Barre-Straße',\n",
       " 'Ort: Deutschen Architekturmuseum',\n",
       " 'Ort: Deutschen Bibliothek',\n",
       " 'Ort: Deutschen Haus',\n",
       " 'Ort: Deutsche Reich',\n",
       " 'Ort: Deutsches Goldschmiedehaus',\n",
       " 'Ort: Deutschland',\n",
       " 'Ort: Deutschlands',\n",
       " 'Ort: Deutschordenstraße',\n",
       " 'Ort: DIEBURG',\n",
       " 'Ort: Dietesheim',\n",
       " 'Ort: Dietrich-Bonhoeffer-Schule',\n",
       " 'Ort: Dietzenbach',\n",
       " 'Ort: DIETZENBACH',\n",
       " 'Ort: Dillenburg',\n",
       " 'Ort: Dillenburger Straße',\n",
       " 'Ort: Dinkelsbühl',\n",
       " 'Ort: Dinner Theater',\n",
       " 'Ort: Dom',\n",
       " 'Ort: Dominikanische Republik',\n",
       " 'Ort: Donau',\n",
       " 'Ort: Donauauen',\n",
       " 'Ort: Dorfelder Weg',\n",
       " 'Ort: Dorfgütingen',\n",
       " 'Ort: Dorfwiesenweg',\n",
       " 'Ort: Dornbusch',\n",
       " 'Ort: DORNBUSCH',\n",
       " 'Ort: Dornholzhausen',\n",
       " 'Ort: Dörnigheim',\n",
       " 'Ort: Dorotheengarage',\n",
       " 'Ort: Dortmund',\n",
       " 'Ort: Drake',\n",
       " 'Ort: Drake-',\n",
       " 'Ort: Drake-Kaserne',\n",
       " 'Ort: Dreieich',\n",
       " 'Ort: Dreieich-',\n",
       " 'Ort: Dreieich-Götzenhain',\n",
       " 'Ort: Dreieich-Sprendlingen',\n",
       " 'Ort: Dreieichstraße',\n",
       " 'Ort: Dreifaltigkeitsgemeinde',\n",
       " 'Ort: Dresden',\n",
       " 'Ort: Dribbdebach',\n",
       " 'Ort: DRK-Haus',\n",
       " 'Ort: Düdelsheim',\n",
       " 'Ort: Dunant-Siedlung',\n",
       " 'Ort: Düsseldorf',\n",
       " 'Ort: E 10.',\n",
       " 'Ort: E 9',\n",
       " 'Ort: Ebbelweiviertel',\n",
       " 'Ort: Ebbelwei-Viertel',\n",
       " 'Ort: Eberswalde',\n",
       " 'Ort: Ecke Haberweg',\n",
       " 'Ort: Eckenheim',\n",
       " 'Ort: ECKENHEIM',\n",
       " 'Ort: Eckenheimer',\n",
       " 'Ort: Eckenheimer Landstraße',\n",
       " 'Ort: Eddersheim',\n",
       " 'Ort: Edersee',\n",
       " 'Ort: Edinburgh',\n",
       " 'Ort: EDINBURGH',\n",
       " 'Ort: Edinburgh Festival Theatre',\n",
       " 'Ort: Edirne',\n",
       " 'Ort: Eduard von Borsodys',\n",
       " 'Ort: Edward',\n",
       " 'Ort: Edwards-Kaserne',\n",
       " 'Ort: Egenolffstraße',\n",
       " 'Ort: Egerland',\n",
       " 'Ort: EGEuropa',\n",
       " 'Ort: Eichwaldhallen',\n",
       " 'Ort: Eiffelturm',\n",
       " 'Ort: Eisenach',\n",
       " 'Ort: Eisenerzgrube Lindenberg',\n",
       " 'Ort: Eisenhüttenstadt-Land',\n",
       " 'Ort: Eisernen Hand',\n",
       " 'Ort: Elbestraße',\n",
       " 'Ort: Elbinger Straße',\n",
       " 'Ort: Elefanten-Apotheke',\n",
       " 'Ort: Elisabethenstraße',\n",
       " 'Ort: Elsaß',\n",
       " 'Ort: Elsasses',\n",
       " 'Ort: Elsaß-Lothringen',\n",
       " 'Ort: Emil-von-Bering-',\n",
       " 'Ort: Emmerich-Josef-Straße',\n",
       " 'Ort: Emmering',\n",
       " 'Ort: England',\n",
       " 'Ort: Enkheim',\n",
       " 'Ort: ENKHEIM',\n",
       " 'Ort: Enkheim Ost',\n",
       " 'Ort: Enkheim-Ost',\n",
       " 'Ort: Epiphaniaskirche',\n",
       " 'Ort: Eppingen',\n",
       " 'Ort: Eppstein',\n",
       " 'Ort: Erbach-Schönberg',\n",
       " 'Ort: Erbsengasse',\n",
       " 'Ort: Eremitage',\n",
       " 'Ort: \" Eremitage \" - Museums',\n",
       " 'Ort: Erfurt',\n",
       " 'Ort: Ergo Bibamus',\n",
       " 'Ort: Erich-Kästner-Schule',\n",
       " 'Ort: ERLANGEN',\n",
       " 'Ort: Erlenbacher Stadtweg',\n",
       " 'Ort: Erlenbach Stadtweg',\n",
       " 'Ort: Ernst-Kahn-Straße',\n",
       " 'Ort: Erzhausen',\n",
       " 'Ort: Eschborn',\n",
       " 'Ort: ESCHBORN',\n",
       " 'Ort: Eschenheimer Anlage',\n",
       " 'Ort: Eschenheimer Turm',\n",
       " 'Ort: Eschenplatz',\n",
       " 'Ort: Eschersheim',\n",
       " 'Ort: ESCHERSHEIM',\n",
       " 'Ort: Eschersheimer Landstraße',\n",
       " 'Ort: Eschwege',\n",
       " 'Ort: Espelkamp',\n",
       " 'Ort: Essen-Frintrop',\n",
       " 'Ort: Estland',\n",
       " 'Ort: Estlands',\n",
       " 'Ort: ESWE-Hochhaus',\n",
       " 'Ort: Eulenstr.',\n",
       " 'Ort: Europa',\n",
       " 'Ort: Europäischen',\n",
       " 'Ort: Europas',\n",
       " 'Ort: Evangelischen AndreasGemeinde',\n",
       " 'Ort: evangelischen Dornbuschgemeinde',\n",
       " 'Ort: evangelischen Gemeindehaus NiederEschbach',\n",
       " 'Ort: evangelischen Limesgemeinde',\n",
       " 'Ort: Ex-DDR',\n",
       " 'Ort: Ex-Jugoslawien',\n",
       " 'Ort: Ex-Jugoslawiens',\n",
       " 'Ort: Fährenstraße',\n",
       " 'Ort: Falkenheim',\n",
       " 'Ort: Falkenstein',\n",
       " 'Ort: Falkstraße',\n",
       " 'Ort: Farkas',\n",
       " 'Ort: Faulenberg',\n",
       " 'Ort: Fechenheim',\n",
       " 'Ort: FECHENHEIM',\n",
       " 'Ort: Feldbergstraße',\n",
       " 'Ort: Ferdinandstraße',\n",
       " 'Ort: Festplatz Hügelstraße',\n",
       " 'Ort: Festzelt Michaelismesse',\n",
       " 'Ort: Feuchtwangen',\n",
       " 'Ort: Feuerbachstraße',\n",
       " 'Ort: Feuerwache',\n",
       " 'Ort: Fichardstraße',\n",
       " 'Ort: Fidschi-Inseln',\n",
       " 'Ort: Fife',\n",
       " 'Ort: Filmmuseum',\n",
       " 'Ort: Finanzplatz Berlin',\n",
       " 'Ort: Finkenhofstraße',\n",
       " 'Ort: Finnland',\n",
       " 'Ort: FISCHBACHTAL',\n",
       " 'Ort: Flachstraße',\n",
       " 'Ort: Flanderns',\n",
       " 'Ort: Flaschenburgstraße',\n",
       " 'Ort: Fliederweg',\n",
       " 'Ort: Flörsheim',\n",
       " 'Ort: FLÖRSHEIM',\n",
       " 'Ort: Flößer- und Deutschherrnbrükke',\n",
       " 'Ort: Forst Roßberg',\n",
       " 'Ort: Franckfurt',\n",
       " 'Ort: Frankenallee',\n",
       " 'Ort: Frankfurt',\n",
       " 'Ort: FRANKFURT A. M.',\n",
       " 'Ort: Frankfurt am Main',\n",
       " 'Ort: FRANKFURT-CITY',\n",
       " 'Ort: Frankfurter Berg',\n",
       " 'Ort: FRANKFURTER BERG',\n",
       " 'Ort: Frankfurter Forst',\n",
       " 'Ort: Frankfurter Landstraße',\n",
       " 'Ort: Frankfurter Römer',\n",
       " 'Ort: Frankfurter Straße',\n",
       " 'Ort: FRANKFURT-NORD',\n",
       " 'Ort: FRANKFURT-NORDWEST',\n",
       " 'Ort: FRANKFURT-OST',\n",
       " 'Ort: Frankfurts',\n",
       " 'Ort: FRANKFURT-SÜD',\n",
       " 'Ort: FRANKFURT-WEST',\n",
       " 'Ort: Frankreich',\n",
       " 'Ort: Frankreich-D-KORR',\n",
       " 'Ort: Frauenzentrum',\n",
       " 'Ort: Frauenzentrum Friedberg',\n",
       " 'Ort: Freibad Maaraue',\n",
       " 'Ort: Freiburg',\n",
       " 'Ort: freien Welt',\n",
       " 'Ort: Freiheitsplatz',\n",
       " 'Ort: Freiheitsstatue',\n",
       " 'Ort: Freiherr-vomStein-Straße',\n",
       " 'Ort: Freilichttheater',\n",
       " 'Ort: Fridericianum',\n",
       " 'Ort: Friedberg',\n",
       " 'Ort: FRIEDBERG',\n",
       " 'Ort: Friedberger Landstraße',\n",
       " 'Ort: Friedberger Straße',\n",
       " 'Ort: Friedel-Baureis-Haus',\n",
       " 'Ort: Friedensbrücke',\n",
       " 'Ort: Friedrich-Ebert-Anlage',\n",
       " 'Ort: Friedrich-Ebert-Schule',\n",
       " 'Ort: Friedrich-Ebert-Straße',\n",
       " 'Ort: Friedrichsdorf',\n",
       " 'Ort: FRIEDRICHSDORF',\n",
       " 'Ort: Friedrichsheim',\n",
       " 'Ort: Friesengasse',\n",
       " 'Ort: Fritz-Tarnow-Straße',\n",
       " 'Ort: FTG-Sportzentrum',\n",
       " 'Ort: Fuchstanzstraße',\n",
       " 'Ort: Fulda',\n",
       " 'Ort: Fuldaquelle',\n",
       " 'Ort: Fürstenlager',\n",
       " 'Ort: Fürstenlagers',\n",
       " 'Ort: Fürth',\n",
       " 'Ort: Galerie Heussenstamm-Stiftung',\n",
       " 'Ort: Gallus',\n",
       " 'Ort: GALLUS',\n",
       " 'Ort: Gallusparks',\n",
       " 'Ort: Gallusviertel',\n",
       " 'Ort: Galluswarte',\n",
       " 'Ort: Gamburg',\n",
       " 'Ort: Gartenschaugeländes',\n",
       " 'Ort: Gartenstraße',\n",
       " 'Ort: Gässi',\n",
       " 'Ort: Gaußstraße',\n",
       " 'Ort: Gebeschusstraße',\n",
       " 'Ort: Gebsattel',\n",
       " 'Ort: Geisenheim',\n",
       " 'Ort: Gelben Foyer',\n",
       " 'Ort: Gelnhausen',\n",
       " 'Ort: GELNHAUSEN',\n",
       " 'Ort: Gelnhäuser Coleman-Kaserne',\n",
       " 'Ort: Gelnhäuser Straße',\n",
       " 'Ort: Gemarkung Fronhausen',\n",
       " 'Ort: Gemeinde Schwanheim',\n",
       " 'Ort: Gemeinde Zwesten',\n",
       " 'Ort: Gendorf',\n",
       " 'Ort: Genf',\n",
       " 'Ort: Georgien',\n",
       " 'Ort: Georgiens',\n",
       " 'Ort: Georg-Muth-Haus',\n",
       " 'Ort: Gerätehaus Dillgasse',\n",
       " 'Ort: Gerhart-Hauptmann-Ring',\n",
       " 'Ort: Gernsbach',\n",
       " 'Ort: Gesamt-Berlin',\n",
       " 'Ort: Geschwister-Scholl-Schule',\n",
       " 'Ort: Gewerbepark',\n",
       " 'Ort: GIESSEN',\n",
       " 'Ort: Gießen',\n",
       " 'Ort: Ginnheim',\n",
       " 'Ort: GINNHEIM',\n",
       " 'Ort: Ginnheimer Landstraße',\n",
       " 'Ort: Ginnheimer Wäldchen',\n",
       " 'Ort: Ginsterweg',\n",
       " 'Ort: Glauburg',\n",
       " 'Ort: Glauburg-',\n",
       " 'Ort: Glauburgplatz',\n",
       " 'Ort: Glauburgstraße',\n",
       " 'Ort: Gluburg-',\n",
       " 'Ort: Gluckstraße',\n",
       " 'Ort: Goethegymnasiums',\n",
       " 'Ort: Goldbergweg',\n",
       " 'Ort: Goldenen Apfel',\n",
       " 'Ort: Goldshöfe',\n",
       " 'Ort: Goldstein',\n",
       " 'Ort: GOLDSTEIN',\n",
       " 'Ort: Goldsteiner Sportanlage',\n",
       " 'Ort: Goldsteinpark',\n",
       " 'Ort: Goldsteinparks',\n",
       " 'Ort: Goldsteins',\n",
       " 'Ort: Goldstein-Siedlung',\n",
       " 'Ort: Goldsteinstraße',\n",
       " 'Ort: Goldstein-Süd',\n",
       " 'Ort: Gonzenheim',\n",
       " 'Ort: Göpfert-Haus',\n",
       " 'Ort: Gorazde',\n",
       " 'Ort: Görlitz',\n",
       " 'Ort: Görlitzer Straße',\n",
       " 'Ort: Gotha',\n",
       " 'Ort: GrabowBuckow',\n",
       " 'Ort: Graebestraße',\n",
       " 'Ort: Gräf-',\n",
       " 'Ort: Gräfelfing',\n",
       " 'Ort: Gräfendeichstraße',\n",
       " 'Ort: Gräfstraße',\n",
       " 'Ort: Grasellenbach',\n",
       " 'Ort: Gravenbruch',\n",
       " 'Ort: Greiffenclauschen Hauses',\n",
       " 'Ort: Grempstraße',\n",
       " 'Ort: Grethenweg',\n",
       " 'Ort: Griechenland',\n",
       " 'Ort: Griesheim',\n",
       " 'Ort: GRIESHEIM',\n",
       " 'Ort: Griesheimer Sporthalle',\n",
       " 'Ort: Griesheimer Uferweg',\n",
       " 'Ort: Grillplatz Bonames',\n",
       " 'Ort: Grönland',\n",
       " 'Ort: Großbritannien',\n",
       " 'Ort: Große Nelkenstraße',\n",
       " 'Ort: Großen Haus',\n",
       " 'Ort: Größe Rosbachs',\n",
       " 'Ort: GROSS-GERAU',\n",
       " 'Ort: Groß-Gerau',\n",
       " 'Ort: Groß-Karben',\n",
       " 'Ort: Grüner Weg',\n",
       " 'Ort: Grünsammelstelle Mörfelden',\n",
       " 'Ort: Guebwiller',\n",
       " 'Ort: Gujarat',\n",
       " 'Ort: Gujarats',\n",
       " 'Ort: Gummersbach',\n",
       " 'Ort: Günthersburgpark',\n",
       " 'Ort: Günthersburgparks',\n",
       " 'Ort: GUS',\n",
       " 'Ort: Gutleut',\n",
       " 'Ort: Gutleutstraße',\n",
       " 'Ort: Gutleutviertel',\n",
       " 'Ort: Gutzkowstraße',\n",
       " 'Ort: Gwinnerstraße',\n",
       " 'Ort: Habelstraße',\n",
       " 'Ort: Halle Beuel',\n",
       " 'Ort: Hallenbad Sachsenhausen',\n",
       " 'Ort: Hamburg',\n",
       " 'Ort: Hamburger Allee',\n",
       " 'Ort: Hamburger Deichtorhallen',\n",
       " 'Ort: Hammanstraße',\n",
       " 'Ort: Hammersbach',\n",
       " 'Ort: HAMMERSBACH',\n",
       " 'Ort: Hanau',\n",
       " 'Ort: HANAU',\n",
       " 'Ort: Hanauer Landstraße',\n",
       " 'Ort: Hanauer Stadtkrankenhaus',\n",
       " 'Ort: Hanau-Rodenbach',\n",
       " 'Ort: Hanaus',\n",
       " 'Ort: Hanau-Wilhelmsbad',\n",
       " 'Ort: Hannover',\n",
       " 'Ort: Hansaallee',\n",
       " 'Ort: Harheim',\n",
       " 'Ort: HARHEIM',\n",
       " 'Ort: Hartmannsweiler Straße',\n",
       " 'Ort: Harvestehuder Weg',\n",
       " 'Ort: Hasloch',\n",
       " 'Ort: Hattersheim',\n",
       " 'Ort: HATTERSHEIM',\n",
       " 'Ort: Hattersheimer Straße',\n",
       " 'Ort: Hauptbahnhof',\n",
       " 'Ort: Hauptstraße',\n",
       " 'Ort: Hauptwache',\n",
       " 'Ort: Haus Dornbusch',\n",
       " 'Ort: Haus Dr. Bäck',\n",
       " 'Ort: Haus Eckenheim',\n",
       " 'Ort: Hausen',\n",
       " 'Ort: HAUSEN',\n",
       " 'Ort: Hausener Mühle',\n",
       " 'Ort: Hausenern',\n",
       " 'Ort: Hausener Weg',\n",
       " 'Ort: Häuser Marktstraße',\n",
       " 'Ort: Häusermeer',\n",
       " 'Ort: Hauses Mertens',\n",
       " 'Ort: Haus Gallus',\n",
       " 'Ort: Haus Nied',\n",
       " 'Ort: Haus Ronneburg',\n",
       " 'Ort: Haus Seeblick',\n",
       " 'Ort: Haus zum Löwen',\n",
       " 'Ort: Havanna',\n",
       " 'Ort: Haydau',\n",
       " 'Ort: Hebelstraße',\n",
       " 'Ort: Heddernheim',\n",
       " 'Ort: HEDDERNHEIM',\n",
       " 'Ort: Heddernheimer Kirchstraße',\n",
       " 'Ort: Heddernheimer Landstraße',\n",
       " 'Ort: Heddernheimers',\n",
       " 'Ort: Heddernheims',\n",
       " 'Ort: Heide',\n",
       " 'Ort: Heidelberg',\n",
       " 'Ort: Heidelberger Schwimmbad',\n",
       " 'Ort: Heilbronn',\n",
       " 'Ort: Heiligenstockwegs',\n",
       " 'Ort: Heiliggeistkirche',\n",
       " 'Ort: Heimatmuseums-Muff',\n",
       " 'Ort: Heinestraße',\n",
       " 'Ort: Heinrich-Becker-Straße',\n",
       " 'Ort: Heinrich-Hoffmann-Straße',\n",
       " 'Ort: Heinrich-von Brentano-Schule',\n",
       " 'Ort: Heister-',\n",
       " 'Ort: Heisterstraße',\n",
       " 'Ort: Heiterkeit',\n",
       " 'Ort: Hellerbrücke',\n",
       " 'Ort: Hemsberg',\n",
       " 'Ort: Henri-Dunant-Ring',\n",
       " 'Ort: Heppenheim',\n",
       " 'Ort: Hermannstraße',\n",
       " 'Ort: Herrenwingert',\n",
       " 'Ort: Herrnstraße',\n",
       " 'Ort: Herxheimer Straße',\n",
       " 'Ort: Herzbachweg',\n",
       " 'Ort: Hesse',\n",
       " 'Ort: Hessen',\n",
       " 'Ort: Hessenland',\n",
       " 'Ort: Hessenpark',\n",
       " 'Ort: Hessenplatz',\n",
       " 'Ort: Hessenplatzes',\n",
       " 'Ort: Hessens',\n",
       " 'Ort: Hessische',\n",
       " 'Ort: HESSISCHE',\n",
       " 'Ort: Hessischen Gemeinschaftsunterkunft',\n",
       " 'Ort: Hessische Rhön',\n",
       " 'Ort: Hessisches Puppenmuseum',\n",
       " 'Ort: Heusenstamm',\n",
       " 'Ort: HEUSENSTAMM',\n",
       " 'Ort: Heussenstamm-Stiftung',\n",
       " 'Ort: Hilfezentrum im Hufeland-Haus',\n",
       " 'Ort: Hilligengasse',\n",
       " 'Ort: Hindenberg',\n",
       " 'Ort: Hinter der Burg',\n",
       " 'Ort: Hinterhaus',\n",
       " 'Ort: Hirsch-',\n",
       " 'Ort: Hirschhornstraße',\n",
       " 'Ort: HL-Parkplatz',\n",
       " 'Ort: Hochheim',\n",
       " 'Ort: HOCHHEIM',\n",
       " 'Ort: Hochrheintal',\n",
       " 'Ort: Höchst',\n",
       " 'Ort: HÖCHST',\n",
       " 'Ort: Höchster Bahnhof',\n",
       " 'Ort: Höchster Schloßpark',\n",
       " 'Ort: Höchster Straße',\n",
       " 'Ort: Hochstraße',\n",
       " 'Ort: Höchsts',\n",
       " 'Ort: Hochtaunus',\n",
       " 'Ort: HOCHTAUNUS',\n",
       " 'Ort: Hochtaunuskreis',\n",
       " 'Ort: HOCHTAUNUSKREIS',\n",
       " 'Ort: Hoechst',\n",
       " 'Ort: Hofbräuhaus',\n",
       " 'Ort: Hofgut Goldstein',\n",
       " 'Ort: Hofhausstraße',\n",
       " 'Ort: Hofheim',\n",
       " 'Ort: HOFHEIM',\n",
       " 'Ort: Hofheimer Kulturamt',\n",
       " 'Ort: Höhe',\n",
       " 'Ort: Hohenstaufenstraße',\n",
       " 'Ort: Hohestein / Ekkenberg',\n",
       " 'Ort: Hohestraße',\n",
       " 'Ort: Holbeinstraße',\n",
       " 'Ort: Holland',\n",
       " 'Ort: Hollywood',\n",
       " 'Ort: Holzhausenpark',\n",
       " 'Ort: Holzhausenstraße',\n",
       " 'Ort: Holzhecke',\n",
       " 'Ort: Holzmannstraße',\n",
       " 'Ort: Homberg an der Ohm',\n",
       " 'Ort: Homburg',\n",
       " 'Ort: Homburger Landstraße',\n",
       " 'Ort: Homburgs',\n",
       " 'Ort: Horkheimers',\n",
       " 'Ort: Hotelflur',\n",
       " 'Ort: Hungerbach',\n",
       " 'Ort: Hyde Park',\n",
       " 'Ort: Idschewan',\n",
       " 'Ort: Idstein',\n",
       " 'Ort: Illingen',\n",
       " 'Ort: Im Mainfeld',\n",
       " 'Ort: Im Sauern',\n",
       " 'Ort: Im Staffel',\n",
       " 'Ort: Im Steinchen',\n",
       " 'Ort: Indien',\n",
       " 'Ort: Indiens',\n",
       " 'Ort: Indochina',\n",
       " 'Ort: Industriehof',\n",
       " 'Ort: INDUSTRIEHOF',\n",
       " 'Ort: Industriehofs',\n",
       " 'Ort: Ingelheim',\n",
       " 'Ort: Innenstadt',\n",
       " 'Ort: INNENSTADT',\n",
       " 'Ort: Iran',\n",
       " 'Ort: Irland',\n",
       " 'Ort: Israel',\n",
       " 'Ort: israelischen Staat',\n",
       " 'Ort: Israels',\n",
       " 'Ort: Istanbul',\n",
       " 'Ort: Istra',\n",
       " 'Ort: Italien',\n",
       " 'Ort: Jahrhunderthalle',\n",
       " 'Ort: Japan',\n",
       " 'Ort: Japanerinnen',\n",
       " 'Ort: Japanische',\n",
       " 'Ort: Jazzkeller',\n",
       " 'Ort: Jazzlife',\n",
       " 'Ort: Jena',\n",
       " 'Ort: Johanna-Kirchner-Altenhilfezentrum',\n",
       " 'Ort: Johanna-Melber-Weg',\n",
       " 'Ort: Johanniter-Abtei',\n",
       " 'Ort: Johanniter-Cronstetten-Altenhilfe',\n",
       " 'Ort: Johanniterturm',\n",
       " 'Ort: Johanniterturms',\n",
       " 'Ort: Johann-Kirchner-Straße',\n",
       " 'Ort: John-F .',\n",
       " 'Ort: Josefskirche',\n",
       " 'Ort: Josefsklause',\n",
       " 'Ort: Jubiläumspark',\n",
       " 'Ort: Judengasse',\n",
       " 'Ort: Judenviertel',\n",
       " 'Ort: Jugendhaus Bonames',\n",
       " 'Ort: Jugendhaus Goldstein',\n",
       " 'Ort: Jugendhaus Goldstein / Schwanheim',\n",
       " 'Ort: Jugendräume',\n",
       " 'Ort: Jugoslawien',\n",
       " 'Ort: Jüterbog',\n",
       " 'Ort: Kaichen',\n",
       " 'Ort: Kaiser-Friedrich-Ring',\n",
       " 'Ort: Kaiser-Sigmund-Straße',\n",
       " 'Ort: Kaiserstraße',\n",
       " 'Ort: Kalbach',\n",
       " 'Ort: KALBACH',\n",
       " 'Ort: Kalbacher Stadtpfad',\n",
       " 'Ort: Kalbacher Straße',\n",
       " 'Ort: Kallebad',\n",
       " 'Ort: Kaltmühle',\n",
       " 'Ort: Kammerspielen',\n",
       " 'Ort: Kanada',\n",
       " 'Ort: Kapellenstraße',\n",
       " 'Ort: Kapersburghütte',\n",
       " 'Ort: Kappelberg',\n",
       " 'Ort: Karben',\n",
       " 'Ort: KARBEN',\n",
       " 'Ort: Karibik',\n",
       " 'Ort: Karl-Eckel-Weg',\n",
       " 'Ort: Karl-Ekkel-Weg',\n",
       " 'Ort: Karl-Kirchner-Siedlung',\n",
       " 'Ort: Karl-Marx-Buchhandlung',\n",
       " 'Ort: Karlsruhe',\n",
       " 'Ort: Karlstraße',\n",
       " 'Ort: Karpaten',\n",
       " 'Ort: Kasach',\n",
       " 'Ort: Kassel',\n",
       " 'Ort: KASSEL',\n",
       " 'Ort: Kastellstraße',\n",
       " 'Ort: Katakombe',\n",
       " 'Ort: Katharinen- und Weißfrauenstift',\n",
       " 'Ort: Käthe-Kollwitz-Haus',\n",
       " 'Ort: Kaukasus',\n",
       " 'Ort: Keglerklause',\n",
       " 'Ort: Kelkheim',\n",
       " 'Ort: Kelsterbach',\n",
       " 'Ort: KELSTERBACH',\n",
       " 'Ort: Kelsterbacher Knoten',\n",
       " 'Ort: Kelsterbacher Knotens',\n",
       " 'Ort: Kelsterbacher Weg',\n",
       " 'Ort: Kelsterbachs',\n",
       " 'Ort: Kennedyallee',\n",
       " 'Ort: - Kennedy-Schule',\n",
       " 'Ort: Kennedy-Schule',\n",
       " 'Ort: Kevadia',\n",
       " 'Ort: Kiefernstraße',\n",
       " 'Ort: Kiew',\n",
       " 'Ort: Kildare',\n",
       " 'Ort: Kimbach',\n",
       " 'Ort: Kinderhorts Siemensallee',\n",
       " 'Ort: Kindertagesstätte',\n",
       " 'Ort: Kindertagesstätte Georgenstraße',\n",
       " 'Ort: Kindertagesstätte Okarben',\n",
       " 'Ort: Kirche Ramholz',\n",
       " 'Ort: Kirchgasse',\n",
       " 'Ort: KirchGöns',\n",
       " 'Ort: Kirch-Göns',\n",
       " 'Ort: Kirch-Gönser Gemarkung',\n",
       " 'Ort: Kirchholz',\n",
       " 'Ort: Kirchplatz',\n",
       " 'Ort: Kirchzarten',\n",
       " 'Ort: Kirdorf',\n",
       " 'Ort: Kirow',\n",
       " 'Ort: Klapperbrunnen',\n",
       " 'Ort: Klappergass',\n",
       " \"Ort: Klappergass'-\",\n",
       " 'Ort: Klappergasse',\n",
       " 'Ort: Klatschmohn',\n",
       " 'Ort: Kleingartenanlage Zur Waldau',\n",
       " 'Ort: Kleingartenverein Miquel',\n",
       " 'Ort: Klein-Gerau',\n",
       " 'Ort: Kleyerstraße',\n",
       " 'Ort: Kloppenheim',\n",
       " 'Ort: Klosterhof',\n",
       " 'Ort: Klosterhof Schlüchtern',\n",
       " 'Ort: Klütz',\n",
       " 'Ort: Klützer Winkel',\n",
       " 'Ort: Knodenkopf',\n",
       " 'Ort: Köbeler Weg',\n",
       " 'Ort: Kochbrunnen',\n",
       " 'Ort: Kohlheck-Apotheke',\n",
       " 'Ort: Kollwitzstraße',\n",
       " 'Ort: Köln',\n",
       " 'Ort: Kolumbien',\n",
       " 'Ort: Kolumbiens',\n",
       " 'Ort: Kommunale Kino',\n",
       " 'Ort: Königsberg',\n",
       " 'Ort: Königshofen',\n",
       " 'Ort: Königstein',\n",
       " 'Ort: KÖNIGSTEIN',\n",
       " 'Ort: Königsteiner Straße',\n",
       " 'Ort: Konrad-von-Preysing-Haus',\n",
       " 'Ort: Kopfbahnhof',\n",
       " 'Ort: Korffstraße',\n",
       " 'Ort: Koselstraße',\n",
       " 'Ort: Kosovo',\n",
       " 'Ort: Krehberg',\n",
       " 'Ort: Kreiskrankenhaus Bad Soden',\n",
       " 'Ort: Kreuzberg',\n",
       " 'Ort: Kriegkstraße',\n",
       " 'Ort: Kriftel',\n",
       " 'Ort: KRIFTEL',\n",
       " 'Ort: Krifteler Straße',\n",
       " 'Ort: Kroatien',\n",
       " 'Ort: Kronberg',\n",
       " 'Ort: Kronberg-Schönberg',\n",
       " 'Ort: KT-Bistro',\n",
       " 'Ort: Kuba',\n",
       " 'Ort: Kuhwald',\n",
       " 'Ort: Kulmbach',\n",
       " 'Ort: Külsheim',\n",
       " 'Ort: Kulturcafé',\n",
       " 'Ort: Kunstgewerbemuseum Berlin',\n",
       " 'Ort: Kurhaus-',\n",
       " 'Ort: Kurhessenstraße',\n",
       " 'Ort: Kurpark',\n",
       " 'Ort: Kutchch',\n",
       " 'Ort: La Florida',\n",
       " 'Ort: Lahn-Dill-Kreis',\n",
       " 'Ort: Lahndreiecks',\n",
       " 'Ort: Lahnstraße',\n",
       " 'Ort: La Matalla',\n",
       " 'Ort: Lampertheim',\n",
       " 'Ort: Landau',\n",
       " 'Ort: LANDAU',\n",
       " 'Ort: Landes Hessen',\n",
       " 'Ort: Landes Nordrhein-Westfalen',\n",
       " 'Ort: Landgerichts Darmstadt',\n",
       " 'Ort: Landgraben',\n",
       " 'Ort: Land Hessen',\n",
       " 'Ort: Landkreis Würzburg',\n",
       " 'Ort: Landstraße',\n",
       " 'Ort: Langen',\n",
       " 'Ort: Langenbergheim',\n",
       " 'Ort: Langen Steg',\n",
       " 'Ort: Langer Weg',\n",
       " 'Ort: Lateinamerika',\n",
       " 'Ort: Lateinamerikas',\n",
       " 'Ort: Lauda',\n",
       " 'Ort: Leinfelden',\n",
       " 'Ort: Leipzig',\n",
       " 'Ort: Leipziger Straße',\n",
       " 'Ort: Lenaustraße',\n",
       " 'Ort: Leningrad',\n",
       " 'Ort: Leninskij Prospekt',\n",
       " 'Ort: Lerchesberg',\n",
       " 'Ort: Lerchesbergs',\n",
       " 'Ort: Lersnerstraße',\n",
       " 'Ort: Lettland',\n",
       " 'Ort: Leuchte',\n",
       " 'Ort: Leunabrücke',\n",
       " 'Ort: Leunastraße',\n",
       " 'Ort: Levern',\n",
       " 'Ort: Lichtenberg',\n",
       " 'Ort: Lichtenfels',\n",
       " 'Ort: Liebigstraße',\n",
       " 'Ort: Liederbach',\n",
       " 'Ort: Lima',\n",
       " 'Ort: LIMA',\n",
       " 'Ort: Limburg',\n",
       " 'Ort: Limburg-Weilburg',\n",
       " 'Ort: Linkstraße',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "file_path_deu = Path.cwd() / \"data\" / \"deu.list\"\n",
    "file_path_eng = Path.cwd() / \"data\" / \"eng.list\"\n",
    "file_path_ned = Path.cwd() / \"data\" / \"ned.list.PER\"\n",
    "\n",
    "# Function to read and process file into a DataFrame\n",
    "def read_and_process_file(file_path):\n",
    "    with open(file_path, 'r',encoding='latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "    data = [line.strip().split(' ', 1) for line in lines]\n",
    "    return pd.DataFrame(data, columns=['NER', 'Entity'])\n",
    "\n",
    "# Read and process files\n",
    "NER_df_deu = read_and_process_file(file_path_deu)\n",
    "NER_df_eng = read_and_process_file(file_path_eng)\n",
    "NER_df_ned = read_and_process_file(file_path_ned)\n",
    "\n",
    "# Filter English DataFrame to use only 'PER' entities\n",
    "NER_df_eng = NER_df_eng[NER_df_eng['NER'] == 'PER']\n",
    "\n",
    "# Combine all DataFrames\n",
    "NER_df = pd.concat([NER_df_deu, NER_df_eng, NER_df_ned])\n",
    "\n",
    "NER_df['NER'] = NER_df['NER'].replace('LOC', 'Ort:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('PER', 'Person:')\n",
    "NER_df['NER'] = NER_df['NER'].replace('ORG', 'Organisation:')\n",
    "\n",
    "# Convert DataFrame to list of elements\n",
    "def create_training_data(row):\n",
    "    if row['NER'] == 'MISC':\n",
    "        return f\"{row['Entity']}\"\n",
    "    else:\n",
    "        return f\"{row['NER']} {row['Entity']}\"\n",
    "\n",
    "NER_list = [create_training_data(row) for _, row in NER_df.iterrows()]\n",
    "NER_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 5</td>\n",
       "      <td>[A, 5]</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "      <td>[[0.5321832895278931, 0.15855109691619873, 0.2...</td>\n",
       "      <td>[-0.24310845136642456, 0.13687995076179504, -0...</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 66</td>\n",
       "      <td>[A, 66]</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "      <td>[[0.4871409237384796, 0.3022191524505615, 0.23...</td>\n",
       "      <td>[0.32661712169647217, 0.14974229037761688, 0.1...</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 661</td>\n",
       "      <td>[A, 66, ##1]</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "      <td>[[0.33346572518348694, 0.23834776878356934, -0...</td>\n",
       "      <td>[0.12139879912137985, 0.2494209259748459, 0.05...</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalen</td>\n",
       "      <td>[A, ##alen]</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "      <td>[[0.07099006325006485, -0.5678775310516357, 1....</td>\n",
       "      <td>[-0.003420088440179825, -0.260809063911438, 0....</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aare-Mündung</td>\n",
       "      <td>[Aar, ##e, -, Mündung]</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "      <td>[[0.3378963768482208, -0.5541061162948608, -0....</td>\n",
       "      <td>[0.2187633365392685, -0.3105980157852173, 0.07...</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47762</th>\n",
       "      <td>Zyg</td>\n",
       "      <td>[Zy, ##g]</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "      <td>[[-0.36112070083618164, -0.00450560450553894, ...</td>\n",
       "      <td>[0.005554556846618652, 0.19277839362621307, -0...</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47763</th>\n",
       "      <td>Zygmunt</td>\n",
       "      <td>[Zy, ##g, ##m, ##unt]</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "      <td>[[-0.20858515799045563, -0.4874523878097534, -...</td>\n",
       "      <td>[0.246858611702919, -0.13464316725730896, -0.2...</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47764</th>\n",
       "      <td>Zyj</td>\n",
       "      <td>[Zy, ##j]</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "      <td>[[-0.07781310379505157, 0.8064130544662476, -0...</td>\n",
       "      <td>[-0.12115689367055893, 0.37860164046287537, -0...</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47765</th>\n",
       "      <td>Zyk</td>\n",
       "      <td>[Zy, ##k]</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "      <td>[[-0.4067027270793915, 0.1616303026676178, 0.6...</td>\n",
       "      <td>[-0.3863324820995331, -0.24419178068637848, 0....</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47766</th>\n",
       "      <td>Zyr</td>\n",
       "      <td>[Zy, ##r]</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "      <td>[[1.0486658811569214, -0.38755977153778076, 0....</td>\n",
       "      <td>[0.7094437479972839, -0.18272826075553894, 0.1...</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47767 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entity                  Tokens                        Token_IDs  \\\n",
       "0               A 5                  [A, 5]                     [32, 435, 4]   \n",
       "1              A 66                 [A, 66]                   [32, 10092, 4]   \n",
       "2             A 661            [A, 66, ##1]            [32, 10092, 26927, 4]   \n",
       "3             Aalen             [A, ##alen]                     [32, 609, 4]   \n",
       "4      Aare-Mündung  [Aar, ##e, -, Mündung]  [15576, 26897, 26935, 14955, 4]   \n",
       "...             ...                     ...                              ...   \n",
       "47762           Zyg               [Zy, ##g]                [10373, 26908, 4]   \n",
       "47763       Zygmunt   [Zy, ##g, ##m, ##unt]   [10373, 26908, 26911, 1937, 4]   \n",
       "47764           Zyj               [Zy, ##j]                [10373, 26963, 4]   \n",
       "47765           Zyk               [Zy, ##k]                [10373, 26917, 4]   \n",
       "47766           Zyr               [Zy, ##r]                [10373, 26900, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "0      [[0.5321832895278931, 0.15855109691619873, 0.2...   \n",
       "1      [[0.4871409237384796, 0.3022191524505615, 0.23...   \n",
       "2      [[0.33346572518348694, 0.23834776878356934, -0...   \n",
       "3      [[0.07099006325006485, -0.5678775310516357, 1....   \n",
       "4      [[0.3378963768482208, -0.5541061162948608, -0....   \n",
       "...                                                  ...   \n",
       "47762  [[-0.36112070083618164, -0.00450560450553894, ...   \n",
       "47763  [[-0.20858515799045563, -0.4874523878097534, -...   \n",
       "47764  [[-0.07781310379505157, 0.8064130544662476, -0...   \n",
       "47765  [[-0.4067027270793915, 0.1616303026676178, 0.6...   \n",
       "47766  [[1.0486658811569214, -0.38755977153778076, 0....   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "0      [-0.24310845136642456, 0.13687995076179504, -0...   \n",
       "1      [0.32661712169647217, 0.14974229037761688, 0.1...   \n",
       "2      [0.12139879912137985, 0.2494209259748459, 0.05...   \n",
       "3      [-0.003420088440179825, -0.260809063911438, 0....   \n",
       "4      [0.2187633365392685, -0.3105980157852173, 0.07...   \n",
       "...                                                  ...   \n",
       "47762  [0.005554556846618652, 0.19277839362621307, -0...   \n",
       "47763  [0.246858611702919, -0.13464316725730896, -0.2...   \n",
       "47764  [-0.12115689367055893, 0.37860164046287537, -0...   \n",
       "47765  [-0.3863324820995331, -0.24419178068637848, 0....   \n",
       "47766  [0.7094437479972839, -0.18272826075553894, 0.1...   \n",
       "\n",
       "                      Target_Token_IDs  \n",
       "0                         [32, 435, 4]  \n",
       "1                       [32, 10092, 4]  \n",
       "2                [32, 10092, 26927, 4]  \n",
       "3                         [32, 609, 4]  \n",
       "4      [15576, 26897, 26935, 14955, 4]  \n",
       "...                                ...  \n",
       "47762                [10373, 26908, 4]  \n",
       "47763   [10373, 26908, 26911, 1937, 4]  \n",
       "47764                [10373, 26963, 4]  \n",
       "47765                [10373, 26917, 4]  \n",
       "47766                [10373, 26900, 4]  \n",
       "\n",
       "[47767 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "model = BertModel.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Function to get embeddings for a list of tokens\n",
    "def get_embeddings(tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: torch.tensor(v).unsqueeze(0) for k, v in tokens.items()})\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "# Process each element in NER_list\n",
    "rows = []\n",
    "for element in NER_list:\n",
    "    \n",
    "    full_tokens = tokenizer(element, padding=False, truncation=False, add_special_tokens=True)\n",
    "    \n",
    "    # Get embeddings for the full tokenized input\n",
    "    full_embeddings = get_embeddings(full_tokens)\n",
    "    \n",
    "    # Split the element into NER type and entity\n",
    "    parts = element.split(': ', 1)\n",
    "    entity = parts[1] if len(parts) > 1 else element\n",
    "    \n",
    "    # Tokenize the entity separately (without special tokens)\n",
    "    entity_tokens = tokenizer(entity, padding=False, truncation=False, add_special_tokens=False)\n",
    "    \n",
    "    # Find the start index of entity tokens in the full tokenization\n",
    "    start_index = 1  # Skip [CLS]\n",
    "    if len(parts) > 1:\n",
    "        start_index += len(tokenizer(parts[0] + ':', add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    # Process each token in the entity\n",
    "    tokens = []\n",
    "    token_ids = []\n",
    "    token_embeddings = []\n",
    "    for i, token_id in enumerate(entity_tokens['input_ids']):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        token_embedding = full_embeddings[start_index + i]\n",
    "        \n",
    "        tokens.append(token)\n",
    "        token_ids.append(token_id)\n",
    "        token_embeddings.append(token_embedding.tolist())\n",
    "    \n",
    "    # Add the end of sequence token ID\n",
    "    token_ids.append(4) #[SEP]\n",
    "    token_ids.insert(0,3) #[CLS]\n",
    "    # Calculate average embedding for the entity tokens\n",
    "    avg_embedding = torch.mean(full_embeddings[start_index:start_index+len(entity_tokens['input_ids'])], dim=0).tolist()\n",
    "    \n",
    "    \n",
    "    rows.append({\n",
    "        'Entity': entity,\n",
    "        'Tokens': tokens,\n",
    "        'Token_IDs': token_ids,#token_ids[1:-2]\n",
    "        'Token_Embeddings': token_embeddings,\n",
    "        'Average_Embedding': avg_embedding,\n",
    "        'Target_Token_IDs': token_ids\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "# check if any element in the list contains '[UNK]'\n",
    "def contains_unk(token_list):\n",
    "    return any('[UNK]' in token for token in token_list)\n",
    "\n",
    "# Filter out rows where the Token column contains '[UNK]'\n",
    "df = df[~df['Tokens'].apply(contains_unk)]\n",
    "# Filter out rows that are longer than 15 Tokens \n",
    "df = df[df['Target_Token_IDs'].apply(len) <= 15]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 15\n",
      "Target_Token_IDs\n",
      "2      2115\n",
      "3     13098\n",
      "4     12997\n",
      "5      8888\n",
      "6      5205\n",
      "7      2500\n",
      "8      1037\n",
      "9       403\n",
      "10      149\n",
      "11       97\n",
      "12       36\n",
      "13        9\n",
      "14       10\n",
      "15        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# determine the longest sequence\n",
    "print(f\"max length: {df['Target_Token_IDs'].apply(len).max()}\")\n",
    "print(df['Target_Token_IDs'].apply(len).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 5</td>\n",
       "      <td>[A, 5]</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "      <td>[[0.5321832895278931, 0.15855109691619873, 0.2...</td>\n",
       "      <td>[-0.24310845136642456, 0.13687995076179504, -0...</td>\n",
       "      <td>[32, 435, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 66</td>\n",
       "      <td>[A, 66]</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "      <td>[[0.4871409237384796, 0.3022191524505615, 0.23...</td>\n",
       "      <td>[0.32661712169647217, 0.14974229037761688, 0.1...</td>\n",
       "      <td>[32, 10092, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 661</td>\n",
       "      <td>[A, 66, ##1]</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "      <td>[[0.33346572518348694, 0.23834776878356934, -0...</td>\n",
       "      <td>[0.12139879912137985, 0.2494209259748459, 0.05...</td>\n",
       "      <td>[32, 10092, 26927, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aalen</td>\n",
       "      <td>[A, ##alen]</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "      <td>[[0.07099006325006485, -0.5678775310516357, 1....</td>\n",
       "      <td>[-0.003420088440179825, -0.260809063911438, 0....</td>\n",
       "      <td>[32, 609, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aare-Mündung</td>\n",
       "      <td>[Aar, ##e, -, Mündung]</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "      <td>[[0.3378963768482208, -0.5541061162948608, -0....</td>\n",
       "      <td>[0.2187633365392685, -0.3105980157852173, 0.07...</td>\n",
       "      <td>[15576, 26897, 26935, 14955, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46546</th>\n",
       "      <td>Zyg</td>\n",
       "      <td>[Zy, ##g]</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "      <td>[[-0.36112070083618164, -0.00450560450553894, ...</td>\n",
       "      <td>[0.005554556846618652, 0.19277839362621307, -0...</td>\n",
       "      <td>[10373, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46547</th>\n",
       "      <td>Zygmunt</td>\n",
       "      <td>[Zy, ##g, ##m, ##unt]</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "      <td>[[-0.20858515799045563, -0.4874523878097534, -...</td>\n",
       "      <td>[0.246858611702919, -0.13464316725730896, -0.2...</td>\n",
       "      <td>[10373, 26908, 26911, 1937, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46548</th>\n",
       "      <td>Zyj</td>\n",
       "      <td>[Zy, ##j]</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "      <td>[[-0.07781310379505157, 0.8064130544662476, -0...</td>\n",
       "      <td>[-0.12115689367055893, 0.37860164046287537, -0...</td>\n",
       "      <td>[10373, 26963, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46549</th>\n",
       "      <td>Zyk</td>\n",
       "      <td>[Zy, ##k]</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "      <td>[[-0.4067027270793915, 0.1616303026676178, 0.6...</td>\n",
       "      <td>[-0.3863324820995331, -0.24419178068637848, 0....</td>\n",
       "      <td>[10373, 26917, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46550</th>\n",
       "      <td>Zyr</td>\n",
       "      <td>[Zy, ##r]</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "      <td>[[1.0486658811569214, -0.38755977153778076, 0....</td>\n",
       "      <td>[0.7094437479972839, -0.18272826075553894, 0.1...</td>\n",
       "      <td>[10373, 26900, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Entity                  Tokens                        Token_IDs  \\\n",
       "0               A 5                  [A, 5]                     [32, 435, 4]   \n",
       "1              A 66                 [A, 66]                   [32, 10092, 4]   \n",
       "2             A 661            [A, 66, ##1]            [32, 10092, 26927, 4]   \n",
       "3             Aalen             [A, ##alen]                     [32, 609, 4]   \n",
       "4      Aare-Mündung  [Aar, ##e, -, Mündung]  [15576, 26897, 26935, 14955, 4]   \n",
       "...             ...                     ...                              ...   \n",
       "46546           Zyg               [Zy, ##g]                [10373, 26908, 4]   \n",
       "46547       Zygmunt   [Zy, ##g, ##m, ##unt]   [10373, 26908, 26911, 1937, 4]   \n",
       "46548           Zyj               [Zy, ##j]                [10373, 26963, 4]   \n",
       "46549           Zyk               [Zy, ##k]                [10373, 26917, 4]   \n",
       "46550           Zyr               [Zy, ##r]                [10373, 26900, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "0      [[0.5321832895278931, 0.15855109691619873, 0.2...   \n",
       "1      [[0.4871409237384796, 0.3022191524505615, 0.23...   \n",
       "2      [[0.33346572518348694, 0.23834776878356934, -0...   \n",
       "3      [[0.07099006325006485, -0.5678775310516357, 1....   \n",
       "4      [[0.3378963768482208, -0.5541061162948608, -0....   \n",
       "...                                                  ...   \n",
       "46546  [[-0.36112070083618164, -0.00450560450553894, ...   \n",
       "46547  [[-0.20858515799045563, -0.4874523878097534, -...   \n",
       "46548  [[-0.07781310379505157, 0.8064130544662476, -0...   \n",
       "46549  [[-0.4067027270793915, 0.1616303026676178, 0.6...   \n",
       "46550  [[1.0486658811569214, -0.38755977153778076, 0....   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "0      [-0.24310845136642456, 0.13687995076179504, -0...   \n",
       "1      [0.32661712169647217, 0.14974229037761688, 0.1...   \n",
       "2      [0.12139879912137985, 0.2494209259748459, 0.05...   \n",
       "3      [-0.003420088440179825, -0.260809063911438, 0....   \n",
       "4      [0.2187633365392685, -0.3105980157852173, 0.07...   \n",
       "...                                                  ...   \n",
       "46546  [0.005554556846618652, 0.19277839362621307, -0...   \n",
       "46547  [0.246858611702919, -0.13464316725730896, -0.2...   \n",
       "46548  [-0.12115689367055893, 0.37860164046287537, -0...   \n",
       "46549  [-0.3863324820995331, -0.24419178068637848, 0....   \n",
       "46550  [0.7094437479972839, -0.18272826075553894, 0.1...   \n",
       "\n",
       "                      Target_Token_IDs  \n",
       "0                         [32, 435, 4]  \n",
       "1                       [32, 10092, 4]  \n",
       "2                [32, 10092, 26927, 4]  \n",
       "3                         [32, 609, 4]  \n",
       "4      [15576, 26897, 26935, 14955, 4]  \n",
       "...                                ...  \n",
       "46546                [10373, 26908, 4]  \n",
       "46547   [10373, 26908, 26911, 1937, 4]  \n",
       "46548                [10373, 26963, 4]  \n",
       "46549                [10373, 26917, 4]  \n",
       "46550                [10373, 26900, 4]  \n",
       "\n",
       "[46551 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_df = Path.cwd() / \"data\" / \"df_file.pkl\"\n",
    "\n",
    "# #Save the DataFrame to a pickle file\n",
    "# df.to_pickle(file_path_df)\n",
    "\n",
    "# #Load the DataFrame from the pickle file\n",
    "df= pd.read_pickle(file_path_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the average embedding (input)\n",
    "        avg_embedding = torch.tensor(self.data.iloc[idx]['Average_Embedding'], dtype=torch.float32)\n",
    "        \n",
    "        # Get the token IDs (target)\n",
    "        token_ids = torch.tensor(self.data.iloc[idx]['Target_Token_IDs'], dtype=torch.int64)\n",
    "        \n",
    "        return avg_embedding, token_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad the target sequences\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack the inputs \n",
    "    inputs_stacked = torch.stack(inputs)\n",
    "    \n",
    "    return inputs_stacked, targets_padded\n",
    "\n",
    "# Create the dataset\n",
    "full_dataset = CustomDataset(df)\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mseiferling/.venvs/embed/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Train loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 13])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.5005948543548584 16.544424057006836\n",
      "Target range: 0 26979\n",
      "\n",
      "Information Validation loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 11])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.318089008331299 16.61168670654297\n",
      "Target range: 0 26979\n",
      "\n",
      "Information Test loader:\n",
      "Input shape: torch.Size([256, 768])\n",
      "Target shape: torch.Size([256, 9])\n",
      "Input dtype: torch.float32\n",
      "Target dtype: torch.int64\n",
      "Input range: -2.8263535499572754 16.70368766784668\n",
      "Target range: 0 26969\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "file_path_train = Path.cwd() / \"data\" / \"train_dataset.pkl\"\n",
    "file_path_test = Path.cwd() / \"data\" / \"test_dataset.pkl\"\n",
    "file_path_val = Path.cwd() / \"data\" / \"val_dataset.pkl\"\n",
    "\n",
    "######################Save the Data to a pickle file\n",
    "# with open(file_path_train, 'wb') as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# with open(file_path_test, 'wb') as f:\n",
    "#     pickle.dump(val_dataset, f)\n",
    "\n",
    "# with open(file_path_val, 'wb') as f:\n",
    "#     pickle.dump(test_dataset, f)\n",
    "\n",
    "######################Load the Data from the pickle file\n",
    "with open(file_path_train, 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_test, 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "with open(file_path_val, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Create DataLoaders with collate function\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) ########### hyperparameter optimization batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#Infromation dataloaders\n",
    "for loader_name, loader in [(\"Train\", train_loader), (\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
    "    print(f\"\\nInformation {loader_name} loader:\")\n",
    "    for batch in loader:\n",
    "        print(\"Input shape:\", batch[0].shape)\n",
    "        print(\"Target shape:\", batch[1].shape)\n",
    "        print(\"Input dtype:\", batch[0].dtype)\n",
    "        print(\"Target dtype:\", batch[1].dtype)\n",
    "        print(\"Input range:\", batch[0].min().item(), batch[0].max().item())\n",
    "        print(\"Target range:\", batch[1].min().item(), batch[1].max().item())\n",
    "        break\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForMaskedLM\n",
    "#### Consider adding dropout layers to prevent overfitting.\n",
    "class CustomLSTMWithBERTMLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, max_seq_length, bert_model_name):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        ####self.dropout = nn.Dropout(0.1)\n",
    "        ####self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        #### Remove these lines to allow fine-tuning\n",
    "        for param in self.bert_mlm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        assert hidden_size == self.bert_mlm.config.hidden_size, \"LSTM hidden size must match BERT hidden size\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Use the input to initialize the hidden state\n",
    "        h = x.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1) #shape:(num_layers, batch_size, hidden_size)\n",
    "        c = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        # dummy input for the LSTM\n",
    "        dummy_input = torch.zeros(batch_size, 1, self.lstm.hidden_size).to(x.device)\n",
    "        outputs = []\n",
    "        for i in range(self.max_seq_length):\n",
    "            out, (h, c) = self.lstm(dummy_input, (h, c))\n",
    "            ####out = self.dropout(out)\n",
    "            ####out = self.layer_norm(out)\n",
    "            mlm_output = self.bert_mlm.cls(out)\n",
    "            \n",
    "            outputs.append(mlm_output)\n",
    "        \n",
    "        output_tensor = torch.cat(outputs, dim=1)\n",
    "        return output_tensor # output shape: (batch_size, generated_seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 768\n",
    "num_layers = 1 #### hyperparameter optimization\n",
    "max_seq_length = 15\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CrossEntropyLoss, expect the inputs in a specific format:\n",
    "# The predictions should be a 2D tensor of shape [N, C] where N is the number of samples and C is the number of classes (vocab size in this case).\n",
    "# The targets should be a 1D tensor of shape [N] containing the class indices.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "bert_model_name = 'bert-base-german-cased'\n",
    "model = CustomLSTMWithBERTMLM(input_size, hidden_size, num_layers, max_seq_length, bert_model_name)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #### no learning rate decay -> could test AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch [37/50], Train Loss: 0.0394, Test Loss: 0.6633\n",
      "Epoch [38/50], Train Loss: 0.0285, Test Loss: 0.6653\n",
      "Epoch [39/50], Train Loss: 0.0281, Test Loss: 0.6730\n",
      "Epoch [40/50], Train Loss: 0.0318, Test Loss: 0.6546\n",
      "Epoch [41/50], Train Loss: 0.0292, Test Loss: 0.6747\n",
      "Epoch [42/50], Train Loss: 0.0297, Test Loss: 0.6688\n",
      "Epoch [43/50], Train Loss: 0.0243, Test Loss: 0.6563\n",
      "Epoch [44/50], Train Loss: 0.0202, Test Loss: 0.6979\n",
      "Epoch [45/50], Train Loss: 0.0234, Test Loss: 0.6832\n",
      "Epoch [46/50], Train Loss: 0.0263, Test Loss: 0.6899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 5.2904, Test Loss: 2.4138\n",
      "Epoch [2/50], Train Loss: 1.6957, Test Loss: 1.3864\n",
      "Epoch [3/50], Train Loss: 1.1064, Test Loss: 1.1308\n",
      "Epoch [4/50], Train Loss: 0.8423, Test Loss: 1.0061\n",
      "Epoch [5/50], Train Loss: 0.6703, Test Loss: 0.9142\n",
      "Epoch [6/50], Train Loss: 0.5547, Test Loss: 0.8645\n",
      "Epoch [7/50], Train Loss: 0.4604, Test Loss: 0.8203\n",
      "Epoch [8/50], Train Loss: 0.3849, Test Loss: 0.8067\n",
      "Epoch [9/50], Train Loss: 0.3334, Test Loss: 0.7719\n",
      "Epoch [10/50], Train Loss: 0.2902, Test Loss: 0.7494\n",
      "Epoch [11/50], Train Loss: 0.2428, Test Loss: 0.7460\n",
      "Epoch [12/50], Train Loss: 0.2114, Test Loss: 0.7496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# this resets the gradients of all parameters to zero from last batch\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# computes the gradient of the loss with respect to each parameter of the model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# updates the parameters based on the computed gradients\u001b[39;00m\n\u001b[1;32m     59\u001b[0m epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.venvs/embed/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/embed/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/embed/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create log file with current date and time\n",
    "current_time_start_epoch = datetime.datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
    "log_path = Path.cwd() / \"log\" / f\"log_{current_time_start_epoch}.txt\"\n",
    "# log hyperparameters and model information\n",
    "with open(log_path, 'w') as log_file:\n",
    "    log_file.write(f\"Model Architecture: {model}\\n\\n\")\n",
    "    log_file.write(f\"BERT model: {bert_model_name}\\n\")\n",
    "    log_file.write(f\"Batch size: {batch_size}\\n\")\n",
    "    log_file.write(f\"Input size: {input_size}\\n\")\n",
    "    log_file.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    log_file.write(f\"Number of layers: {num_layers}\\n\")\n",
    "    log_file.write(f\"Max sequence length: {max_seq_length}\\n\")\n",
    "    log_file.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    log_file.write(f\"Loss function: CrossEntropyLoss\\n\")\n",
    "    log_file.write(f\"Number of epochs: {num_epochs}\\n\\n\")\n",
    "    \n",
    "best_model_path = Path.cwd() / \"data\" / \"best_model.pth\"\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        # inference with batch\n",
    "        outputs = model(inputs)\n",
    "        # processing of output and target for training\n",
    "        outputs = outputs[:, :targets.shape[1], :] # we only need to consider the outputs for which we have a target during training\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1)) # Reshape outputs from [batch_size, sequence_length, vocab_size] to be [batch_size * sequence_length, vocab_size]  \n",
    "        targets = targets.reshape(-1) # Reshape targets from [batch_size, sequence_length] to be [batch_size * sequence_length] \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad() # this resets the gradients of all parameters to zero from last batch\n",
    "        loss.backward() # computes the gradient of the loss with respect to each parameter of the model\n",
    "        optimizer.step() # updates the parameters based on the computed gradients\n",
    "    \n",
    "        epoch_train_losses.append(loss.item())\n",
    "   \n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Compute test loss\n",
    "    model.eval()\n",
    "    epoch_test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs[:, :targets.shape[1], :]\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            epoch_test_losses.append(loss.item())\n",
    "    \n",
    "    avg_test_loss = sum(epoch_test_losses) / len(epoch_test_losses)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Save the model if the test loss is the best seen so far\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_test_loss,\n",
    "        }, best_model_path)\n",
    "        \n",
    "    # Log epoch information\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] - {current_time}\\n\")\n",
    "        log_file.write(f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\\n\\n\")\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, marker='o', label='Test Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# get the best loss for the filename\n",
    "best_loss_str = f\"{best_test_loss:.4f}\".replace('.', '_')\n",
    "\n",
    "# Construct the file path\n",
    "plot_file_path = Path.cwd() / \"log\" / f\"loss_plot_{current_time_start_epoch}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_file_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#### Implement more comprehensive evaluation metrics (e.g., perplexity, BLEU score for generated text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()  # for inference\n",
    "# to resume training, set the model train mode\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_IDs</th>\n",
       "      <th>Token_Embeddings</th>\n",
       "      <th>Average_Embedding</th>\n",
       "      <th>Target_Token_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42000</th>\n",
       "      <td>Tall</td>\n",
       "      <td>[Tall]</td>\n",
       "      <td>[21935, 4]</td>\n",
       "      <td>[[-0.01224720198661089, 0.16867724061012268, -...</td>\n",
       "      <td>[-0.01224720198661089, 0.16867724061012268, -0...</td>\n",
       "      <td>[21935, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42001</th>\n",
       "      <td>Tam</td>\n",
       "      <td>[Tam]</td>\n",
       "      <td>[17524, 4]</td>\n",
       "      <td>[[0.16409598290920258, 0.793487548828125, -0.7...</td>\n",
       "      <td>[0.16409598290920258, 0.793487548828125, -0.71...</td>\n",
       "      <td>[17524, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42002</th>\n",
       "      <td>Tamar</td>\n",
       "      <td>[Tam, ##ar]</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "      <td>[[1.1463826894760132, 0.7378958463668823, 0.41...</td>\n",
       "      <td>[0.4127388000488281, 0.5559422373771667, -0.04...</td>\n",
       "      <td>[17524, 33, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42003</th>\n",
       "      <td>Tamara</td>\n",
       "      <td>[Tam, ##ara]</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "      <td>[[0.774370014667511, 0.2208947241306305, 0.663...</td>\n",
       "      <td>[0.5452055931091309, 0.20736496150493622, 0.25...</td>\n",
       "      <td>[17524, 3738, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42004</th>\n",
       "      <td>Tamara Boros</td>\n",
       "      <td>[Tam, ##ara, Bor, ##os]</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "      <td>[[1.0219417810440063, 0.18805761635303497, 0.3...</td>\n",
       "      <td>[0.8827532529830933, 0.0814586728811264, 0.267...</td>\n",
       "      <td>[17524, 3738, 3888, 224, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42005</th>\n",
       "      <td>Tamarine Tanasugarn</td>\n",
       "      <td>[Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.8510592579841614, -0.1594560146331787, 0.3...</td>\n",
       "      <td>[0.4551093578338623, -0.1477975845336914, 0.10...</td>\n",
       "      <td>[17524, 22328, 26897, 15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42006</th>\n",
       "      <td>Tamas</td>\n",
       "      <td>[Tam, ##as]</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "      <td>[[1.130900263786316, 0.7732470631599426, 0.357...</td>\n",
       "      <td>[0.6211729049682617, 0.5434377789497375, 0.150...</td>\n",
       "      <td>[17524, 45, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42007</th>\n",
       "      <td>Tamas Ajan</td>\n",
       "      <td>[Tam, ##as, A, ##jan]</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "      <td>[[1.140594720840454, 1.101357102394104, 0.5228...</td>\n",
       "      <td>[0.2719385027885437, 0.47032666206359863, 0.33...</td>\n",
       "      <td>[17524, 45, 32, 10761, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42008</th>\n",
       "      <td>Tamas Szekeres</td>\n",
       "      <td>[Tam, ##as, Sz, ##ek, ##ere, ##s]</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "      <td>[[1.1150412559509277, 1.2008758783340454, 0.47...</td>\n",
       "      <td>[0.7783443927764893, 0.3929256498813629, 0.270...</td>\n",
       "      <td>[17524, 45, 14620, 1752, 1031, 26902, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42009</th>\n",
       "      <td>Tambuyser</td>\n",
       "      <td>[Tam, ##bu, ##yse, ##r]</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "      <td>[[0.9211291670799255, 0.2917001247406006, -0.3...</td>\n",
       "      <td>[0.6440316438674927, 0.13280363380908966, -0.0...</td>\n",
       "      <td>[17524, 13874, 18940, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42010</th>\n",
       "      <td>Tamer</td>\n",
       "      <td>[Tam, ##er]</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "      <td>[[1.3231805562973022, 0.5179563164710999, 0.93...</td>\n",
       "      <td>[0.7314438819885254, 0.7326130867004395, 0.298...</td>\n",
       "      <td>[17524, 6, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42011</th>\n",
       "      <td>Taminiaux</td>\n",
       "      <td>[Tam, ##ini, ##aux]</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "      <td>[[0.7550164461135864, 0.8477469682693481, -0.4...</td>\n",
       "      <td>[0.31464725732803345, 0.3305900990962982, -0.5...</td>\n",
       "      <td>[17524, 5381, 8805, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42012</th>\n",
       "      <td>Tamira</td>\n",
       "      <td>[Tam, ##ira]</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "      <td>[[0.7360913157463074, 0.31316858530044556, 0.5...</td>\n",
       "      <td>[0.37108471989631653, -0.010409042239189148, 0...</td>\n",
       "      <td>[17524, 7872, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42013</th>\n",
       "      <td>Tamme</td>\n",
       "      <td>[Tam, ##me]</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "      <td>[[0.8440600037574768, 0.8673146963119507, 0.84...</td>\n",
       "      <td>[0.4066227674484253, 0.6386289000511169, 0.443...</td>\n",
       "      <td>[17524, 373, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42014</th>\n",
       "      <td>Tamminga</td>\n",
       "      <td>[Tam, ##min, ##ga]</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "      <td>[[1.234696626663208, -0.8913580179214478, 0.67...</td>\n",
       "      <td>[0.4619470536708832, -0.8386526703834534, 0.32...</td>\n",
       "      <td>[17524, 734, 529, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42015</th>\n",
       "      <td>Tammo</td>\n",
       "      <td>[Tam, ##mo]</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "      <td>[[0.5058582425117493, 0.5409940481185913, -0.2...</td>\n",
       "      <td>[0.3185293972492218, 0.021780773997306824, -0....</td>\n",
       "      <td>[17524, 4359, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42016</th>\n",
       "      <td>Tammy</td>\n",
       "      <td>[Tam, ##my]</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "      <td>[[0.910221517086029, 1.2008426189422607, 0.694...</td>\n",
       "      <td>[0.47359079122543335, 0.8301605582237244, 0.23...</td>\n",
       "      <td>[17524, 16601, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42017</th>\n",
       "      <td>Tan</td>\n",
       "      <td>[Tan]</td>\n",
       "      <td>[15297, 4]</td>\n",
       "      <td>[[-0.293621301651001, 0.779782772064209, -0.14...</td>\n",
       "      <td>[-0.293621301651001, 0.779782772064209, -0.144...</td>\n",
       "      <td>[15297, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42018</th>\n",
       "      <td>Tanaka</td>\n",
       "      <td>[Tan, ##aka]</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "      <td>[[-0.6399266123771667, 0.15942348539829254, 1....</td>\n",
       "      <td>[-0.10410656034946442, -0.09060264378786087, 0...</td>\n",
       "      <td>[15297, 10550, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42019</th>\n",
       "      <td>Tanasugarn</td>\n",
       "      <td>[Tan, ##as, ##ug, ##arn]</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "      <td>[[0.2947482764720917, 0.4754425287246704, 0.42...</td>\n",
       "      <td>[0.057072967290878296, -0.4458737373352051, 0....</td>\n",
       "      <td>[15297, 45, 389, 7074, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42020</th>\n",
       "      <td>Tandjung</td>\n",
       "      <td>[Tan, ##d, ##ju, ##n, ##g]</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "      <td>[[-0.4749807119369507, 0.528508722782135, 0.11...</td>\n",
       "      <td>[0.15352052450180054, -0.29984623193740845, -0...</td>\n",
       "      <td>[15297, 26904, 14405, 26898, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42021</th>\n",
       "      <td>Tandy</td>\n",
       "      <td>[Tan, ##dy]</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "      <td>[[-0.8069059252738953, 0.6859312057495117, 0.4...</td>\n",
       "      <td>[-0.3132078945636749, 0.3191651403903961, 0.08...</td>\n",
       "      <td>[15297, 6484, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42022</th>\n",
       "      <td>Tang</td>\n",
       "      <td>[Tan, ##g]</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "      <td>[[0.033787306398153305, -0.24927137792110443, ...</td>\n",
       "      <td>[0.15189029276371002, -0.43078309297561646, -0...</td>\n",
       "      <td>[15297, 26908, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42023</th>\n",
       "      <td>Tanghe</td>\n",
       "      <td>[Tan, ##gh, ##e]</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "      <td>[[-0.07595154643058777, 0.12992946803569794, 0...</td>\n",
       "      <td>[-0.23548705875873566, -0.060995280742645264, ...</td>\n",
       "      <td>[15297, 13901, 26897, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42024</th>\n",
       "      <td>Tangs</td>\n",
       "      <td>[Tan, ##gs]</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "      <td>[[0.13123495876789093, -0.16866567730903625, 0...</td>\n",
       "      <td>[0.12566909193992615, -0.07040099054574966, 0....</td>\n",
       "      <td>[15297, 753, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42025</th>\n",
       "      <td>Tania</td>\n",
       "      <td>[Tan, ##ia]</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "      <td>[[0.21963322162628174, 1.1262986660003662, 0.7...</td>\n",
       "      <td>[0.05399937182664871, 0.3011305630207062, 0.20...</td>\n",
       "      <td>[15297, 544, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42026</th>\n",
       "      <td>Tania Kloeck</td>\n",
       "      <td>[Tan, ##ia, Kl, ##oe, ##ck]</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "      <td>[[0.28413882851600647, 1.5511207580566406, 0.2...</td>\n",
       "      <td>[0.4187382161617279, 0.2899208664894104, -0.14...</td>\n",
       "      <td>[15297, 544, 815, 14635, 110, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42027</th>\n",
       "      <td>Tania Medo</td>\n",
       "      <td>[Tan, ##ia, Med, ##o]</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "      <td>[[0.5423315167427063, 1.4148615598678589, 0.79...</td>\n",
       "      <td>[0.3000876307487488, 0.23904681205749512, 0.40...</td>\n",
       "      <td>[15297, 544, 1371, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42028</th>\n",
       "      <td>Tania Polak</td>\n",
       "      <td>[Tan, ##ia, Pol, ##ak]</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "      <td>[[0.5157328248023987, 1.47681725025177, 0.6004...</td>\n",
       "      <td>[0.30176252126693726, 0.2940174341201782, -0.0...</td>\n",
       "      <td>[15297, 544, 984, 464, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42029</th>\n",
       "      <td>Tania Poppe</td>\n",
       "      <td>[Tan, ##ia, Pop, ##pe]</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "      <td>[[0.4346654713153839, 1.436200499534607, 0.319...</td>\n",
       "      <td>[0.2676783800125122, 0.15604820847511292, 0.17...</td>\n",
       "      <td>[15297, 544, 4528, 3500, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42030</th>\n",
       "      <td>Tania Prinsier</td>\n",
       "      <td>[Tan, ##ia, Prin, ##sie, ##r]</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "      <td>[[0.5102294683456421, 1.3143318891525269, 0.30...</td>\n",
       "      <td>[0.7683398127555847, 0.12537333369255066, 0.10...</td>\n",
       "      <td>[15297, 544, 3269, 2952, 26900, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42031</th>\n",
       "      <td>Taniguchi</td>\n",
       "      <td>[Tan, ##ig, ##uch, ##i]</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "      <td>[[-0.5881728529930115, 1.0786540508270264, 0.7...</td>\n",
       "      <td>[-0.03565274178981781, 0.24008022248744965, -0...</td>\n",
       "      <td>[15297, 80, 108, 26899, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42032</th>\n",
       "      <td>Tanja</td>\n",
       "      <td>[Tan, ##ja]</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "      <td>[[-0.20097672939300537, 0.6485582590103149, 0....</td>\n",
       "      <td>[-0.048050444573163986, 0.1633566915988922, 0....</td>\n",
       "      <td>[15297, 3171, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42033</th>\n",
       "      <td>Tanne</td>\n",
       "      <td>[Tan, ##ne]</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "      <td>[[-0.363036572933197, 0.45801427960395813, 0.0...</td>\n",
       "      <td>[0.29078078269958496, 0.022456184029579163, -0...</td>\n",
       "      <td>[15297, 175, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42034</th>\n",
       "      <td>Tanneke</td>\n",
       "      <td>[Tan, ##ne, ##ke]</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "      <td>[[-0.536466121673584, 0.41328921914100647, 0.6...</td>\n",
       "      <td>[0.046449288725852966, 0.1203099712729454, -0....</td>\n",
       "      <td>[15297, 175, 772, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42035</th>\n",
       "      <td>Tanner</td>\n",
       "      <td>[Tan, ##ner]</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "      <td>[[-0.38445645570755005, 0.7667683959007263, 1....</td>\n",
       "      <td>[0.06236431002616882, 0.5567158460617065, 0.40...</td>\n",
       "      <td>[15297, 344, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42036</th>\n",
       "      <td>Tanno</td>\n",
       "      <td>[Tan, ##no]</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "      <td>[[-0.26311466097831726, 0.5683020353317261, 0....</td>\n",
       "      <td>[0.1073189526796341, 0.16556155681610107, 0.02...</td>\n",
       "      <td>[15297, 9706, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42037</th>\n",
       "      <td>Tanny</td>\n",
       "      <td>[Tan, ##ny]</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "      <td>[[-0.7399507761001587, 0.5193020701408386, 0.4...</td>\n",
       "      <td>[-0.14292527735233307, 0.29106348752975464, 0....</td>\n",
       "      <td>[15297, 9381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42038</th>\n",
       "      <td>Tansens</td>\n",
       "      <td>[Tan, ##sens]</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "      <td>[[-1.0175524950027466, 0.42136335372924805, 1....</td>\n",
       "      <td>[-0.522026538848877, 0.5173361301422119, 0.640...</td>\n",
       "      <td>[15297, 8531, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42039</th>\n",
       "      <td>Tansikoezjina</td>\n",
       "      <td>[Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "      <td>[[-0.15889112651348114, 0.5353723764419556, 1....</td>\n",
       "      <td>[0.16954562067985535, 0.22987063229084015, 0.5...</td>\n",
       "      <td>[15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42040</th>\n",
       "      <td>Tansykkoezina</td>\n",
       "      <td>[Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "      <td>[[-0.5009126663208008, 0.6513158679008484, 1.4...</td>\n",
       "      <td>[0.06688310205936432, 0.5194838643074036, 0.59...</td>\n",
       "      <td>[15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42041</th>\n",
       "      <td>Tanya</td>\n",
       "      <td>[Tan, ##ya]</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "      <td>[[-0.27348271012306213, 0.7305521965026855, 0....</td>\n",
       "      <td>[-0.15534719824790955, 0.21959252655506134, 0....</td>\n",
       "      <td>[15297, 5630, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42042</th>\n",
       "      <td>Tao</td>\n",
       "      <td>[Ta, ##o]</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "      <td>[[0.6866366267204285, -0.32815611362457275, -0...</td>\n",
       "      <td>[0.5244305729866028, -0.42992928624153137, -0....</td>\n",
       "      <td>[2127, 26910, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42043</th>\n",
       "      <td>Tap</td>\n",
       "      <td>[Ta, ##p]</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "      <td>[[0.0655214861035347, -0.1980666071176529, 0.0...</td>\n",
       "      <td>[-0.09740108251571655, 0.21112042665481567, -0...</td>\n",
       "      <td>[2127, 26920, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42044</th>\n",
       "      <td>Tapias</td>\n",
       "      <td>[Ta, ##p, ##ias]</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "      <td>[[0.3809676170349121, 0.13550198078155518, -0....</td>\n",
       "      <td>[0.33656492829322815, 0.6334041953086853, -0.5...</td>\n",
       "      <td>[2127, 26920, 3019, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42045</th>\n",
       "      <td>Tappert</td>\n",
       "      <td>[Ta, ##pper, ##t]</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "      <td>[[-0.004451925400644541, 0.3471279740333557, 0...</td>\n",
       "      <td>[0.1981954127550125, 0.2921486794948578, -0.06...</td>\n",
       "      <td>[2127, 9804, 26901, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42046</th>\n",
       "      <td>Tar</td>\n",
       "      <td>[Tar]</td>\n",
       "      <td>[10397, 4]</td>\n",
       "      <td>[[0.12060574442148209, 0.9793983697891235, -0....</td>\n",
       "      <td>[0.12060574442148209, 0.9793983697891235, -0.9...</td>\n",
       "      <td>[10397, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42047</th>\n",
       "      <td>Tara</td>\n",
       "      <td>[Tar, ##a]</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "      <td>[[0.21004708111286163, 1.0180401802062988, -0....</td>\n",
       "      <td>[0.07090392708778381, 0.5829620957374573, -0.0...</td>\n",
       "      <td>[10397, 26903, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42048</th>\n",
       "      <td>Tarabini</td>\n",
       "      <td>[Tar, ##ab, ##ini]</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "      <td>[[-0.2942725419998169, 0.550433337688446, -0.1...</td>\n",
       "      <td>[-0.24213707447052002, 0.6941204071044922, 0.1...</td>\n",
       "      <td>[10397, 228, 5381, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42049</th>\n",
       "      <td>Tarallo</td>\n",
       "      <td>[Tar, ##all, ##o]</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "      <td>[[-0.028452834114432335, -0.6263936161994934, ...</td>\n",
       "      <td>[0.09504646807909012, -0.5627711415290833, 0.3...</td>\n",
       "      <td>[10397, 211, 26910, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Entity                                      Tokens  \\\n",
       "42000                 Tall                                      [Tall]   \n",
       "42001                  Tam                                       [Tam]   \n",
       "42002                Tamar                                 [Tam, ##ar]   \n",
       "42003               Tamara                                [Tam, ##ara]   \n",
       "42004         Tamara Boros                     [Tam, ##ara, Bor, ##os]   \n",
       "42005  Tamarine Tanasugarn  [Tam, ##arin, ##e, Tan, ##as, ##ug, ##arn]   \n",
       "42006                Tamas                                 [Tam, ##as]   \n",
       "42007           Tamas Ajan                       [Tam, ##as, A, ##jan]   \n",
       "42008       Tamas Szekeres           [Tam, ##as, Sz, ##ek, ##ere, ##s]   \n",
       "42009            Tambuyser                     [Tam, ##bu, ##yse, ##r]   \n",
       "42010                Tamer                                 [Tam, ##er]   \n",
       "42011            Taminiaux                         [Tam, ##ini, ##aux]   \n",
       "42012               Tamira                                [Tam, ##ira]   \n",
       "42013                Tamme                                 [Tam, ##me]   \n",
       "42014             Tamminga                          [Tam, ##min, ##ga]   \n",
       "42015                Tammo                                 [Tam, ##mo]   \n",
       "42016                Tammy                                 [Tam, ##my]   \n",
       "42017                  Tan                                       [Tan]   \n",
       "42018               Tanaka                                [Tan, ##aka]   \n",
       "42019           Tanasugarn                    [Tan, ##as, ##ug, ##arn]   \n",
       "42020             Tandjung                  [Tan, ##d, ##ju, ##n, ##g]   \n",
       "42021                Tandy                                 [Tan, ##dy]   \n",
       "42022                 Tang                                  [Tan, ##g]   \n",
       "42023               Tanghe                            [Tan, ##gh, ##e]   \n",
       "42024                Tangs                                 [Tan, ##gs]   \n",
       "42025                Tania                                 [Tan, ##ia]   \n",
       "42026         Tania Kloeck                 [Tan, ##ia, Kl, ##oe, ##ck]   \n",
       "42027           Tania Medo                       [Tan, ##ia, Med, ##o]   \n",
       "42028          Tania Polak                      [Tan, ##ia, Pol, ##ak]   \n",
       "42029          Tania Poppe                      [Tan, ##ia, Pop, ##pe]   \n",
       "42030       Tania Prinsier               [Tan, ##ia, Prin, ##sie, ##r]   \n",
       "42031            Taniguchi                     [Tan, ##ig, ##uch, ##i]   \n",
       "42032                Tanja                                 [Tan, ##ja]   \n",
       "42033                Tanne                                 [Tan, ##ne]   \n",
       "42034              Tanneke                           [Tan, ##ne, ##ke]   \n",
       "42035               Tanner                                [Tan, ##ner]   \n",
       "42036                Tanno                                 [Tan, ##no]   \n",
       "42037                Tanny                                 [Tan, ##ny]   \n",
       "42038              Tansens                               [Tan, ##sens]   \n",
       "42039        Tansikoezjina     [Tan, ##s, ##iko, ##e, ##z, ##ji, ##na]   \n",
       "42040        Tansykkoezina     [Tan, ##sy, ##k, ##ko, ##e, ##zin, ##a]   \n",
       "42041                Tanya                                 [Tan, ##ya]   \n",
       "42042                  Tao                                   [Ta, ##o]   \n",
       "42043                  Tap                                   [Ta, ##p]   \n",
       "42044               Tapias                            [Ta, ##p, ##ias]   \n",
       "42045              Tappert                           [Ta, ##pper, ##t]   \n",
       "42046                  Tar                                       [Tar]   \n",
       "42047                 Tara                                  [Tar, ##a]   \n",
       "42048             Tarabini                          [Tar, ##ab, ##ini]   \n",
       "42049              Tarallo                           [Tar, ##all, ##o]   \n",
       "\n",
       "                                               Token_IDs  \\\n",
       "42000                                         [21935, 4]   \n",
       "42001                                         [17524, 4]   \n",
       "42002                                     [17524, 33, 4]   \n",
       "42003                                   [17524, 3738, 4]   \n",
       "42004                        [17524, 3738, 3888, 224, 4]   \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]   \n",
       "42006                                     [17524, 45, 4]   \n",
       "42007                          [17524, 45, 32, 10761, 4]   \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]   \n",
       "42009                    [17524, 13874, 18940, 26900, 4]   \n",
       "42010                                      [17524, 6, 4]   \n",
       "42011                             [17524, 5381, 8805, 4]   \n",
       "42012                                   [17524, 7872, 4]   \n",
       "42013                                    [17524, 373, 4]   \n",
       "42014                               [17524, 734, 529, 4]   \n",
       "42015                                   [17524, 4359, 4]   \n",
       "42016                                  [17524, 16601, 4]   \n",
       "42017                                         [15297, 4]   \n",
       "42018                                  [15297, 10550, 4]   \n",
       "42019                          [15297, 45, 389, 7074, 4]   \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]   \n",
       "42021                                   [15297, 6484, 4]   \n",
       "42022                                  [15297, 26908, 4]   \n",
       "42023                           [15297, 13901, 26897, 4]   \n",
       "42024                                    [15297, 753, 4]   \n",
       "42025                                    [15297, 544, 4]   \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]   \n",
       "42027                       [15297, 544, 1371, 26910, 4]   \n",
       "42028                          [15297, 544, 984, 464, 4]   \n",
       "42029                        [15297, 544, 4528, 3500, 4]   \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]   \n",
       "42031                         [15297, 80, 108, 26899, 4]   \n",
       "42032                                   [15297, 3171, 4]   \n",
       "42033                                    [15297, 175, 4]   \n",
       "42034                               [15297, 175, 772, 4]   \n",
       "42035                                    [15297, 344, 4]   \n",
       "42036                                   [15297, 9706, 4]   \n",
       "42037                                   [15297, 9381, 4]   \n",
       "42038                                   [15297, 8531, 4]   \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]   \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]   \n",
       "42041                                   [15297, 5630, 4]   \n",
       "42042                                   [2127, 26910, 4]   \n",
       "42043                                   [2127, 26920, 4]   \n",
       "42044                             [2127, 26920, 3019, 4]   \n",
       "42045                             [2127, 9804, 26901, 4]   \n",
       "42046                                         [10397, 4]   \n",
       "42047                                  [10397, 26903, 4]   \n",
       "42048                              [10397, 228, 5381, 4]   \n",
       "42049                             [10397, 211, 26910, 4]   \n",
       "\n",
       "                                        Token_Embeddings  \\\n",
       "42000  [[-0.01224720198661089, 0.16867724061012268, -...   \n",
       "42001  [[0.16409598290920258, 0.793487548828125, -0.7...   \n",
       "42002  [[1.1463826894760132, 0.7378958463668823, 0.41...   \n",
       "42003  [[0.774370014667511, 0.2208947241306305, 0.663...   \n",
       "42004  [[1.0219417810440063, 0.18805761635303497, 0.3...   \n",
       "42005  [[0.8510592579841614, -0.1594560146331787, 0.3...   \n",
       "42006  [[1.130900263786316, 0.7732470631599426, 0.357...   \n",
       "42007  [[1.140594720840454, 1.101357102394104, 0.5228...   \n",
       "42008  [[1.1150412559509277, 1.2008758783340454, 0.47...   \n",
       "42009  [[0.9211291670799255, 0.2917001247406006, -0.3...   \n",
       "42010  [[1.3231805562973022, 0.5179563164710999, 0.93...   \n",
       "42011  [[0.7550164461135864, 0.8477469682693481, -0.4...   \n",
       "42012  [[0.7360913157463074, 0.31316858530044556, 0.5...   \n",
       "42013  [[0.8440600037574768, 0.8673146963119507, 0.84...   \n",
       "42014  [[1.234696626663208, -0.8913580179214478, 0.67...   \n",
       "42015  [[0.5058582425117493, 0.5409940481185913, -0.2...   \n",
       "42016  [[0.910221517086029, 1.2008426189422607, 0.694...   \n",
       "42017  [[-0.293621301651001, 0.779782772064209, -0.14...   \n",
       "42018  [[-0.6399266123771667, 0.15942348539829254, 1....   \n",
       "42019  [[0.2947482764720917, 0.4754425287246704, 0.42...   \n",
       "42020  [[-0.4749807119369507, 0.528508722782135, 0.11...   \n",
       "42021  [[-0.8069059252738953, 0.6859312057495117, 0.4...   \n",
       "42022  [[0.033787306398153305, -0.24927137792110443, ...   \n",
       "42023  [[-0.07595154643058777, 0.12992946803569794, 0...   \n",
       "42024  [[0.13123495876789093, -0.16866567730903625, 0...   \n",
       "42025  [[0.21963322162628174, 1.1262986660003662, 0.7...   \n",
       "42026  [[0.28413882851600647, 1.5511207580566406, 0.2...   \n",
       "42027  [[0.5423315167427063, 1.4148615598678589, 0.79...   \n",
       "42028  [[0.5157328248023987, 1.47681725025177, 0.6004...   \n",
       "42029  [[0.4346654713153839, 1.436200499534607, 0.319...   \n",
       "42030  [[0.5102294683456421, 1.3143318891525269, 0.30...   \n",
       "42031  [[-0.5881728529930115, 1.0786540508270264, 0.7...   \n",
       "42032  [[-0.20097672939300537, 0.6485582590103149, 0....   \n",
       "42033  [[-0.363036572933197, 0.45801427960395813, 0.0...   \n",
       "42034  [[-0.536466121673584, 0.41328921914100647, 0.6...   \n",
       "42035  [[-0.38445645570755005, 0.7667683959007263, 1....   \n",
       "42036  [[-0.26311466097831726, 0.5683020353317261, 0....   \n",
       "42037  [[-0.7399507761001587, 0.5193020701408386, 0.4...   \n",
       "42038  [[-1.0175524950027466, 0.42136335372924805, 1....   \n",
       "42039  [[-0.15889112651348114, 0.5353723764419556, 1....   \n",
       "42040  [[-0.5009126663208008, 0.6513158679008484, 1.4...   \n",
       "42041  [[-0.27348271012306213, 0.7305521965026855, 0....   \n",
       "42042  [[0.6866366267204285, -0.32815611362457275, -0...   \n",
       "42043  [[0.0655214861035347, -0.1980666071176529, 0.0...   \n",
       "42044  [[0.3809676170349121, 0.13550198078155518, -0....   \n",
       "42045  [[-0.004451925400644541, 0.3471279740333557, 0...   \n",
       "42046  [[0.12060574442148209, 0.9793983697891235, -0....   \n",
       "42047  [[0.21004708111286163, 1.0180401802062988, -0....   \n",
       "42048  [[-0.2942725419998169, 0.550433337688446, -0.1...   \n",
       "42049  [[-0.028452834114432335, -0.6263936161994934, ...   \n",
       "\n",
       "                                       Average_Embedding  \\\n",
       "42000  [-0.01224720198661089, 0.16867724061012268, -0...   \n",
       "42001  [0.16409598290920258, 0.793487548828125, -0.71...   \n",
       "42002  [0.4127388000488281, 0.5559422373771667, -0.04...   \n",
       "42003  [0.5452055931091309, 0.20736496150493622, 0.25...   \n",
       "42004  [0.8827532529830933, 0.0814586728811264, 0.267...   \n",
       "42005  [0.4551093578338623, -0.1477975845336914, 0.10...   \n",
       "42006  [0.6211729049682617, 0.5434377789497375, 0.150...   \n",
       "42007  [0.2719385027885437, 0.47032666206359863, 0.33...   \n",
       "42008  [0.7783443927764893, 0.3929256498813629, 0.270...   \n",
       "42009  [0.6440316438674927, 0.13280363380908966, -0.0...   \n",
       "42010  [0.7314438819885254, 0.7326130867004395, 0.298...   \n",
       "42011  [0.31464725732803345, 0.3305900990962982, -0.5...   \n",
       "42012  [0.37108471989631653, -0.010409042239189148, 0...   \n",
       "42013  [0.4066227674484253, 0.6386289000511169, 0.443...   \n",
       "42014  [0.4619470536708832, -0.8386526703834534, 0.32...   \n",
       "42015  [0.3185293972492218, 0.021780773997306824, -0....   \n",
       "42016  [0.47359079122543335, 0.8301605582237244, 0.23...   \n",
       "42017  [-0.293621301651001, 0.779782772064209, -0.144...   \n",
       "42018  [-0.10410656034946442, -0.09060264378786087, 0...   \n",
       "42019  [0.057072967290878296, -0.4458737373352051, 0....   \n",
       "42020  [0.15352052450180054, -0.29984623193740845, -0...   \n",
       "42021  [-0.3132078945636749, 0.3191651403903961, 0.08...   \n",
       "42022  [0.15189029276371002, -0.43078309297561646, -0...   \n",
       "42023  [-0.23548705875873566, -0.060995280742645264, ...   \n",
       "42024  [0.12566909193992615, -0.07040099054574966, 0....   \n",
       "42025  [0.05399937182664871, 0.3011305630207062, 0.20...   \n",
       "42026  [0.4187382161617279, 0.2899208664894104, -0.14...   \n",
       "42027  [0.3000876307487488, 0.23904681205749512, 0.40...   \n",
       "42028  [0.30176252126693726, 0.2940174341201782, -0.0...   \n",
       "42029  [0.2676783800125122, 0.15604820847511292, 0.17...   \n",
       "42030  [0.7683398127555847, 0.12537333369255066, 0.10...   \n",
       "42031  [-0.03565274178981781, 0.24008022248744965, -0...   \n",
       "42032  [-0.048050444573163986, 0.1633566915988922, 0....   \n",
       "42033  [0.29078078269958496, 0.022456184029579163, -0...   \n",
       "42034  [0.046449288725852966, 0.1203099712729454, -0....   \n",
       "42035  [0.06236431002616882, 0.5567158460617065, 0.40...   \n",
       "42036  [0.1073189526796341, 0.16556155681610107, 0.02...   \n",
       "42037  [-0.14292527735233307, 0.29106348752975464, 0....   \n",
       "42038  [-0.522026538848877, 0.5173361301422119, 0.640...   \n",
       "42039  [0.16954562067985535, 0.22987063229084015, 0.5...   \n",
       "42040  [0.06688310205936432, 0.5194838643074036, 0.59...   \n",
       "42041  [-0.15534719824790955, 0.21959252655506134, 0....   \n",
       "42042  [0.5244305729866028, -0.42992928624153137, -0....   \n",
       "42043  [-0.09740108251571655, 0.21112042665481567, -0...   \n",
       "42044  [0.33656492829322815, 0.6334041953086853, -0.5...   \n",
       "42045  [0.1981954127550125, 0.2921486794948578, -0.06...   \n",
       "42046  [0.12060574442148209, 0.9793983697891235, -0.9...   \n",
       "42047  [0.07090392708778381, 0.5829620957374573, -0.0...   \n",
       "42048  [-0.24213707447052002, 0.6941204071044922, 0.1...   \n",
       "42049  [0.09504646807909012, -0.5627711415290833, 0.3...   \n",
       "\n",
       "                                        Target_Token_IDs  \n",
       "42000                                         [21935, 4]  \n",
       "42001                                         [17524, 4]  \n",
       "42002                                     [17524, 33, 4]  \n",
       "42003                                   [17524, 3738, 4]  \n",
       "42004                        [17524, 3738, 3888, 224, 4]  \n",
       "42005     [17524, 22328, 26897, 15297, 45, 389, 7074, 4]  \n",
       "42006                                     [17524, 45, 4]  \n",
       "42007                          [17524, 45, 32, 10761, 4]  \n",
       "42008           [17524, 45, 14620, 1752, 1031, 26902, 4]  \n",
       "42009                    [17524, 13874, 18940, 26900, 4]  \n",
       "42010                                      [17524, 6, 4]  \n",
       "42011                             [17524, 5381, 8805, 4]  \n",
       "42012                                   [17524, 7872, 4]  \n",
       "42013                                    [17524, 373, 4]  \n",
       "42014                               [17524, 734, 529, 4]  \n",
       "42015                                   [17524, 4359, 4]  \n",
       "42016                                  [17524, 16601, 4]  \n",
       "42017                                         [15297, 4]  \n",
       "42018                                  [15297, 10550, 4]  \n",
       "42019                          [15297, 45, 389, 7074, 4]  \n",
       "42020             [15297, 26904, 14405, 26898, 26908, 4]  \n",
       "42021                                   [15297, 6484, 4]  \n",
       "42022                                  [15297, 26908, 4]  \n",
       "42023                           [15297, 13901, 26897, 4]  \n",
       "42024                                    [15297, 753, 4]  \n",
       "42025                                    [15297, 544, 4]  \n",
       "42026                   [15297, 544, 815, 14635, 110, 4]  \n",
       "42027                       [15297, 544, 1371, 26910, 4]  \n",
       "42028                          [15297, 544, 984, 464, 4]  \n",
       "42029                        [15297, 544, 4528, 3500, 4]  \n",
       "42030                 [15297, 544, 3269, 2952, 26900, 4]  \n",
       "42031                         [15297, 80, 108, 26899, 4]  \n",
       "42032                                   [15297, 3171, 4]  \n",
       "42033                                    [15297, 175, 4]  \n",
       "42034                               [15297, 175, 772, 4]  \n",
       "42035                                    [15297, 344, 4]  \n",
       "42036                                   [15297, 9706, 4]  \n",
       "42037                                   [15297, 9381, 4]  \n",
       "42038                                   [15297, 8531, 4]  \n",
       "42039  [15297, 26902, 3119, 26897, 26916, 9600, 1179, 4]  \n",
       "42040  [15297, 1467, 26917, 1186, 26897, 4122, 26903, 4]  \n",
       "42041                                   [15297, 5630, 4]  \n",
       "42042                                   [2127, 26910, 4]  \n",
       "42043                                   [2127, 26920, 4]  \n",
       "42044                             [2127, 26920, 3019, 4]  \n",
       "42045                             [2127, 9804, 26901, 4]  \n",
       "42046                                         [10397, 4]  \n",
       "42047                                  [10397, 26903, 4]  \n",
       "42048                              [10397, 228, 5381, 4]  \n",
       "42049                             [10397, 211, 26910, 4]  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[42000:42050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 0:\n",
      "  1. Ta (probability: 13.8622)\n",
      "\n",
      "Position 1:\n",
      "  1. ##o (probability: 18.7628)\n",
      "\n",
      "Position 2:\n",
      "  1. [SEP] (probability: 23.0732)\n",
      "\n",
      "Position 3:\n",
      "  1. [SEP] (probability: 25.0103)\n",
      "\n",
      "Position 4:\n",
      "  1. [SEP] (probability: 23.8662)\n",
      "\n",
      "Position 5:\n",
      "  1. [SEP] (probability: 22.2218)\n",
      "\n",
      "Position 6:\n",
      "  1. [SEP] (probability: 20.8109)\n",
      "\n",
      "Position 7:\n",
      "  1. [SEP] (probability: 18.9489)\n",
      "\n",
      "Position 8:\n",
      "  1. [SEP] (probability: 17.2232)\n",
      "\n",
      "Position 9:\n",
      "  1. [SEP] (probability: 16.1217)\n",
      "\n",
      "Position 10:\n",
      "  1. [SEP] (probability: 15.6949)\n",
      "\n",
      "Position 11:\n",
      "  1. [SEP] (probability: 15.7490)\n",
      "\n",
      "Position 12:\n",
      "  1. [SEP] (probability: 16.2052)\n",
      "\n",
      "Position 13:\n",
      "  1. [SEP] (probability: 16.7751)\n",
      "\n",
      "Position 14:\n",
      "  1. [SEP] (probability: 17.1009)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# # end inference if this token is predicted\n",
    "# eos_token_id = tokenizer.convert_tokens_to_ids('[SEP]')  # EOS token\n",
    "\n",
    "# Example forward pass\n",
    "#input_tensor = torch.randn(1,768)\n",
    "\n",
    "tensor = torch.tensor(df[\"Average_Embedding\"][42042]).unsqueeze(0) #159\n",
    "# Generate Gaussian noise\n",
    "noise_level = 0.6\n",
    "noise = torch.randn_like(tensor) * noise_level\n",
    "# Add noise to tensor\n",
    "input_tensor = tensor + noise\n",
    "\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "for batch_idx in range(output_tensor.size(0)):\n",
    "    for position in range(output_tensor.size(1)):  # Iterate over sequence length\n",
    "        position_probs = output_tensor[batch_idx, position, :]  # Get probabilities for this position\n",
    "        \n",
    "        # Get top 5 probabilities\n",
    "        top_5_values, top_5_indices = torch.topk(position_probs, 1, dim=0)\n",
    "        \n",
    "        print(f\"Position {position}:\")\n",
    "        for i, (value, index) in enumerate(zip(top_5_values, top_5_indices), 1):\n",
    "            token = tokenizer.convert_ids_to_tokens([index.item()])[0]\n",
    "            probability = value.item()\n",
    "            print(f\"  {i}. {token} (probability: {probability:.4f})\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(df[\"Average_Embedding\"][8]).unsqueeze(0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
